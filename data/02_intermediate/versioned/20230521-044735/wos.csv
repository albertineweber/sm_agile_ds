entry,database,title,abstract,author_keywords,authors,authors_affiliations,publication_year,source_title,publisher,url,doi,query_date,processing_date
,wos,Achieving Agile Big Data Science: The Evolution of a Team Agile Process Methodology,"While there has been a rapid increase in the use of data science and the related field of big data, there has been minimal discussion on how teams using these techniques should best plan, coordinate and communicate their activities. To help address this gap, this paper reports on a mixed method qualitative study exploring how a big data science team within a Fortune 500 organization used two different agile process methodologies. The study helps clarify the concept of agility within a big data science project, as well as the key process challenges teams encounter when executing a big data science project. Specifically, three key issues were identified: (a) the challenge in task duration estimation, (b) how to account for team members that might be pulled onto other tasks for short bursts and (c) coordination challenges across the different groups within the big data science team. Our findings help explain how different process methodologies might mitigate or exacerbate these challenges and supports previous research showing that big data science teams would benefit from an increased focus on their process methodology and that adopting an Agile Kanban methodology, which focuses on minimizing work-in-progress, could prove beneficial for many big data science teams.",Big Data Science; Agile; Process Methodology,"Saltz, JS; Shamshurin, I",Syracuse University,2019.0,2019 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,,20230520-160000,20230521-044735
,wos,Applying Scrum in Data Science Projects,"The rise of big data has led to an increase in data science projects conducted by organizations. Such projects aim to create valuable insights by improving decision making or enhancing an organization's service offering through datadriven services. However, the majority of data science projects still fail to deliver the expected value. To increase the success rate of projects, the use of process models or methodologies is recommended in the literature. Nevertheless, organizations are hardly using them because they are considered too rigid and they do not support the typical iterative and open nature of data science projects. To overcome this problem, this research suggests applying Agile methodologies to data science projects. Agile methodologies were originally developed in the software engineering domain and are characterised by their iterative approach towards software development. In this study, we selected the Scrum approach and integrated it into the CRISP-DM methodology for data science projects using a Design Science Research approach. This new methodology was then evaluated in three different case organizations using expert interviews. Analysis of the expert interviews resulted in a further refinement of the Agile data science methodology proposed by this research.",Data Science; Agile; Scrum,"Baijens, J; Helms, R; Iren, D",Open University Netherlands,2020.0,"2020 IEEE 22ND CONFERENCE ON BUSINESS INFORMATICS (CBI 2020), VOL I - RESEARCH PAPERS",IEEE COMPUTER SOC,,10.1109/CBI49978.2020.00011,20230520-160000,20230521-044735
,wos,SKI: An Agile Framework for Data Science,"This paper explores data science project management by first noting the need for a new process management framework and then defines a process framework that effectively supports the needs of a data science team. The paper also reports on a pilot study of teams using the framework. The framework adheres to the lean Kanban philosophy but augments Kanban by providing a structured iteration process for teams to incrementally explore and learn via lean hypothesis testing. Specifically, the Structured Kanban Iteration (SKI) framework focuses on having teams define capability-based iterations (as opposed to Kanban-like no iterations or Scrum-like time-based sprints). Furthermore, unlike Kanban, the framework leverages Scrum best practices to define roles, meetings and artifacts. Thus. SKI implements the Kanban process, but with a more repeatable and structured approach.",Data Science; Big Data; Agile; Process Methodology,"Saltz, J; Suthrland, A",Syracuse University,2019.0,2019 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,,20230520-160000,20230521-044735
,wos,Adopting Agile Software Development Methodologies in Big Data Projects - a Systematic Literature Review of Experience Reports,"During the last decade, agile software development methodologies have been widely adopted in various project contexts. Big data projects are different from software engineering projects in all three aspects - people, processes and technologies. Recent research has shown that agile approaches are suitable and beneficial when applied in big data projects. The aim of the current study is to investigate which of the agile software development methodologies are currently applied in big data projects and what are the key considerations for their application. As a first step towards achieving this aim, the paper presents a systematic literature review of research articles reporting real-world experience of adopting agile methodologies in different big data science contexts. The findings of the study are beneficial to both practitioners and researchers to define and adopt agile approaches which are well suited for their big data projects.",agile software development; big data science; methodology adoption; real-world experience; systematic literature review,"Krasteva, I; Ilieva, S",,2020.0,2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,10.1109/BigData50022.2020.9378118,20230520-160000,20230521-044735
,wos,Web Services-based Report Generation System from Big Data in the Manufacturing Industry based on Agile Software Development,"In the manufacturing industry, there are many information technology (IT) systems and machines connected to different databases with complex big data. In this study, the web service system is developed by using different programming languages consisting of C#, Javascript, HTML, CSS, Ext JS and structured query language (SQL). In addition to these programming languages, model view controller (MVC), a software design pattern is also used for developing a database interface. The development of this system allows the web service system to search for reports that meet the user needs and also has the user interface (UI) for convenience and speed. In this study, agile software development method is used in accordance with scrum framework, which consist of 4 steps: 1) product backlog creation, 2) sprint backlog creation, 3) sprint or the system development and testing and test case writing and 4) daily scrum. Sprint review is held to report the results of unit test, functional test, integration test and user acceptance test (UAT). This sprint review enables the development of an appropriate and comprehensive system and results in collaboration and understanding among stakeholders, including technology adoption among users.",Big Data; Web Services; Agile,"Kattiyawong, P; Tangprasert, S",King Mongkuts University of Technology North Bangkok,2020.0,"PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS (ICEIS), VOL 1",SCITEPRESS,,10.5220/0009459102260232,20230520-160000,20230521-044735
,wos,Agile Elastic Desktop Corporate Architecture for Big Data,"New challenges in the dynamically changing business environment require companies to experience digital transformation and more effective use of Big Data generated in their expanding online business activities. A possible solution for solving real business problems concerning Big Data resources is proposed in this paper. The defined Agile Elastic Desktop Corporate Architecture for Big Data is based on virtualizing the unused desktop resources and organizing them in order to serve the needs of Big Data processing, thus saving resources needed for additional infrastructure in an organization. The specific corporate business needs are analyzed within the developed R&D environment and, based on that, the unused desktop resources are customized and configured into required Big Data tools. The R&D environment of the proposed Agile Elastic Desktop Corporate Architecture for Big Data could be implemented on the available unused resources of hundreds desktops.",Agile Elastic Desktop Corporate Architecture; Desktop Virtualization; Big Data; Digital Transformation,"Kisimov, V; Kabakchieva, D; Naydenov, A; Stefanova, K",University of National & World Economics - Bulgaria,2020.0,CYBERNETICS AND INFORMATION TECHNOLOGIES,INST INFORMATION & COMMUNICATION TECHNOLOGIES-BULGARIAN ACAD SCIENCES,,10.2478/cait-2020-0025,20230520-160000,20230521-044735
,wos,Current approaches for executing big data science projects-a systematic literature review,"There is an increasing number of big data science projects aiming to create value for organizations by improving decision making, streamlining costs or enhancing business processes. However, many of these projects fail to deliver the expected value. It has been observed that a key reason many data science projects don't succeed is not technical in nature, but rather, the process aspect of the project. The lack of established and mature methodologies for executing data science projects has been frequently noted as a reason for these project failures. To help move the field forward, this study presents a systematic review of research focused on the adoption of big data science process frameworks. The goal of the review was to identify (1) the key themes, with respect to current research on how teams execute data science projects, (2) the most common approaches regarding how data science projects are organized, managed and coordinated, (3) the activities involved in a data science projects life cycle, and (4) the implications for future research in this field. In short, the review identified 68 primary studies thematically classified in six categories. Two of the themes (workflow and agility) accounted for approximately 80% of the identified studies. The findings regarding workflow approaches consist mainly of adaptations to CRISP-DM (vs entirely new proposed methodologies). With respect to agile approaches, most of the studies only explored the conceptual benefits of using an agile approach in a data science project (vs actually evaluating an agile framework being used in a data science context). Hence, one finding from this research is that future research should explore how to best achieve the theorized benefits of agility. Another finding is the need to explore how to efficiently combine workflow and agile frameworks within a data science context to achieve a more comprehensive approach for project execution.",Big data science; Project execution; Process frameworks; Big data science workflows; Agile data science,"Saltz, JS; Krasteva, I",Syracuse University; University of Sofia,2022.0,PEERJ COMPUTER SCIENCE,PEERJ INC,,10.7717/peerj-cs.862,20230520-160000,20230521-044735
,wos,Exploring the Challenges of Integrating Data Science Roles in Agile Autonomous Teams,"The notion of autonomous teams is core to agile software development. However, autonomy in agile teams is challenged by increasingly complex software projects, large-scale agile and perhaps increasingly multidisciplinary teams. At the same time, data science roles are making their way into agile teams as companies seek to reap the potential advantages of using data to develop better and more competitive services and products. The consequences of implementing such roles in traditional agile teams are largely unknown. In this paper, we take an exploratory approach to the topic of data science roles in agile teams by a set of interviews with five data scientists as well as three members of an agile software development team. Based on the interviews we identify a set of challenges associated with incorporating the role in agile autonomous teams. Based on these challenges we discuss preliminary recommendations for companies seeking to integrate data science roles in agile teams.",Data science; Agile; Software development; Teams; Autonomy,"Hukkelberg, I; Berntzen, M",University of Oslo,2019.0,AGILE PROCESSES IN SOFTWARE ENGINEERING AND EXTREME PROGRAMMING - WORKSHOPS,SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-030-30126-2_5,20230520-160000,20230521-044735
,wos,A Big Data on Private Cloud Agile Provisioning Framework Based on OpenStack,"On the bases of the OpenStack private cloud delivery big data platform, numerous entities yearn for attaining agile and standardized big data delivery platform, reclaiming the resources, managing the total cost of ownership (TCO) and adapting to multiple big data open source or commercial off-the-shelf (COTS) solutions. Nevertheless, as regards the big data platform running on cloud computing, the big data platform is disintegrated from the cloud computing system by virtual machines since neither being based on OpenStack private cloud nor on big data platform can achieve end-to-end resource delivery, together with ensuring that it is quite convenient for the long-term operations. Accordingly, establishing an across framework between private cloud and big data platform is quite essential. The big data on cloud agile provision framework could realize fast resource delivery based on predefined orchestration template of private cloud, operating system, big data platform, monitor, inspection system, etc. Through the deployment of this framework, it is capable of attaining the delivery of agile, low cost, standardized and high adaptability the big data on cloud, as well as the high-quality operation of the big data on cloud with the help of integration configuration management database (CMDB) with the automatic inspection system.",cloud computing; agile resource provisioning; big data platform orchestration; inspection and rule engine,"Lu, M; Zhou, X","Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Legend Holdings; Lenovo; Legend Holdings; Lenovo",2018.0,2018 IEEE 3RD INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND BIG DATA ANALYSIS (ICCCBDA),IEEE,,,20230520-160000,20230521-044735
,wos,Lessons learned to improve the UX practices in agile projects involving data science and process automation,"Context: User-Centered Design (UCD) and Agile methodologies focus on human issues. Nevertheless, agile methodologies focus on contact with contracting customers and generating value for them. Usually, the communication between end users (they use the software and have low decision power) and the agile team is mediated by customers (they have high decision power but do not use the software). However, they do not know the actual problems that end users (may) face in their routine, and they may not be directly affected by software shortcomings. In this context, UX issues are typically identified only after the implementation, during user testing and validation. Objective: Aiming to improve the understanding and definition of the problem in agile projects, this research investigates the practices and difficulties experienced by agile teams during the development of data science and process automation projects. Also, we analyze the benefits and the teams' perceptions regarding user participation in these projects. Method: We collected data from four agile teams, in the context of an academia and industry collaboration focusing on delivering data science and process automation solutions. Therefore, we applied a carefully designed questionnaire answered by developers, scrum masters, and UX designers. In total, 18 subjects answered the questionnaire. Results: From the results, we identify practices used by the teams to define and understand the problem and to represent the solution. The practices most often used are prototypes and meetings with stakeholders. Another practice that helped the team to understand the problem was using Lean Inception (LI) ideation workshops. Also, our results present some specific issues regarding data science projects. Conclusion: We observed that end-user participation can be critical to understanding and defining the problem. They help to define elements of the domain and barriers in the implementation. We identified a need for approaches that facilitate user-team communication in data science projects to understand the data and its value to the users' routine. We also identified insights about the need of more detailed requirements representations to support the development of data science solutions.",Agile; User -centered design; Lean inception; User involvement; User participation; User experience; Data science,"Ferreira, B; Marques, S; Kalinowski, M; Lopes, H; Barbosa, SDJ",,2023.0,INFORMATION AND SOFTWARE TECHNOLOGY,ELSEVIER,,10.1016/j.infsof.2022.107106,20230520-160000,20230521-044735
,wos,Agile manufacturing practices: the role of big data and business analytics with multiple case studies,"The purpose of this study was to examine the role of big data and business analytics (BDBA) in agile manufacturing practices. Literature has discussed the benefits and challenges related to the deployment of big data within operations and supply chains, but there has not been a study of the facilitating roles of BDBA in achieving an enhanced level of agile manufacturing practices. As a response to this gap, and drawing upon multiple qualitative case studies undertaken among four UK organisations, we present and validate a framework for the role of BDBA within agile manufacturing. The findings show that market turbulence has negative universal effects and that agile manufacturing enablers are being progressively deployed and aided by BDBA to yield better competitive and business performance objectives. Further, the level of intervention was found to differ across companies depending on the extent of deployment of BDBA, which accounts for variations in outcomes.",big data and business analytics; agile manufacturing; enablers; competitive advantage; performance,"Gunasekaran, A; Yusuf, YY; Adeleye, EO; Papadopoulos, T",California State University System; California State University Bakersfield; University of Central Lancashire; University of Kent,2018.0,INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH,TAYLOR & FRANCIS LTD,,10.1080/00207543.2017.1395488,20230520-160000,20230521-044735
,wos,Application of Methodologies and Process Models in Big Data Projects,"The concept of Big Data is being used in different business sectors; however, it is not certain which methodologies and process models have been used for the development of these kind of projects. This paper presents a systematic literature review of studies reported between 2012 and 2017 related to agile and non-agile methodologies applied in Big Data projects. For validating our review process, a text mining method was used. The results reveal that since 2016 the number of articles that integrate the agile manifesto in Big Data project has increased, being Scrum the agile framework most commonly applied. We also found that 44% of articles obtained from a manual systematic literature review were automatically identified by applying text mining.",Agile Methodologies; Big Data; Systematic Literature Review; Text Mining,"Quelal, R; Mendoza, LE; Villavicencio, M",Escuela Superior Politecnica del Litoral; Simon Bolivar University,2019.0,"PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS (ICEIS 2019), VOL 2",SCITEPRESS,,10.5220/0007729602770284,20230520-160000,20230521-044735
,wos,Enhanced Framework for Big Data Requirement Elicitation,"Requirement engineering is one of the software development life cycle phases; it has been recognized as an important phase for collecting and analyzing a system's goals. However, despite its importance, requirement engineering has several limitations such as incomplete requirements, vague requirements, lack of prioritization, and less user involvement, all of which affect requirement quality. With the emergence of big data technology, the complexity of big data, which is defined by large data volume, high velocity, and large data variety, has gradually increased, affecting the quality of big data software requirements. This study proposes a framework with four sequential phases to improve requirement engineering quality through big data software development. By integrating the proposed framework's phases in which user requirements are collected in a complete vision using traditional requirement elicitation techniques with agile methodology and mind mapping, the collected requirements are displayed via a graphical representation using mind maps to achieve high requirement accuracy with connectivity and modifiability, enabling the accurate prioritization of requirements implemented using agile SCRUM methodology. The proposed framework improves requirement quality in big data software development, which is represented by accuracy, completeness, connectivity, and modifiability to understand the value of the collected requirements and effectively affect the quality of the implementation phase.",Requirement engineering; big data requirement; agile methodology; mind mapping,"Hesham, A; Emam, OE; Salah, M",Egyptian Knowledge Bank (EKB); Helwan University; Egyptian Knowledge Bank (EKB); British University in Egypt,2021.0,INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS,SCIENCE & INFORMATION SAI ORGANIZATION LTD,,,20230520-160000,20230521-044735
,wos,A Scalable Methodology to Guide Student Teams Executing Computing Projects,"This article reports on a sequential mixed-methods research study, which compared different approaches on how to guide students through a semester-long data science project. Four different methodologies, ranging from a traditional just assign some intermediate milestones to other more Agile methodologies, were first compared via a controlled experiment. The results of this initial experiment showed that the project methodology used made a significant difference in student outcomes. Surprisingly, the Agile Kanban approach was found to be much more effective than the Agile Scrum methodology. Based on these initial results, in the second semester, we focused on use of the Kanban methodology. The findings in the second, more qualitative phase, confirmed the methodology's usefulness and scalability. A key issue when using the scrum methodology was that the students had a very difficult time estimating what could be completed in each of their two-week sprints. The Kanban board, which visually shows and limits work-in-progress, was found to be an effective way for students to communicate with each other as well as with their instructor. In addition, Agile Kanban also streamlined the work required for instructors to efficiently provide guidance to student teams and to understand each team's status. In summary, a scalable Kanban methodology, which can be applied to a wide variety of student computing projects, was found to an effective methodology to guide and manage student projects, improving student outcomes and minimizing instructor workload.",Project-based learning; scalable methodologies; project methodologies,"Saltz, JS; Heckman, RR",Syracuse University,2018.0,ACM TRANSACTIONS ON COMPUTING EDUCATION,ASSOC COMPUTING MACHINERY,,10.1145/3145477,20230520-160000,20230521-044735
,wos,Managing and Composing Teams in Data Science: An Empirical Study,"Data science projects have become commonplace over the last decade. During this time, the practices of running such projects, together with the tools used to run them, have evolved considerably. Furthermore, there are various studies on data science workflows and data science project teams. However, studies looking into both workflows and teams are still scarce and comprehensive works to build a holistic view do not exist. This study bases on a prior case study on roles and processes in data science. The goal here is to create a deeper understanding of data science projects and development processes. We conducted a survey targeted at experts working in the field of data science (n=50) to understand data science projects' team structure, roles in the teams, utilized project management practices and the challenges in data science work. Results show little difference between big data projects and other data science. The found differences, however, give pointers for future research on how agile data science projects are, and how important is the role of supporting project management personnel. The current study is work in progress and attempts to spark discussion and new research directions.",Data science; agile practices; teamwork; project management,"Aho, T; Kilamo, T; Lwakatare, L; Mikkonen, T; Sievi-Korte, O; Yaman, S",Tampere University; University of Helsinki; University of Jyvaskyla,2021.0,2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,10.1109/BigData52589.2021.9671737,20230520-160000,20230521-044735
,wos,A Review and Future Direction of Business Analytics Project Delivery,"Business analytics is a core competency critical to organizations to stay competitive; however, many organizations are challenged at business analytics delivery, and these projects have a high rate of failure. The volume, variety, and velocity of the big data phenomenon and the lack of current methodologies for delivering business analytics projects are the primary challenges. Applying traditional information technology project methodologies is problematic and has been identified as the largest contributing factor for business analytics project failure. Business analytics projects focus on delivering data insights as well as software delivery. Agile methodologies focus on the ability to respond to change through incremental, iterative processes. Agile methodologies in software delivery have been on the rise, and the dynamic principles align with the discovery nature of business analytics projects. This article explores the big data phenomenon, its impact on business analytics project delivery, and recommends an agile framework for business analytic project delivery using agile methodology principles and practices.",Agile methodologies; Analytics projects; Big data; CRISP-DM; Agile software development,"Larson, D",,2019.0,ALIGNING BUSINESS STRATEGIES AND ANALYTICS: BRIDGING BETWEEN THEORY AND PRACTICE,SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-319-93299-6_7,20230520-160000,20230521-044735
,wos,Big Data Analytics on Cyber Attack Graphs for Prioritizing Agile Security Requirements,"In enterprise environments, the amount of managed assets and vulnerabilities that can be exploited is staggering. Hackers' lateral movements between such assets generate a complex big data graph, that contains potential hacking paths. In this vision paper, we enumerate risk-reduction security requirements in large scale environments, then present the Agile Security methodology and technologies for detection, modeling, and constant prioritization of security requirements, agile style. Agile Security models different types of security requirements into the context of an attack graph, containing business process targets and critical assets identification, configuration items, and possible impacts of cyber-attacks. By simulating and analyzing virtual adversary attack paths toward cardinal assets, Agile Security examines the business impact on business processes and prioritizes surgical requirements. Thus, handling these requirements backlog that are constantly evaluated as an outcome of employing Agile Security, gradually increases system hardening, reduces business risks and informs the IT service desk or Security Operation Center what remediation action to perform next. Once remediated, Agile Security constantly recomputes residual risk, assessing risk increase by threat intelligence or infrastructure changes versus defender's remediation actions in order to drive overall attack surface reduction.",Security Requirements; Requirements Prioritization; Agile Security; Attack Graph; Graph Analytics; Attack Path; Remediation Requirements; Attack Surface; Cyber Digital Twin,"Hadar, E; Hassanzadeh, A",Accenture,2019.0,2019 27TH IEEE INTERNATIONAL REQUIREMENTS ENGINEERING CONFERENCE (RE 2019),IEEE COMPUTER SOC,,10.1109/RE.2019.00042,20230520-160000,20230521-044735
,wos,Agile supply chain analytic approach: a case study combining agile and CRISP-DM in an end-to-end supply chain,"In the current competitive environment, Big Data Analytics (BDA) has become a prominent metric to reach an integrated, efficient, and effective supply chain (SC). In the literature, the BDA capabilities have been at the forefront of research in operational supply chain management (SCM), however, there has been a paucity of literature regarding its technical and organisational implementation in the industry. Hence, despite its capabilities and importance, many organisations are reluctant to adopt this promising concept in SC operations management, due to the ambiguity of its practical implementation from a technical and organisational point of view. To address this gap, this paper draws on agile project management, data mining model processing, and case study approaches to propose and test a framework for BDA organisational implementation in SCM. The feasibility of the proposed framework is illustrated and tested by a case study in an end-to-end SC within a large corporation. Our contribution lies in handling the organisational, managerial, and socio-technical challenges of BDA projects implementation in SCM.",Supply chain management; Big Data Analytics; agile methodology; CRISP-DM methodology; Scrum; project management,"Hamdani, FE; Quintero, IAQ; Enjolras, M; Camargo, M; Monticolo, D; Lelong, C",Universite de Lorraine,,SUPPLY CHAIN FORUM,TAYLOR & FRANCIS LTD,,10.1080/16258312.2022.2064721,20230520-160000,20230521-044735
,wos,Big Data analytics in Agile software development: A systematic mapping study,"Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.",Agile software development; Software analytics; Data analytics; Machine learning; Artificial intelligence; Literature review,"Biesialska, K; Franch, X; Muntes-Mulero, V",Universitat Politecnica de Catalunya,2021.0,INFORMATION AND SOFTWARE TECHNOLOGY,ELSEVIER,,10.1016/j.infsof.2020.106448,20230520-160000,20230521-044735
,wos,Agile Business Intelligence: Combining Big Data and Business Intelligence to Responsive Decision Model,"Big data and instant information on the external environment of enterprises are critical for decision-making; however, little attention is paid to contemporary business intelligence (BI) theories and practices. This study proposes a new business decision model, agile BI (ABI), which integrates external big data and internal BI to facilitate decision-making for enterprises in a dynamic and rapidly changing environment. This novel model presents two contributions to research in this field: (1) it proposes a new architecture combining external and internal information and (2) it integrates external big data, such as hot searches, social media, news, popular issues, and competitors' information, to increase the accuracy of BI. This study takes international expansion as an example and simulates the international investment decisions of Taiwanese firms by importing data from a search engine, competitors, and firm-specific datasets. The results show that the proposed ABI model not only responds quickly to the external environment but also enhances decision-making efficiently.",Big data; Business intelligence; Decision making,"Chang, BJ",Feng Chia University,2018.0,JOURNAL OF INTERNET TECHNOLOGY,"LIBRARY & INFORMATION CENTER, NAT DONG HWA UNIV",,10.3966/160792642018111906007,20230520-160000,20230521-044735
,wos,Identifying the most Common Frameworks Data Science Teams Use to Structure and Coordinate their Projects,"This paper presents the results of a study focused on exploring which framework, if any, teams use to execute data science projects. The study consisted of a survey of 109 industry professionals, as well as an evaluation of relevant framework terms searched at Google. Overall, CRISP-DM was the most commonly used framework, with Scrum and Kanban being the second and third most frequently used. We note that CRISP-DM is a life cycle framework, whereas Scrum and Kanban are team coordination frameworks. Hence, this research also notes the potential demand for a framework that integrates both life cycle and team coordination aspects of leading a data science project.",Data Science; Big Data; Process Methodology,"Saltz, JS; Hotz, N",Syracuse University; Indiana University System; Indiana University Bloomington,2020.0,2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,10.1109/BigData50022.2020.9377813,20230520-160000,20230521-044735
,wos,DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects,"Decision making is the lifeblood of the enterprise - from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being data drivenis more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.",Big Data; Methodology; Decisional systems; Agile; Data governance; Data quality,"Sfaxi, L; Ben Aissa, MM",Universite de Carthage; Universite de Carthage,2020.0,DATA & KNOWLEDGE ENGINEERING,ELSEVIER,,10.1016/j.datak.2020.101862,20230520-160000,20230521-044735
,wos,An Improved Agile Framework For Implementing Data Science Initiatives in the Government,"Implementing data mining projects in governmental organizations is emerging in the Middle East. The literature has been showing that there is a significant gap between the problems defined by the research in data mining and the problems in real world projects. The gap is to the level of semantics between the data scientists and the business users. Trying to fill this gap, we have developed an improved Agile data mining framework to fulfill the government business objectives and needs. The previous works had been claiming that handling such project is not yet mature in the region. For this an Agile implementation framework is required. We are also proposing a systematic way for identifying business problems as part of the framework. The process is Agile, so it would start from investigating the data set dimensions to identify business problems. It also allows early Business people cooperation with data scientist. We've applied the proposed framework in one of the Middle East government organizations. The business team and the data scientists have been showing their satisfaction regarding the results of using the proposed framework. The proposed framework have helped both business and data scientist to implement their first initiative in data mining. The proposed framework also helped in efficiently mapping the project with the core business objectives and problems using real world dataset.",data mining; data science; agile framework; business problems; business objectives,"Qadadeh, W; Abdallah, S",,2020.0,2020 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2020),IEEE COMPUTER SOC,,10.1109/ICICT50521.2020.00012,20230520-160000,20230521-044735
,wos,"Big Data Analytics as a mediator in Lean, Agile, Resilient, and Green (LARG) practices effects on sustainable supply chains","The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of 'Big Data Analytics' (BDA) as a mediator between 'sustainable supply chain business performance' and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirtyseven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.",Big data analytics; Manufacturing firms; Supply chain and logistics management; LARG; Business performance and innovation; Sustainability,"Raut, RD; Mangla, SK; Narwane, VS; Dora, M; Liu, MQ",National Institute of Industrial Engineering (NITIE); University of Plymouth; Somaiya Vidyavihar University; K J Somaiya College of Engineering; Brunel University; Hunan University,2021.0,TRANSPORTATION RESEARCH PART E-LOGISTICS AND TRANSPORTATION REVIEW,PERGAMON-ELSEVIER SCIENCE LTD,,10.1016/j.tre.2020.102170,20230520-160000,20230521-044735
,wos,How big data analytics use improves supply chain performance: considering the role of supply chain and information system strategies,"Purpose Drawing on the dynamic capabilities theory, this paper proposes that supply chain (SC) strategies (i.e. the lean SC and agile SC strategies) will mediate the relationship between big data analytics (BDA) and SC performance. Furthermore, from the perspective of strategic alignment, this study hypothesizes that the effect of the SC strategy on SC performance is differently moderated by the information system (IS) strategy (i.e. the IS innovator and IS conservative strategies). Design/methodology/approach This study used 159 match-paired questionnaires collected from Chinese firms to empirically test the hypotheses. Findings Results show the positive direct and indirect impact of BDA on SC performance. Specifically, the lean and agile SC strategies mediate the relationship between BDA and SC performance. Furthermore, the results indicate that the IS innovator and IS conservative strategies differentially moderate the effect of the lean and agile SC strategies on SC performance. Specifically, the IS innovator strategy positively moderates the effect of the agile SC strategy on SC performance. By contrast, the IS conservative strategy positively moderates the effect of the lean SC strategy on SC performance but negatively moderates the effect of the agile SC strategy on SC performance. Originality/value This study provides a comprehensive understanding of how SC and IS strategies can help firms leverage BDA to improve SC performance.",Big data analytics; Supply chain performance; Lean supply chain strategy; Agile supply chain strategy; IS innovator Strategy; IS conservative Strategy,"Wei, SB; Yin, JM; Chen, W","Hefei University of Technology; Nanjing University of Aeronautics & Astronautics; Chinese Academy of Sciences; University of Science & Technology of China, CAS",2022.0,INTERNATIONAL JOURNAL OF LOGISTICS MANAGEMENT,EMERALD GROUP PUBLISHING LTD,,10.1108/IJLM-06-2020-0255,20230520-160000,20230521-044735
,wos,IoT Analytics and Agile Optimization for Solving Dynamic Team Orienteering Problems with Mandatory Visits,"Transport activities and citizen mobility have a deep impact on enlarged smart cities. By analyzing Big Data streams generated through Internet of Things (IoT) devices, this paper aims to show the efficiency of using IoT analytics, as an agile optimization input for solving real-time problems in smart cities. IoT analytics has become the main core of large-scale Internet applications, however, its utilization in optimization approaches for real-time configuration and dynamic conditions of a smart city has been less discussed. The challenging research topic is how to reach real-time IoT analytics for use in optimization approaches. In this paper, we consider integrating IoT analytics into agile optimization problems. A realistic waste collection problem is modeled as a dynamic team orienteering problem with mandatory visits. Open data repositories from smart cities are used for extracting the IoT analytics to achieve maximum advantage under the city environment condition. Our developed methodology allows us to process real-time information gathered from IoT systems in order to optimize the vehicle routing decision under dynamic changes of the traffic environments. A series of computational experiments is provided in order to illustrate our approach and discuss its effectiveness. In these experiments, a traditional static approach is compared against a dynamic one. In the former, the solution is calculated only once at the beginning, while in the latter, the solution is re-calculated periodically as new data are obtained. The results of the experiments clearly show that our proposed dynamic approach outperforms the static one in terms of rewards.",IoT analytics; big data streams; agile optimization; smart cities; transport analytics; dynamic team orienteering problem,"Li, YD; Peyman, M; Panadero, J; Juan, AA; Xhafa, F",UOC Universitat Oberta de Catalunya; Universitat Politecnica de Valencia; Universitat Politecnica de Catalunya,2022.0,MATHEMATICS,MDPI,,10.3390/math10060982,20230520-160000,20230521-044735
,wos,Analysis of Software Engineering for Agile Machine Learning Projects,"The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects.",scrum; machine learning project; software engineering; agile methodology,"Singla, K; Bose, J; Naik, C",,2018.0,IEEE INDICON: 15TH IEEE INDIA COUNCIL INTERNATIONAL CONFERENCE,IEEE,,,20230520-160000,20230521-044735
,wos,Using a coach to improve team performance when the team uses a Kanban process methodology,"Teams are increasing their use of the Kanban process methodology across a range of information system projects, including software development and data science projects. While the use of Kanban is growing, little has been done to explore how to improve team performance for teams that use Kanban. One possibility is to introduce a Kanban Coach (KC). This work reports on exploring the use of a Kanban Coach, with respect to both how the coach could interact with the team as well as how the use of a coach impacts team results. Specifically, this paper reports on an experiment where teams either had, or did not have, a Kanban Coach. A quantitative and qualitative analysis of the data collected during the experiment found that introducing KC led to significant improvement of team performance. Coordination Theory and Shared Mental Models were then employed to provide an explanation as to why a KC leads to better project results. While this experiment was done within a data science project context, the results are likely applicable across a range of information system projects.",project management; process methodology; agile; team performance; Kanban; Kanban process methodology,"Shamshurin, I; Saltz, JS",Syracuse University,2019.0,IJISPM-INTERNATIONAL JOURNAL OF INFORMATION SYSTEMS AND PROJECT MANAGEMENT,SCIKA,,10.12821/ijispm070204,20230520-160000,20230521-044735
,wos,Identifying and Addressing 6 Key Questions when Using Data Driven Scrum,"Data Driven Scrum (DDS) enables lean data science project agility and addresses the key challenges that have been identified when using Scrum in a data science context. However, little has been written with respect to the questions or challenges teams might encounter when trying to use DDS. Based on a survey of 18 team leads trying to use DDS, this paper describes six common questions teams might encounter when trying to implement DDS, as well as how to address these challenges.",,"Saltz, JS; Sutherland, A; Jombart, T",Syracuse University; University of London; London School of Hygiene & Tropical Medicine,2021.0,2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,10.1109/BigData52589.2021.9671930,20230520-160000,20230521-044735
,wos,"CRISP-DM for Data Science: Strengths, Weaknesses and Potential Next Steps","This paper explores the strengths and weaknesses of CRISP-DM when used for data science projects. The paper then explores what key actions data science teams using CRISP-DM should consider that addresses CRISP-DM's weaknesses. In brief, CRISP-DM, which is the most popular framework teams use to execute data science projects, provides an easy to understand description of the data science project workflow (i.e., the data science life cycle). However, CRISP-DM's project phases miss some key aspects of the data science project life cycle. In addition, CRISP-DM's task-focused approach fails to address how a team should prioritize tasks, and in general, collaborate and communicate. Hence, this paper also describes how CRISP-DM could be combined with a team coordination framework, such as Scrum or Data Driven Scrum, which is a newer collaboration framework developed to address the unique data science coordination challenges.",,"Saltz, JS",Syracuse University,2021.0,2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,10.1109/BigData52589.2021.9671634,20230520-160000,20230521-044735
,wos,Integration of Lean practices and Industry 4.0 technologies: smart manufacturing for next-generation enterprises,"Industry 4.0 technologies have attempted to transform current industrial settings to a level that we have never seen before. While at the same time, prevailing applications of Lean tools and techniques over the last 20 years have already dramatically reduced wastes ranging from shop floor production to cross-functional enterprise processes. This paper aims to provide a comprehensive review and report on links between Lean tools and Industry 4.0 technologies, and on how simultaneous implementation of these two paradigms affects the operational performance of factories. The existing and potential enhancements of Lean practices enabled by Industry 4.0 technologies such as wireless networks, big data, cloud computing, and virtual reality (VR) will also be explored. A cloud-based Kanban decision support system is also presented as a real-world demonstrator for integration of an Industry 4.0 technology (cloud computing) and a major Lean tool (Kanban).",Industry 4; 0; Lean tools; Smart factory; Cloud Kanban,"Shahin, M; Chen, FF; Bouzary, H; Krishnaiyer, K",University of Texas System; University of Texas at San Antonio (UTSA),2020.0,INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY,SPRINGER LONDON LTD,,10.1007/s00170-020-05124-0,20230520-160000,20230521-044735
,wos,Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization,"Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.",Technology roadmapping; Technology management; Data science; Digital transformation; Data-driven organization; Big data,"Kayabay, K; Gokalp, MO; Gokalp, E; Eren, PE; Kocyigit, A",Middle East Technical University; Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK); RLUK- Research Libraries UK; University of Cambridge; Hacettepe University,2022.0,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,ELSEVIER SCIENCE INC,,10.1016/j.techfore.2021.121264,20230520-160000,20230521-044735
,wos,Literature Review on Big Data Analytics and Demand Modeling in Supply Chain,"New digital technologies have been introduced into our business and social environments, causing a major change that is recognized as the digital transformation in recent years. While environmental shifts suggest that most of the organization starts using advanced technologies such as Internet of Things (IoT), Mobile applications, Blackchain, Intelligence Things, catboats and many more in their supply chain planning to gain an early competitive advantage and these technologies generates enormous amount of data that the traditional business intelligence system difficult to handle processing of vast data in real-time or nearly real time causes abstraction to the insight discovery, demand modeling and supply chain optimization, Big Data initiatives for demand modeling and supply chain optimization promise to answer these challenges by incorporating various services, methods and tools for more agile and adaptably analytics and decision making, there by this paper focus on reviewing the level of analytics and the forecasting methods being used in the supply chain, understating the fundamentals of supply chain and role of demand modeling, there by proposing a high level framework for supply chain analytics in the context of big data with the knowledge of data science, artificial intelligence, big data echo system and supply chain.",Supply chain; Demand modeling; Big data Analytics; Forecasting methods; supply chain framework,"Kumar, TP; Manjunath, TN; Hegadi, RS",Solapur University,2018.0,"2018 3RD INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, COMMUNICATION, COMPUTER, AND OPTIMIZATION TECHNIQUES (ICEECCOT - 2018)",IEEE,,,20230520-160000,20230521-044735
,wos,"The application framework of big data technology in the COVID-19 epidemic emergency management in local government-a case study of Hainan Province, China","Background As COVID-19 continues to spread globally, traditional emergency management measures are facing many practical limitations. The application of big data analysis technology provides an opportunity for local governments to conduct the COVID-19 epidemic emergency management more scientifically. The present study, based on emergency management lifecycle theory, includes a comprehensive analysis of the application framework of China's SARS epidemic emergency management lacked the support of big data technology in 2003. In contrast, this study first proposes a more agile and efficient application framework, supported by big data technology, for the COVID-19 epidemic emergency management and then analyses the differences between the two frameworks. Methods This study takes Hainan Province, China as its case study by using a file content analysis and semistructured interviews to systematically comprehend the strategy and mechanism of Hainan's application of big data technology in its COVID-19 epidemic emergency management. Results Hainan Province adopted big data technology during the four stages, i.e., migration, preparedness, response, and recovery, of its COVID-19 epidemic emergency management. Hainan Province developed advanced big data management mechanisms and technologies for practical epidemic emergency management, thereby verifying the feasibility and value of the big data technology application framework we propose. Conclusions This study provides empirical evidence for certain aspects of the theory, mechanism, and technology for local governments in different countries and regions to apply, in a precise, agile, and evidence-based manner, big data technology in their formulations of comprehensive COVID-19 epidemic emergency management strategies.",COVID-19; Emergency management; Big data technology; Application framework; Local government; Hainan Province; China,"Mao, ZJ; Zou, Q; Yao, H; Wu, JY",Huazhong University of Science & Technology; Huazhong University of Science & Technology,2021.0,BMC PUBLIC HEALTH,BMC,,10.1186/s12889-021-12065-0,20230520-160000,20230521-044735
,wos,Reframing business reporting in a Big Data world,"This paper investigates the challenges raised by the datafication of the business environment in the area of performance management. Big Data and its powerful analytics are now essential elements of the business landscape, and the mindset of managers and decision-makers has a crucial impact on how the opportunities offered by these new technologies can be capitalised. Our focus is on how and to what extent business performance management models need to be reviewed and reframed in this era of Big Data. We propose a model, called Balanced ScoreCard System Thinking, that may insure an integrative, highly dynamic and agile construction. This top-down-bottom-up approach assess the way in which every segment of the Balance Score Card is affected by Big Data and Business Analytics. These findings may enable business leaders to develop a more agile and forward-looking approach to performance management, which is only made possible through these new technologies of data analytics.",Business reporting; Big Data; Business Analytics; Balanced Scorecard; System Thinking; Drivers,"Dutescu, A; Pugna, IB; Stanila, GO",Bucharest University of Economic Studies,2019.0,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON BUSINESS EXCELLENCE,SCIENDO,,10.2478/picbe-2019-0062,20230520-160000,20230521-044735
,wos,Agile Construction of Data Science DSLs (Tool Demo),"Domain Specific Languages (DSLs) have proven useful in the domain of data science, as witnessed by the popularity of SQL. However, implementing and maintaining a DSL incurs a significant effort which limits their utility in context of fast-changing data science frameworks and libraries. We propose an approach and a Python-based library/tool NLDSL which simplifies and streamlines implementation of DSLs modeling pipelines of operations. In particular, syntax description and operation implementation are bundled together as annotated and terse Python functions, which simplifies extending and maintaining a DSL. To support ad hoc DSL elements, NLDSL offers a mechanism to define DSL-level functions as first-class DSL elements. Our tool automatically supports each DSL by code completions and in-editor documentation in a multitude of IDEs implementing the Microsoft's Language Server Protocol. To circumvent the problem of a limited expressiveness of a external DSL, our tool allows embedding DSL statements in the source code comments of a general purpose language and to translate the DSL to such a language during editing. We demonstrate and evaluate our approach and tool by implementing a DSL for data tables which is translated to either Pandas or to PySpark code. A preliminary evaluation shows that this DSL can be defined in a concise and maintainable way, and that it can cover a majority of processing steps of popular Spark/Pandas tutorials.",DSL development; Code generation; Assisted editing and IntelliSense; Data analysis frameworks; Apache Spark; Python Pandas,"Andrzejak, A; Kiefer, K; Costa, DE; Wenz, O",Ruprecht Karls University Heidelberg,2019.0,PROCEEDINGS OF THE 18TH ACM SIGPLAN INTERNATIONAL CONFERENCE ON GENERATIVE PROGRAMMING: CONCEPTS AND EXPERIENCES (GPCE '19),ASSOC COMPUTING MACHINERY,,10.1145/3357765.3359516,20230520-160000,20230521-044735
,wos,Big data analytics capability and co-innovation: An empirical study,"There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.",Business; Economics; Information science; Big data analytics capabilities; Co-innovation; Big data; Co-creation,"Lozada, N; Arias-Perez, J; Perdomo-Charry, G",Universidad de Antioquia,2019.0,HELIYON,ELSEVIER SCI LTD,,10.1016/j.heliyon.2019.e02541,20230520-160000,20230521-044735
,wos,Agile manufacturing: an evolutionary review of practices,"Academics and practitioners have long acknowledged the importance of agile manufacturing and related supply chains in achieving firm sustainable competitiveness. However, limited, if any, research has focused on the evolution of practices within agile manufacturing supply chains and how these are related to competitive performance objectives. To address this gap, we reviewed the literature on an agile manufacturing drawing on the evolution of manufacturing agility, attributes of agile manufacturing, the drivers of agile manufacturing, and the identification of the enabling competencies deployable for agile manufacturing. Our thesis is that agile manufacturing is at the centre of achieving a sustainable competitive advantage, especially in light of current unprecedented market instability coupled with complex customer requirements. In this regard, the emphasis which agile manufacturing places on responsive adaptability would counter the destabilising influence of competitive pressures on organisations performance criteria. We have identified five enabling competencies as the agility enablers and practices of agile manufacturing, that is, transparent customisation, agile supply chains, intelligent automation, total employee empowerment and technology integration, and further explored their joint deployment to create positive multiplier effects. Future research directions were also provided with respect to the operationalisation of the five identified enablers and the potential for emergent technologies of big data, blockchain, and Internet of Things to shape future agile manufacturing practices.",agile manufacturing; supply chain management; practices; competitive advantage,"Gunasekaran, A; Yusuf, YY; Adeleye, EO; Papadopoulos, T; Kovvuri, D; Geyi, DG",California State University System; California State University Bakersfield; University of Central Lancashire; University of Kent,2019.0,INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH,TAYLOR & FRANCIS LTD,,10.1080/00207543.2018.1530478,20230520-160000,20230521-044735
,wos,Towards the Automation of Industrial Data Science: A Meta-learning based Approach,"In context of the fourth industrial revolution (industry 4.0), the industrial big data is subject to grow rapidly to respond the agile industrial computing and manufacturing technologies. This data evolution can be captured using ubiquitous integrated sensors and multiple smart machines. We believe the use of data science methodologies, for the selection of models and configuration of hyper-parameters, may help to better control such data evolution. But, at the same time, the industrial practitioners and researchers often lack machine-learning expertise to directly retrieve the benefit from valuable manufacturing big data. Such a lack poses the major obstacle to yield value from even-though familiar data. In this case, a collaboration with data scientists may become an exigence along with the extensive machine learning knowledge which presumably may result to pursue further delays and effort. Multiple approaches for automating machine learning (AutoML) have been proposed for the past recent years in order to alleviate this deficiency. These approaches are expected to perform better along with accomplishment of computing resources which are mostly not readily accessible. To address this research challenge, in this paper, we propose a meta-learning based approach that may serve an effective decision support system for the AutoML process.",Automated Machine Learning; Manufacturing Big Data; Industry 4.0; Industrial Data Science; Meta-learning,"Garouani, M; Ahmad, A; Bouneffa, M; Lewandowski, A; Bourguin, G; Hamlich, M",Universite du Littoral-Cote-d'Opale; Hassan II University of Casablanca,2021.0,"PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS (ICEIS 2021), VOL 1",SCITEPRESS,,10.5220/0010457107090716,20230520-160000,20230521-044735
,wos,Ambidextrous organization and agility in big data era: The role of business process management systems,"Purpose The purpose of this paper is to explore the effect of big data analytics-capable business process management systems (BDA-capable BPMS) on ambidextrous organizations' agility. In particular, how the functionalities of BDA-capable BPMS may improve organizational dynamism and reactiveness to challenges of Big Data era will be explored. Design/methodology/approach A theoretical analysis of the potential of BDA-capable BPMS in increasing organizational agility, with particular attention to the ambidextrous organizations, has been performed. A conceptual framework was subsequently developed. Next, the proposed conceptual framework was applied in a real-world context. Findings The research proposes a framework highlighting the importance of BDA-capable BPMS in increasing ambidextrous organizations' agility. Moreover, the authors apply the framework to the cases of consumer-goods companies that have included BDA in their processes management. Research limitations/implications The principal limitations are linked to the need to validate quantitatively the proposed framework. Practical implications The value of the proposed framework is related to its potential in helping managers to fully understand and exploit the potentiality of BDA-capable BPMS. Moreover, the implications show some guidelines to ease the implementation of such systems within ambidextrous organizations. Originality/value The research offers a model to interpret the effects of BDA-capable BPMS on ambidextrous organizations' agility. In this way, the research addresses a significant gap by exploring the importance of information systems for ambidextrous organizations' agility.",Information systems; Big data; Agile; Agility; Ambidexterity,"Rialti, R; Marzi, G; Silic, M; Ciappei, C",University of Pisa; University of Florence,2018.0,BUSINESS PROCESS MANAGEMENT JOURNAL,EMERALD GROUP PUBLISHING LTD,,10.1108/BPMJ-07-2017-0210,20230520-160000,20230521-044735
,wos,"Big Data Hysteria, Cognizance and Scope","In real time scenario, every second man and machine have been generating a huge amount of data. Social networking sites like Facebook, tweeter, Instagram, search engine google, yahoo and video shearing websites like YouTube and many real time applications generates enormous quantity of data. These data-sets have different attributes (i.e. volume, velocity, complexity etc.) in it, known as `Big Data'. To manage, process and analyze big data, we require advance hardware platform, software stack and analytics techniques. Big data Analytics emerges as a major application for future data-sets, generating by parallel and distributes systems. This paper has discussed about hype on Big Data, its characteristics, different considerations (i.e. Hardware, Software, Platform, NoSql Data Base, Languages). It has summarized the Techniques of Big Data and light up on scope with other technologies (i.e. IoT, Agile, Lean Six Sigma).",Big Data; Volume; Inherent properties; Big Data Management; Analytics techniques,"Harsh, R; Acharya, G; Chaudhary, S",,2018.0,2018 4TH INTERNATIONAL CONFERENCE FOR CONVERGENCE IN TECHNOLOGY (I2CT),IEEE,,,20230520-160000,20230521-044735
,wos,Panel: Addressing the Shortage of Big Data Skills with Inter-Disciplinary Big Data Curriculum,"One major challenge faced by enterprises in contemporary era is the shortage of big data and analytics (BDA) professionals. These professionals possess skills to analyze and derive intelligence from big data generated by enterprises. Currently, universities and colleges have not been able to produce sufficient professionals to meet the ever-growing demand for data analytics skill sets. A possible reason for this may be the narrow focus and lack of interdisciplinary approach to big data analytics education. To address this challenge, this panel brings together practitioners, researchers and educators in big data and analytics; to explore the potentials for developing an interdisciplinary curriculum that will deliver data analytics skills to students across all other academic majors. This might help to produce more agile data analytics professionals and close the gap between demand and supply of those professionals.",Analytics; Big Data; Education; Curriculum,"Nwokeji, JC; Stachel, R; Holmes, T; Aglan, F; Udenze, EC; Orji, R",Gannon University; Gannon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Dalhousie University,2019.0,2019 IEEE FRONTIERS IN EDUCATION CONFERENCE (FIE 2019),IEEE,,,20230520-160000,20230521-044735
,wos,Decision Making Analysis in Corporate Sectors Using the Concept of Big Data Analytics,"Big data Analytics is one of the most common technology used by all kind of Organisations Worldwide. It has become a huge problem for all the organisations to store and process the massive data gathered. The main function of big data analytics is to collect, store, examine large amount of data to find the unseen patterns and unknown interrelationships according to the organisations requirement. In this internet era, the data is collected continuously and densely from multiple resources in every department of the organisations. The decision makers requires new methods and techniques to understand and analyse the massive data adequately before taking agile decisions. The achievement of the top management in taking strategic decisions depends on the characteristics of the information used. In this paper the role of big data analytics in decision making, theoretical framework of big data analytics, applications of big data analytics and decisions to data quadrants are discussed.",Big data analytics; Decision making; Structured and unstructured data; Structured and unstructured decisions,"Sheshasaayee, A; Bhargavi, K",,2019.0,"INTERNATIONAL CONFERENCE ON INTELLIGENT DATA COMMUNICATION TECHNOLOGIES AND INTERNET OF THINGS, ICICI 2018",SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-030-03146-6_55,20230520-160000,20230521-044735
,wos,A framework to Data Delivery Security for Big Data Annotation Delivery System,"Big data annotation plays an important role in Artificial Intelligence model training. The proliferation of data annotation tasks has brought the issue of security of the big data delivery. This work identifies the security framework associated with encryption and compression procedures that support data delivery safety. In this paper, we propose an agile framework that caters to various types of data under RESTful web services. All the procedures are automatically operated by the server without human intervention. This work assists the company delivers the tagged data products to users with a high-security level avoiding the risk of information disclosure.",Big data; compression; encryption; lightweight framework; data delivery,"Yang, YH; He, HL; Wang, DL; Ding, ZX",,2018.0,2018 IEEE 15TH INTERNATIONAL CONFERENCE ON MOBILE AD HOC AND SENSOR SYSTEMS (MASS),IEEE,,10.1109/MASS.2018.00082,20230520-160000,20230521-044735
,wos,Intelligent ERP for SCM agility and graph theory technique for adaptation in automotive industry in India,"A supply chain's performance is determined by its capacity to stay market-sensitive while maintaining network integration. The challenges in remaining market sensitive are in designing, analyzing, and maintaining a supply chain to its optimum performance where a few strategic aspects of supply chain control the entire processes. With the emergence of the new business era of Big Data Analytics and its interoperability with ERP as an effective Business Intelligence tool I-ERP (Intelligent ERP) that ensures agility of supply chain as one of its primary qualities for expanding market share and maintaining survival is becoming a need in the volatile and complex supply chain network. The emphasis now is on adaptability of ERP with Big Data Analytics such as Machine Learning and Predictive analysis technique in the supply chain in Automotive industry in India by addressing manufacturing/business needs proactively. This paper is to identify the factors that contribute for the supply chain management to remain agile within the automotive industry through empirical data and an attempt has been made to comprehend the ability of SCM to remain agile with the interoperability between Big Data Analytics and ERP through a review of the literature. Identify the challenges in implementing the interoperability and propose technique based on graph theory for future research, IT and supply chain managers consideration.",Supply chain management; Big data analytics; Intelligent-ERP; Agility,"Jayender, P; Kundu, GK",Vellore Institute of Technology (VIT); VIT Vellore,,INTERNATIONAL JOURNAL OF SYSTEM ASSURANCE ENGINEERING AND MANAGEMENT,SPRINGER INDIA,,10.1007/s13198-021-01361-y,20230520-160000,20230521-044735
,wos,Open Data Lake to Support Machine Learning on Arctic Big Data,"The era of big data is evolving with the introduction of the data lake concept. While a data warehouse provides a well-structured model to manage big data, a data lake accepts data of any types and formats with or without schema and provides access to the data for diverse communities of users. A data lake provides flexible, agile, and scalable solution to manage the ever-increasing volume of big data we are witnessing in the world today, including many siloed data collected over the years by researchers through Arctic expeditions. In this paper, we present our conceptual model of a data lake for integrating the diverse huge amount of data collected by researchers during Arctic expedition. We also design a baseline metadata using a data-driven approach to manage the disparately huge structured, semi-structured, and unstructured data collected from the Arctic region. The resulting open data lake not only effectively manages big Arctic data but also supports machine learning on these big data.",big data; data management; data lake; open data; reusability; FAIR principle; CARE principle; Arctic data; Arctic expedition; machine learning; data mining,"Olawoyin, AM; Leung, CK; Cuzzocrea, A",University of Manitoba; University of Calabria,2021.0,2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,10.1109/BigData52589.2021.9671453,20230520-160000,20230521-044735
,wos,The Era of Big Data and Path towards Sustainability,"This paper is an attempt to throw light on the applications of big data analytics in nurturing sustainable develoment through a descriptive tnetadata study. Big data is widely used in many areas such as hospitality, transporataion, health, governance, e-commerce etc. Common applications of big data include consumer profiling, personalised pricing, marketing, advertising and predictive analysis. One of the formidable challanges confronted by the businesses in the contemporary period is to reconcile the conflicting interests of profit maximisation and fostering sustainability. The unprecedented magnitude of data generated within the organisations do have the potential to bring gainful insights for efficient resource utilisation and waste minimisation. The advent of big data aids in making these conflicting interests complementary by providing efficient and precise predicitions. .A number of studies are going on to explore possible options avialable to leverage big data analytics to create social and environmental value. Novel analytical approaches, enormous amounts of data and new technology would help in gaining insights to frame more agile and efficient policies to protect the environment. This paper discusses how big data is applied in different areas to foster sustainability.",Big data; Sustainability; Gartner's model,"Victor, V; Maria, FF",Hungarian University of Agriculture & Life Sciences,2018.0,"INNOVATION MANAGEMENT AND EDUCATION EXCELLENCE THROUGH VISION 2020, VOLS I -XI",INT BUSINESS INFORMATION MANAGEMENT ASSOC-IBIMA,,,20230520-160000,20230521-044735
,wos,The future of Artificial Intelligence for the BioTech Big Data landscape,"Recent Industry 4.0 advancements are making available massive amounts of data for the development of innovative BioTech solutions. However, several challenges need to be overcome to correctly use data and novel, non-pharma technologies to greatly speed up discovery, optimization and market delivery of products, and services. In this review, we bring your attention to the important aspects of Big Data and Artificial Intelligence (Al) that have an impact on the future of the field and briefly touch upon how disciplines such as Hyper-Automation, Infrastructure as Code (laC) and DevOps - a set of practices that combines software development with Information Technology (IT) operations (Ops) - can accelerate Big Data and Al adoption in your Agile Digital Transformation journey.",,"Artico, F; Edge, AL; Langham, K",GlaxoSmithKline; AstraZeneca; Medimmune,2022.0,CURRENT OPINION IN BIOTECHNOLOGY,ELSEVIER SCI LTD,,10.1016/j.copbio.2022.102714,20230520-160000,20230521-044735
,wos,Batch-based agile program management approach for coordinating IT multi-project concurrent development,"Software development projects have undergone remarkable changes with the arrival of agile development approaches. Many firms are facing a need to use these approaches to manage entities consisting of multiple projects (i.e. programs) simultaneously and efficiently. New technologies such as big data provide a huge power and rich demand for the IT application system of the commercial bank which has the characteristics of multiple sub-projects, strong inter-project correlation, and numerous project participating teams. Hence, taking the IT program management of a bank in China as a case, we explore the methods to solve the problems in multi-project concurrent development practice through integrating the ideas of program and batch management. First, to coordinate the multi-project development process, this paper presents the batch-based agile program management approach that synthesizes concurrent engineering with agile methods. And we compare the application of batch management between software development projects and manufacturing process. Further, we analyze the concurrent multi-project development practice in the batch-based agile program management, including the overlapping between stages, individual project's activities, and multiple projects based on common resources and environment to stimulate the knowledge transfer. Third, to facilitate the communication and coordination of batch-based program management, we present the double-level responsibility organizational structure of batch management.",software development project; program management; batch management; coordination; multi-project management; banking industry,"Yang, Q; Bi, YX; Wang, QR; Yao, T",University of Science & Technology Beijing,2021.0,CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS,SAGE PUBLICATIONS LTD,,10.1177/1063293X211015236,20230520-160000,20230521-044735
,wos,What Are the Critical Success Factors for Agile Analytics Projects?,"To get value from BI (Business Intelligence) and Big Data initiatives, organizations need to develop the capability to successfully execute their analytics projects. Via updating Chow and Cao's list of 12 success factors for agile projects, 43 attributes of these potential critical success factors (CSFs) were identified. Data from four case studies of analytics projects suggest that the critical success factors for analytics projects may be Strong Customer Involvement and a Methodical Project Definition Process.",Analytics projects; agile projects; agile project management; project success factors,"Tsoy, M; Staples, DS",Queens University - Canada,2021.0,INFORMATION SYSTEMS MANAGEMENT,TAYLOR & FRANCIS INC,,10.1080/10580530.2020.1818899,20230520-160000,20230521-044735
,wos,Discover the Digital Technology Application in Fashion Business Models,"In the last few decades, there has been a surge of interest in the development of fashion business models to assist fashion companies in reducing cost and to efficiently manage the business processes. These business models are developed to manage the internal operations within the company through adopting complex formulae and algorithms to reduce waste at each procedure. However in today's fashion business market, global sourcing and global corporation are much more important than before. The relationship between the fashion company and its suppliers, the relationship between the fashion company and its customers, and the management of these relationships: all of them are critical components of an effective business strategy. The competition between fashion companies is no longer on a company level but instead is subjected to supply chain versus supply chain. Trying to take the massive information into consideration by using traditional digital technology is not a wise decision when developing business strategy. On further thinking, the information flow through the supply chain has the same characteristics 5Vs as big data: Volume, Velocity, Variety, Value and Veracity. In other words, the management of information flow in supply chain is the management of big data. There is no doubt that digital technology under the big data environment will fundamentally change the whole supply chain. The first objective of this paper is to identify the key weakness of lean and agile logistic supply chain models in literature. The second objective is to point out the technological challenges in developing Tomorrow's models to build an agile response with a lean platform: How to set up virtual networks from the early designing stage to the last consuming and feedback stage? How to set up the information standardization and synchronisation process in the system? How to specify the consumer requirements in fitting effects and functional performance of garments? The last objective is to discover the digital technology under big data environment which will enable the information to efficiently flow through the whole supply chain.",Business Model; Supply Chain Logistic; Big Data; Information Flow Management; Digital Technology,"Liu, ZC; Li, Y; Wang, YY",N8 Research Partnership; RLUK- Research Libraries UK; University of Manchester,2018.0,"TEXTILE BIOENGINEERING AND INFORMATICS SYMPOSIUM (TBIS) PROCEEDINGS, 2018",TEXTILE BIOENGINEERING & INFORMATICS SOCIETY LTD,,,20230520-160000,20230521-044735
,wos,Guest Editorial Special Issue on Big Data and Computational Intelligence for Agile Wireless IoT,,,"Wu, CLMG; Jin, YC; Li, J; Yau, KLA; Qadir, J",University of Electro-Communications - Japan; University of Surrey; Shanghai Jiao Tong University; Sunway University,2020.0,IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,,10.1109/TETCI.2020.2990757,20230520-160000,20230521-044735
,wos,Agile Lossless Compression Algorithm for Big Data of Solar Energy Harvesting Wireless Sensor Network,"Time series data are collected through most of the applications that permeate our lives today. Internet of Things (IoT) sensor data are generated through smart applications and stored in databases. Time series databases require huge storage spaces, as over time they consume a large amount of memory. In this paper, we propose an enhanced compression algorithm for time series data generated by IoT systems that monitor the production of electrical energy by solar panels. The best way to ensure that solar energy systems have high efficiency is to continuously monitor all electrical and environmental factors. However, this requires the collection of enormous quantities of data that can be used to detect defects in the generation of electric energy or in solar panels. As the data must be available for analysis, a lossless compression algorithm is needed. In addition, the compressed data must be in a format that can be queried to perform analysis operations dependent on speed; this means that the decompression of data should not be time-consuming. Our results showed the high speed of the compression process along with good compression rate (16.6%) after applying the proposed compression algorithm.",lossless compression algorithm; solar energy; compressed time series data; wireless sensor network,"El-Hageen, HM; Albalawi, H; Alatwi, AM; Abd Elrahman, WR; Faqeh, STM",University of Tabuk; University of Tabuk; Egyptian Knowledge Bank (EKB); Egyptian Atomic Energy Authority (EAEA); University of Tabuk; University of Tabuk,2022.0,SENSORS AND MATERIALS,"MYU, SCIENTIFIC PUBLISHING DIVISION",,10.18494/SAM4106,20230520-160000,20230521-044735
,wos,Smart hospitality-Interconnectivity and interoperability towards an ecosystem,"The Internet and cloud computing changed the way business operate. Standardised web-based applications simplify data interchange which allow internal applications and business partners systems to become interconnected and interoperable. This study conceptualises the smart and agile hospitality enterprises of the future, and proposes a smart hospitality ecosystem that adds value to all stakeholders. Internal data from applications among all stakeholders, consolidated with external environment context form the hospitality big data on the cloud that enables members to use business intelligence analysis to generate scenarios that enhance revenue management performance. By connecting to smart tourism network, sensors and content extractors can assist to collect external information, and beacons to deliver context-based promotion messages and add value. The proposed model enables fully integrated applications, using big data to enhance hospitality decision making as well as strengthen competitiveness and improve strategies performance.",Smart hospitality; Interconnectivity and interoperability; Hospitality ecosystem; ICT; Big data; Sensor and beacon,"Buhalis, D; Leung, R",Bournemouth University; I Shou University,2018.0,INTERNATIONAL JOURNAL OF HOSPITALITY MANAGEMENT,ELSEVIER SCI LTD,,10.1016/j.ijhm.2017.11.011,20230520-160000,20230521-044735
,wos,Discovering and merging related analytic datasets,"The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues. (C) 2020 Elsevier Ltd. All rights reserved.",Schema augmentation; Schema complement; Data quality; SAP HANA,"Liu, RT; Simon, E; Amann, B; Gancarski, S",Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite,2020.0,INFORMATION SYSTEMS,PERGAMON-ELSEVIER SCIENCE LTD,,10.1016/j.is.2020.101495,20230520-160000,20230521-044735
,wos,Unlocking the Potential of NextGen Multi-Model Databases for Semantic Big Data Projects,"A new vision in semantic big data processing is to create enterprise data hubs, with a 360 degrees view on all data that matters to a corporation. As we discuss in this paper, a new generation of multi-model database systems seems a promising architectural choice for building such scalable, non-native triple stores. In this paper, we first characterize this new generation of multi-model databases. Then, discussing an example scenario, we show how they allow for agile and flexible schema management, spanning a large design space for creative and incremental data modelling. We identify the challenge of generating sound triple-views from data stored in several, interlinked models, for SPARQL querying. We regard this as one of several appealing research challenges where the semantic big data and the database architecture community may join forces.",Semantic data management; schema evolution; multi-model DBMS,"Holubova, I; Scherzinger, S",Charles University Prague,2019.0,PROCEEDINGS OF THE INTERNATIONAL WORKSHOP ON SEMANTIC BIG DATA (SBD 2019),ASSOC COMPUTING MACHINERY,,10.1145/3323878.3325807,20230520-160000,20230521-044735
,wos,"Towards DesignOps Design Development, Delivery and Operations for the AECO Industry","The overwhelming success of companies build on top of cloud computing technologies has been driven by their ability to create systems for processing big data at scale and designing high-quality digital products as well as being agile and capable of handling constant changes in the market. This runs somewhat contrary to the AECO industry, which generates an abundance of multidisciplinary data and faces numerous design challenges but is not as prone to agile management. The entire methodology for designing and delivering projects has historically been oriented toward getting all requirements defined and specified in advance. In that context, change of the workflow in AECO is often seen as an exception. Not only this is far from the paradigm or principles of today's business technologies, but today's enterprises are characterized by an opposing set of values. Latest software engineering methodologies, like DevOps and its design incarnation - DesignOps were created solely to tackle those issues in the IT industry. This paperwill present how those methodologies could be successfully implemented in the AECO industry and increase the efficiency of existing design pipelines. We demonstrate a prototype of a software platform, an entire automated ecosystem where design operations are made in the cloud by a collection of automatic or semi-automatic microservices and where data flows seamlessly between various disciplines. The system leverages the potential of distributed computing, performance-driven design, evolutionary optimization, big data, and modern web design.",DesignOps; DevOps; Cloud computing; Performance design; Optimization; High performance computing,"Kosicki, M; Tsiliakos, M; ElAshry, K; Borgstrom, O; Rod, A; Tarabishy, S; Nguyen, C; Davis, A; Tsigkari, M",,2023.0,TOWARDS RADICAL REGENERATION,SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-031-13249-0_6,20230520-160000,20230521-044735
,wos,manufacturing and Industry 4.0 combinative application: Practices and perceived benefits,"This paper investigates the industry practices regarding the combinative use of Industry 4.0 and lean tools in the manufacturing sector. Following review of the literature, a questionnaire survey was distributed among manufacturing professionals in organizations which have already adopted Industry 4.0 technology and lean manufacturing, with the aim to highlight the popular combinations of tools as seen in manufacturing practice and capture the perceived level of their contribution to operational performance. The survey results show that Real time data, IoT for data exchange, big data analytics, Cyber Physical Systems (CPS), predictive algorithms and robots are among the most popular 14.0 applications used to support lean attributes like continuous flow, Kanban, standardised work, TPM and continuous improvement. It also emerges that although the beneficial impact of lean production across the respondents' organizations is widely accepted, the perceived impact of Industry 4.0 tools is not as clear. Copyright (C) 2021 The Authors.",Lean production; Industry 4.0; production management; information technology; decision making,"Marinelli, M; Deshmukh, AA; Janardhanan, M; Nielsen, I",RLUK- Research Libraries UK; University of Leicester; Aalborg University,2021.0,IFAC PAPERSONLINE,ELSEVIER,,10.1016/j.ifacol.2021.08.034,20230520-160000,20230521-044735
,wos,IoT Agile Framework Enhancement,"Internet of Things (IoT) is considered as a trend nowadays. Devices connected to the internet interact with surrounding; this poses strong challenges in handling big data with a certain level of security. In this paper IoT devices will be divided in to two categories high vulnerability devices and low vulnerability devices. The classification depends on the ease of attacks. In order to ensure the security of IoT devices, an agile approach is used to secure high vulnerability devices as first step and then low vulnerability devices by applying encryption algorithms.",Internet of Things; Agile approach; Encryption algorithms,"Gabr, B; Azer, MA",Egyptian Knowledge Bank (EKB); Nile University; Egyptian Knowledge Bank (EKB); Nile University,2018.0,2018 1ST INTERNATIONAL CONFERENCE ON COMPUTER APPLICATIONS & INFORMATION SECURITY (ICCAIS' 2018),IEEE,,,20230520-160000,20230521-044735
,wos,Understanding how the Ad Hoc use of Big Data Analytics Impacts Agility: A Sensemaking-based Model,"As business environments become increasingly complex and turbulent, organizations are required to be more agile. Use of big data analytics (BDA) can be a differentiator for organizations seeking to improve agility to quickly sense and respond to novel and complex events. Usage of BDA comprises two types: the routine use and the ad hoc use. The latter is more associated with the unplanned analysis of big data to understand unexpected events, and its effects have not been studied in distinction to the former in the analytics literature. We draw on sensemaking, the organizational theory of the process of understanding novel and complex events, to investigate how the ad hoc use of BDA improves agility of organizations. Analysis of a survey of 107 business executives and senior managers demonstrated the positive effects of the ad hoc use of BDA on agility, through mediation by sensemaking.",big data; analytics; ad hoc use; sensemaking; agility,"Hosoya, R; Kamioka, T",Hitotsubashi University; Hitotsubashi University,2018.0,"2018 INTERNATIONAL CONFERENCE ON ADVANCES IN BIG DATA, COMPUTING AND DATA COMMUNICATION SYSTEMS (ICABCD)",IEEE,,,20230520-160000,20230521-044735
,wos,Modelling the relationship of digital technologies with lean and agile strategies,"As the world becomes globalised, companies fight for survival by connecting their in-house processes with external suppliers/customers. To remain competitive, companies must integrate innovative capabilities like 'industry-4.0 technologies' with their operation and supply chain (SC) strategies. The integration of various strategies has been investigated with the associated effect on performance; however, studies on how industry 4.0 technologies might support integrated strategies are still incipient. This work investigates the hierarchical relationships of 'industry 4.0 technologies' with lean and agile strategies. Adopting the 'Interpretive Structural Modelling (ISM)' technique to present a model depicting the linkage, the work also classifies the technologies and practices according to their 'driving' and 'dependency' powers. The findings revealed that the technologies have a high affinity to enable the implementation of lean and agile strategies. Among the nine technologies included in the study, 'Cyber-Physical-System', 'Internet-of-Things', 'Cloud-Computing', and 'Big-Data-Analytics' have the highest driving powers, signifying their higher affinity with the practices. Meanwhile, all the practices have a high enough affinity to be influenced by the technologies, except for a few (3/16 of lean and 2/9 of agile) that possess affinities too low to be driven by these technologies. The theoretical and managerial impacts of the research are also emphasised.",Lean SCM; agile SCM; digital technologies; interpretive structural modelling; industry 4; 0,"Raji, IO; Shevtshenko, E; Rossi, T; Strozzi, F",Universita Carlo Cattaneo - Liuc,2021.0,SUPPLY CHAIN FORUM,TAYLOR & FRANCIS LTD,,10.1080/16258312.2021.1925583,20230520-160000,20230521-044735
,wos,Exploring the Digital Transformation Based on Big Data with Ubiquitous Internet of Everything,"Digital technologies present both game-changing opportunities for and existential threats to companies. Digital services in consumer-facing organizations offer novelty value propositions, closer consumer relationships and higher automation of consumer-facing processes. Facing big digital data streams generated by ubiquitous Internet of Everything(IoE) and savvy customers with mobile computing and social media, this paper focuses on digital transformation journeys seeking digital capabilities and digital leadership to upgrade organizational performance, one is discovering big data value, the other is dual methods with agile. The finding provides practical implications that can help guide practitioners in digital transformation.",Internet of Everything(IoE); big data; analytics capabilities,"Wang, XX",Beijing Jiaotong University,2020.0,PROCEEDINGS OF NINETEENTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS,UNIV CALGARY PRESS,,,20230520-160000,20230521-044735
,wos,An Overview of Fashion Business Models in Big Data Environment,"The competitive pressure in the fashion industry not only exist between companies, they also exist between the networks of linked partners (known as supply chains). Consumers' needs are changing faster than ever before. Textile and garment manufactures are forced to lower the production costs and increase the efficiency so that they can respond quickly when there is a new trend in the market. Big data (BD) has been a hot topic for the last decade; this concept is mainly about extracting valuable information from voluminous data. In the fashion business, big data is playing a more and more important role in trend forecasting, consumer behavior studying etc. This paper reviews the history of conducting business in different developing stages of information technology. The basic garment supply chain in the traditional business era and in e-business era, as well as the development in fashion industry has been critically viewed.",Fast Fashion; Supply Chain Management; Agile; Fashion Management; Fashion Big Data,"Wang, YY; Li, Y; Perry, P; Liu, ZC",N8 Research Partnership; RLUK- Research Libraries UK; University of Manchester,2018.0,"TEXTILE BIOENGINEERING AND INFORMATICS SYMPOSIUM (TBIS) PROCEEDINGS, 2018",TEXTILE BIOENGINEERING & INFORMATICS SOCIETY LTD,,,20230520-160000,20230521-044735
,wos,"Work-in-progress: Data Science Challenge-X: self-directed, competence-based, project-based learning","We discuss in this paper the implementation of a project-based self-direct learning competency-based project module in our Bachelor Data Science programme. The goal of the course is to integrate in a later stage all project modules, which are now divided in two: one with and one without external industry partners, treating different aspects of data science with a pre-defined goal and clear objectives for the project. Switching for a competency-based learner-based paradigm with agile aspects and intrinsic focus, we define the core project goals as secondary and develop core data science competences which are acquired by the students and reflected in a learning journal.",project-based learning; intrinsic motivation; self-directed learning,"Benites, F; Schlatter, M; Messerli, M; Custer, R",FHNW University of Applied Sciences & Arts Northwestern Switzerland,2022.0,PROCEEDINGS OF THE 2022 IEEE GLOBAL ENGINEERING EDUCATION CONFERENCE (EDUCON 2022),IEEE,,10.1109/EDUCON52537.2022.9766710,20230520-160000,20230521-044735
,wos,Advanced Customer Analytics: Strategic Value Through Integration of Relationship-Oriented Big Data,"As more firms adopt big data analytics to better understand their customers and differentiate their offerings from competitors, it becomes increasingly difficult to generate strategic value from isolated and unfocused ad hoc initiatives. To attain sustainable competitive advantage from big data, firms must achieve agility in combining rich data across the organization to deploy analytics that sense and respond to customers in a dynamic environment. A key challenge in achieving this agility lies in the identification, collection, and integration of data across functional silos both within and outside the organization. Because it is infeasible to systematically integrate all available data, managers need guidance in finding which data can provide valuable and actionable insights about customers. Leveraging relationship marketing theory, we develop a framework for identifying and evaluating various sources of big data in order to create a value-justified data infrastructure that enables focused and agile deployment of advanced customer analytics. Such analytics move beyond siloed transactional customer analytics approaches of the past and incorporate a variety of rich, relationship-oriented constructs to provide actionable and valuable insights. We develop a customized kernel-based learning method to take advantage of these rich constructs and instantiate the framework in a novel prototype system that accurately predicts a variety of customer behaviors in a challenging environment, demonstrating the framework's ability to drive significant value.",big data; customer acquisition; customer analytics; customer expansion; data integration; data management; design science; IT strategic value; relationship marketing; customer retention,"Kitchens, B; Dobolyi, D; Li, JJ; Abbasi, A",University of Virginia; University of Virginia; University of Virginia,2018.0,JOURNAL OF MANAGEMENT INFORMATION SYSTEMS,"ROUTLEDGE JOURNALS, TAYLOR & FRANCIS LTD",,10.1080/07421222.2018.1451957,20230520-160000,20230521-044735
,wos,Why we need a small data paradigm,"BackgroundThere is great interest in and excitement about the concept of personalized or precision medicine and, in particular, advancing this vision via various big data' efforts. While these methods are necessary, they are insufficient to achieve the full personalized medicine promise. A rigorous, complementary small data' paradigm that can function both autonomously from and in collaboration with big data is also needed. By small data' we build on Estrin's formulation and refer to the rigorous use of data by and for a specific N-of-1 unit (i.e., a single person, clinic, hospital, healthcare system, community, city, etc.) to facilitate improved individual-level description, prediction and, ultimately, control for that specific unit.Main bodyThe purpose of this piece is to articulate why a small data paradigm is needed and is valuable in itself, and to provide initial directions for future work that can advance study designs and data analytic techniques for a small data approach to precision health. Scientifically, the central value of a small data approach is that it can uniquely manage complex, dynamic, multi-causal, idiosyncratically manifesting phenomena, such as chronic diseases, in comparison to big data. Beyond this, a small data approach better aligns the goals of science and practice, which can result in more rapid agile learning with less data. There is also, feasibly, a unique pathway towards transportable knowledge from a small data approach, which is complementary to a big data approach. Future work should (1) further refine appropriate methods for a small data approach; (2) advance strategies for better integrating a small data approach into real-world practices; and (3) advance ways of actively integrating the strengths and limitations from both small and big data approaches into a unified scientific knowledge base that is linked via a robust science of causality.ConclusionSmall data is valuable in its own right. That said, small and big data paradigms can and should be combined via a foundational science of causality. With these approaches combined, the vision of precision health can be achieved.",Precision medicine; Personalized medicine; Precision health; Small data; Artificial intelligence; Data science,"Hekler, EB; Klasnja, P; Chevance, G; Golaszewski, NM; Lewis, D; Sim, I",University of California System; University of California San Diego; University of California System; University of California San Diego; University of Michigan System; University of Michigan; University of California System; University of California San Francisco,2019.0,BMC MEDICINE,BMC,,10.1186/s12916-019-1366-x,20230520-160000,20230521-044735
,wos,Hacking marketing: how do firms develop marketers' expertise and practices in a digital era?,"PurposeDigital technologies, digitalised consumers and the torrent of customer data have been transforming marketing practice. In discussing such trends, existing research has either focussed on the skills marketers need or broad-based approaches such as agile methods but has given less consideration to just how such skills or approaches might be developed and used in marketers' day-to-day activities and in the organisation of marketing in the firm. This is what the authors address in this paper.Design/methodology/approachThis paper adopts an in-depth case study approach to examine an exemplary digital enterprise in transformation of their digital marketing. The insights were gathered from 25 interviews, netnography and document analysis of the case organisation in addition to 10 interviews with independent experts.FindingsDrawing on practice-oriented approach, the authors show how organisations respond to the emerging trends of digital consumers and big data by taking a 'hacking marketing' approach and developing novel marketing expertise at disciplinary boundaries. The authors put forward three sets of practices that enable and shape the hacking marketing approach. These include spanning the expertise boundary, making value measurable and experimenting through which their adaptive, iterative and multidisciplinary work occurs. This explains how managing digital consumers and big data is not within the realm of information technology (IT) functions but marketing and how marketing professionals are changing their practice and moving their disciplinary boundaries.Practical implicationsThis study offers practical contributions for firms in terms of identifying new work practices and expertise that marketing specialists need in managing digital platforms, digitalised consumers and big data. This study's results show that enterprises need to design and implement strong training programmes to prepare their marketing workforce in adopting experimentations of agile approach and data-driven decision making. In addition, Marketing education should be changed so that programmes consider a review of their courses and include the novel marketing models and approaches into their curriculum.Originality/valueThis study contributes to the nascent discussions by unpacking how enterprises can develop new marketing expertise and practices beyond skillsets and how such practices form new hacking marketing approach which addresses the problem of the inability of the conventional marketing approach to show its value within the firm.",Digitalised consumers; Practice; Expertise; Digital marketing; Digital organisation; Occupations; Hacking,"Hafezieh, N; Pollock, N; Ryan, A",RLUK- Research Libraries UK; University of London; Royal Holloway University London; RLUK- Research Libraries UK; University of Edinburgh; University of Limerick,2023.0,JOURNAL OF ENTERPRISE INFORMATION MANAGEMENT,EMERALD GROUP PUBLISHING LTD,,10.1108/JEIM-12-2021-0530,20230520-160000,20230521-044735
,wos,Automotive Big Data Pipeline: Disaggregated Hyper-Converged Infrastructure vs Hyper-Converged Infrastructure,"Big data disrupts everything it touches, but automotive is probably one of the top industries that enjoy and leverage the benefits. The Automotive Big Data Pipeline (ABDP) is a big data pipeline base on the automotive use case and is required to scale up agile and high performance in real-time or in batch. Nonetheless, there're many alternative infrastructure designs but lack of knowledge, which fits the best for the automotive domain. It leads this paper into a question: What kinds of infrastructure design could provide better performance for the ABDP? In this paper, we introduce two well-known infrastructure designs called Hyper-Converged infrastructure (HCI) and Disaggregated Hyper-Converged infrastructure (DHCI). HCI combines standard data center hardware using locally attached storage resources to create fast, common building blocks. However, does single standard hardware fit all the requirements? DHCI scale independently from compute and storage provides an option. It provides a more cost-efficient and flexible solution; however, there is no comparison from the performance point of view. Therefore, to address it, our objective is to conduct an empirical performance comparison to see which one performs better. The experiment result shows that DHCI performs almost the same as HCI on CPU utilization, memory, and network consumption. However, regarding storage and running time metrics, DHCI performs slightly higher storage throughput, IOPs, and less running time than HCI.",,"Wang, C; Kim, B",,2020.0,2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,10.1109/BigData50022.2020.9378045,20230520-160000,20230521-044735
,wos,Towards Microservice Identification Approaches for Architecting Data Science Workflows,"In order to support fast development cycles and deploying software components in productive environments, there are three crucial trends in data science. These are agile process models, development of many technologies and increasing usage of cloud platforms. Therefore, effective architectures are needed to support this trend in data science context. This paper explores and evaluates first approaches for, why and how microservice architecture style can support fast development cycles for data science workflows. Microservices are becoming a popular architectural style for designing modern applications due to several advantages like scalability, reliability and maintainability. First, this paper points out the research gap on why microservices could be a suitable way to design data science workflows. Second, it defines relevant research questions for future research that addresses challenges of the microservice architectural style in the data science context. An essential prerequisite for this architecture style is to identify the right context of a microservice for data science workflows. (C) 2021 The Authors. Published by Elsevier B.V.",Microservice; Microservice Identification; Data Science Workflows; Software Architecture,"Schroer, C",Volkswagen; Carl von Ossietzky Universitat Oldenburg,2021.0,INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS / INTERNATIONAL CONFERENCE ON PROJECT MANAGEMENT / INTERNATIONAL CONFERENCE ON HEALTH AND SOCIAL CARE INFORMATION SYSTEMS AND TECHNOLOGIES 2020 (CENTERIS/PROJMAN/HCIST 2020),ELSEVIER SCIENCE BV,,10.1016/j.procs.2021.01.198,20230520-160000,20230521-044735
,wos,A Methodology to Manage Structured and Semi-structured Data in Knowledge Oriented Graph,"Data has become fundamental to every business process and research area like never before. To date, one of the main open points of research activities is to manage the data acquired in the field by sensors, logs etc. by modeling the data structures according to the analyzes that will be carried out. In fact, with the advent of Big Data, the need to have a single reference data structure has been reduced, but with modern architectures there is a tendency to generate specific and optimized data structures for the analyzes that will be carried out. In this work we propose an agile data modeling methodology guided by analytics focused on the management of structured and semi-structured data sources.",Events graphs; Graph rewriting; Data management,"Bellandi, V; Ceravolo, P; D'Andrea, GA; Maghool, S; Siccardi, S",University of Milan,2022.0,"ENGINEERING APPLICATIONS OF NEURAL NETWORKS, EAAAI/EANN 2022",SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-031-08223-8_18,20230520-160000,20230521-044735
,wos,Importance of Project Management in Business Analytics: Academia and Real World,"Project management constitutes a powerful lever as organizations face increasing pressure to manage projects to budget, on time, and deliver more insights, in less time and with rapidly increasing amounts of data. This is critical especially in business analytics, with more than75% of organizations planning big data investments over the next several years. But the manipulation of massive amounts of data presents challenges - budgetary, time constraints, execution, proper manager skillsets, and such like. These challenges have cramped recent project rollouts, as only 37% of organizations have deployed big data projects in the past year; this suggests that filling the gap between data and insight remains a substantial hurdle as well as evolving need of project management for such projects. This chapter offers real-world examples of how project management professionals tackle big data challenges in a rapidly evolving, data-rich environment. Simultaneously, it establishes a bridge between business and academia as they both recognize the joint necessity to develop highly trained project managers to utilize the powerful and cutting edge analytical tools available to create value.",Analytics; Project management; Business analytics; Data science; Business intelligence; Agile methods,"Shah, S; Gochtovtt, A; Baldini, G",Drexel University,2019.0,ALIGNING BUSINESS STRATEGIES AND ANALYTICS: BRIDGING BETWEEN THEORY AND PRACTICE,SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-319-93299-6_6,20230520-160000,20230521-044735
,wos,Machine Learning-based Estimation of Story Points in Agile Development: Industrial Experience and Lessons Learned,"Estimating story points is an important activity in agile software engineering. Story-point estimation enables software development teams to, among other things, better scope products, prioritize requirements, allocate resources and measure progress. Several machine learning techniques have been proposed for automated story-point estimation. However, most of these techniques use open-source projects for evaluation. There are important differences between open-source and commercial projects with respect to story authoring. The goal of this paper is to evaluate a state-of-the-art machine learning technique, known as Deep-SE [3], for estimating story points in a commercial project. Our dataset is comprised of 4,727 stories for a data anonymization product developed by a 27-member agile team at a healthcare data science company, IQVIA. Over this dataset, Deep-SE achieved a mean absolute error of 1.46, significantly better than three different baselines. Model performance nonetheless varied across stories, with the estimation error being larger for stories that had higher points. Our results further indicate that model performance is correlated with certain story characteristics such as the level of detail and the frequency of vague terms in the stories. An important take-away from our study is that, before organizations attempt to introduce machine learning-based estimation into agile development, they need to better embrace agile best practices, particularly in relation to story authoring and expert-based estimation.",Agile Development; Story-point Estimation; Machine Learning,"Abadeer, M; Sabetzadeh, M",University of Ottawa,2021.0,29TH IEEE INTERNATIONAL REQUIREMENTS ENGINEERING CONFERENCE WORKSHOPS (REW 2021),IEEE COMPUTER SOC,,10.1109/REW53955.2021.00022,20230520-160000,20230521-044735
,wos,Interdisciplinarity in Data Science Pedagogy: A Foundational Design,"Data science is an interdisciplinary field that generates insights in data to aid decision-making. Recognizing that data scientists must be interdisciplinary, agile, and able to adapt to data analysis across many domains, both academia and the industry are striving to integrate interdisciplinary learning and transferable skills into data science curriculum. This paper introduces an interdisciplinary approach to teaching the foundations of data science. We evaluate two different interdisciplinary formats. The first format considers collaborative efforts among instructors with different academic disciplines. The second involves a sole instructor that discusses data science concepts from different disciplines and related to business processes, computer science, and programming. We demonstrate that interdisciplinarity ensures favorable learning experiences and produces high learning outcomes. We also show that our course design maintains and promotes interdisciplinarity even in situations where logistical constraints would not support the use of multiple instructors to deliver one course.",Data science; interdisciplinarity; pedagogy; analytics; curriculum design,"Asamoah, DA; Doran, D; Schiller, S",University System of Ohio; Wright State University Dayton,2020.0,JOURNAL OF COMPUTER INFORMATION SYSTEMS,TAYLOR & FRANCIS INC,,10.1080/08874417.2018.1496803,20230520-160000,20230521-044735
,wos,The concept and competitiveness of agile organization in the fourth industrial revolution's drift,"Corporate competitiveness is constantly being shaped by the Fourth Industrial Revolution, the explosive development of technology, the globalization and the hyper-competition. The VUCA status has now become a permanent reality: volatility and complexity cannot be traced to traditional corporate operations. The Industry 4.0 projects a physical, a digital and a biological megatrend such as advanced robotics, artificial intelligence, new materials, personalized healing, self-driving cars. Through usage of the resources and knowledge sharing, the global economy is experiencing mutations such as the sharing economy, the peer to peer economy, the gig economy in the labor market and the Big Data in planning. Meanwhile, the disruptive innovations are transforming industries and gaining exponentially competitive advantage. The special business concepts were born and whom cannot be handled by models of classic macro and micro economics: the largest taxi company in the world does not own any taxicab (Uber), the largest accommodation company does not own any property (Airbnb), the largest telecommunication company has no infrastructure (Skype), the world's most valuable retailer has no inventory (Alibaba), the most popular media doesn't create its own content (Facebook), the world's largest cinema doesn't have its own movie (Netflix). In the meantime, those are treasuring huge profits, business influence and information capital. The competitiveness of their agile way of working can be proved. These call for changes not only in the market, but also in organizational and individual terms. An adaptive corporate structure and leadership, a self-organizing group, an agile working method hold companies in the direction of growing track and changes in the future. My research about discovering some aspects of agile way of working versus traditional organization work. My hypothesis is that employees are more motivated, effective and committed in an agile team than in a classic hierarchy or matrix. I added own business and project-based worker as their flexible, effective work is a must. My hypotheses are partially fulfilled.",agile; Industry 4.0; VUCA; sharing economy; competitiveness,"Balog, K",Hungarian University of Agriculture & Life Sciences,2020.0,STRATEGIC MANAGEMENT,"UNIV NOVI SAD, FAC ECONOMICS SUBOTICA",,10.5937/StraMan2003014B,20230520-160000,20230521-044735
,wos,A survey study of success factors in data science projects,"In recent years, the data science community has pursued excellence and made significant research efforts to develop advanced analytics, focusing on solving technical problems at the expense of organizational and socio-technical challenges. According to previous surveys on the state of data science project management, there is a significant gap between technical and organizational processes. In this article we present new empirical data from a survey to 237 data science professionals on the use of project management methodologies for data science. We provide additional profiling of the survey respondents' roles and their priorities when executing data science projects. Based on this survey study, the main findings are: (1) Agile data science lifecycle is the most widely used framework, but only 25% of the survey participants state to follow a data science project methodology. (2) The most important success factors are precisely describing stakeholders' needs, communicating the results to end-users, and team collaboration and coordination. (3) Professionals who adhere to a project methodology place greater emphasis on the project's potential risks and pitfalls, version control, the deployment pipeline to production, and data security and privacy.",data science; survey; project management; factor analysis; success factors,"Martinez, I; Viles, E; Olaizola, IG",University of Navarra,2021.0,2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,10.1109/BigData52589.2021.9671588,20230520-160000,20230521-044735
,wos,"Artificial Intelligence in Surveillance, Diagnosis, Drug Discovery and Vaccine Development against COVID-19","As of August 6th, 2021, the World Health Organization has notified 200.8 million laboratory-confirmed infections and 4.26 million deaths from COVID-19, making it the worst pandemic since the 1918 flu. The main challenges in mitigating COVID-19 are effective vaccination, treatment, and agile containment strategies. In this review, we focus on the potential of Artificial Intelligence (AI) in COVID-19 surveillance, diagnosis, outcome prediction, drug discovery and vaccine development. With the help of big data, AI tries to mimic the cognitive capabilities of a human brain, such as problem-solving and learning abilities. Machine Learning (ML), a subset of AI, holds special promise for solving problems based on experiences gained from the curated data. Advances in AI methods have created an unprecedented opportunity for building agile surveillance systems using the deluge of real-time data generated within a short span of time. During the COVID-19 pandemic, many reports have discussed the utility of AI approaches in prioritization, delivery, surveillance, and supply chain of drugs, vaccines, and non-pharmaceutical interventions. This review will discuss the clinical utility of AI-based models and will also discuss limitations and challenges faced by AI systems, such as model generalizability, explainability, and trust as pillars for real-life deployment in healthcare.",COVID-19; machine learning; artificial intelligence; drug discovery; SARS-CoV-2; pandemic; diagnosis; prediction; surveillance; vaccine,"Arora, G; Joshi, J; Mandal, RS; Shrivastava, N; Virmani, R; Sethi, T",Yale University; Cleveland Clinic Foundation; University of Pennsylvania; Pennsylvania Medicine; Montefiore Medical Center; Yeshiva University; Albert Einstein College of Medicine; Indraprastha Institute of Information Technology Delhi,2021.0,PATHOGENS,MDPI,,10.3390/pathogens10081048,20230520-160000,20230521-044735
,wos,What more than a hundred project groups reveal about teaching visualization,"The growing number of students can be a challenge for teaching visualization lectures, supervision, evaluation, and grading. Moreover, designing visualization courses by matching the different experiences and skills of the students is a major goal in order to find a common solvable task for all of them. Particularly, the given task is important to follow a common project goal, to collaborate in small project groups, but also to further experience, learn, or extend programming skills. In this article, we survey our experiences from teaching 116 student project groups of 6 bachelor courses on information visualization with varying topics. Moreover, two teaching strategies were tried: 2 courses were held without lectures and assignments but with weekly scrum sessions (further denoted by TS1) and 4 courses were guided by weekly lectures and assignments (further denoted by TS2). A total number of 687 students took part in all of these 6 courses. Managing the ever growing number of students in computer and data science is a big challenge in these days, i.e., the students typically apply a design-based active learning scenario while being supported by weekly lectures, assignments, or scrum sessions. As a major outcome, we identified a regular supervision either by lectures and assignments or by regular scrum sessions as important due to the fact that the students were relatively unexperienced bachelor students with a wide range of programming skills, but nearly no visualization background. In this article, we explain different subsequent stages to successfully handle the upcoming problems and describe how much supervision was involved in the development of the visualization project. The project task description is given in a way that it has a minimal number of requirements but can be extended in many directions while most of the decisions are up to the students like programming languages, visualization approaches, or interaction techniques. Finally, we discuss the benefits and drawbacks of both teaching strategies.",Information visualization; Interaction; Education; Teaching,"Burch, M; Melby, E",Eindhoven University of Technology,2020.0,JOURNAL OF VISUALIZATION,SPRINGER,,10.1007/s12650-020-00659-6,20230520-160000,20230521-044735
,wos,What is Good Feedback in Big Data Projects for Cyberinfrastructure Diffusion in e-Science?,"This paper investigates the role of feedback in big data projects for cyberinfrastructure (CI) diffusion in e-science. For many of these projects, large-scale and heterogeneous datasets, multidisciplinary and dispersed experts, and advanced technologies are brought together to harness analytic insights. However, without effective CI and computational tools, the accuracy and meaningfulness of analytics results are compromised. In fact, without CI tools, raw data remain raw with hidden insights, as data analytics cannot be executed at all. In order to improve such tools for meaningful results, we argue to conceptualize the communication mechanism of 'feedback' in agile software development, with the goal of producing CI tools that are responsive to users. Based on a grounded analysis of interview data, we concluded that feedback helps developers in big data projects understand users' needs, makes tools user-friendly, prevents emergencies, and is better for developers than no feedback. Furthermore, good feedback is often structured, specific, actionable, timely, generalizable, and delivered in a tactful way. Despite the limitation of the findings being exploratory and yet to be evaluated experimentally, we argued that they still can motivate developers to be proactive seekers of feedback for their tools, productively guide developers' communication with users, and ultimately promote further adoption and diffusion of CI tools in e-science.",feedback; agile software development; e-science; cyberinfrastructure; technology adoption; diffusion of innovations,"Kee, KF; McCain, JC",Chapman University System; Chapman University,2018.0,2018 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,,20230520-160000,20230521-044735
,wos,INFORMED: an incubator at the US FDA for driving innovations in data science and agile technology,"Information Exchange and Data Transformation (INFORMED), a multidisciplinary initiative anchored in the FDA Oncology Center of Excellence, is a decentralized science and technology incubator designed to harness the power of big data and advanced analytics to improve disease outcomes.",,"Khozin, S; Pazdur, R; Shah, A",US Food & Drug Administration (FDA); Centers for Medicare & Medicaid Services,2018.0,NATURE REVIEWS DRUG DISCOVERY,NATURE PUBLISHING GROUP,,10.1038/nrd.2018.34,20230520-160000,20230521-044735
,wos,To Offload or Not? An Analysis of Big Data Offloading Strategies from Edge to Cloud,"Large reductions in completion times can result from transfer of Big Data tasks from edge nodes to cloud resources, which can reduce the completion times by up to 97% and meet client deadlines for computational tasks with responsive and agile solutions. Using scientific programs of varying computational complexity to model resource-intensive tasks, we demonstrate that the task complexity of the computational jobs, the Wide Area Network (WAN) speed and the potential overload of edge servers (as reflected by CPU workloads) are crucial for achieving total reductions in task completion time edge-cloud orchestrators are situated in edge nodes. With continuous access to the parameters ofWireless Local Area Network (WLAN) speed (for data exchanges between client and edge resources), WAN speed (for data exchanges between edge and cloud resources) edge server CPU workload and the complexities in Big Data analytics requirements, accurate edge-to-cloud offloading decisions can be made to minimise total task completion time by the use of cloud computing resources. This work supports the major research efforts have been recently made to develop novel resource orchestration solutions to flexibly link edge nodes with centralised cloud resources so as to maximise the efficiency with which such a continuum of resources can be accessed by users.",Application-level orchestration; Cloud-to-Edge continuum; Big Data analytics; WLAN; WAN; Computational complexity; Server workload,"Singh, R; Kovacs, J; Kiss, T",University of Westminster,2022.0,2022 IEEE WORLD AI IOT CONGRESS (AIIOT),IEEE,,10.1109/AIIOT54504.2022.9817276,20230520-160000,20230521-044735
,wos,Internet of Things and Other E-Solutions in Supply Chain Management May Generate Threats in the Energy Sector-The Quest for Preventive Measures,"Energy firms are the beneficiaries and initiators of innovation, and energy investments are a crucial area of business activity that is specially protected in any country. This is no wonder, as energy security is the basis for the functioning of states and economies. The Internet of Things and Big Data create both new challenges and new threats. This study aimed to identify the potential threats and determine preventive measures, as well as to establish the agile principles related to energy firms' logistics. The method of the narrative summary in combination with the literature searching method was used. Two conclusions emerged: first, research serves to develop the discipline of management science; second, the identification of risks associated with innovation serves practitioners. In addition, the study defined further research directions.",energy; risk; Internet of Things; bid data; industry 4; 0; supply chain management; logistics; agile; framing,"Dobrowolski, Z",Jagiellonian University,2021.0,ENERGIES,MDPI,,10.3390/en14175381,20230520-160000,20230521-044735
,wos,"Big data, industry 4.0 and cyber-physical systems integration: A smart industry context","The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the nextgeneration industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms. (c) 2021 Elsevier Ltd. All rights reserved. Selection and peer-review under responsibility of the scientific committee of the 2nd International Conference on Manufacturing Material Science and Engineering.",Agile management; Heterogeneity; Internet-of-things; Smart factory; Smart manufacturing,"Singh, H",Lovely Professional University,2021.0,MATERIALS TODAY-PROCEEDINGS,ELSEVIER,,10.1016/j.matpr.2020.07.170,20230520-160000,20230521-044735
,wos,Robotics in the Modern World of Work - Results From an Empirical Study Regarding Business Ethics,"The digital revolution is changing the world. Robots, big data and artificial intelligence are the key technologies of the future and the basis of important innovations for the future development of the economy and society. In companies, this fact requires strategic rethinking and adjustments in ever-shorter time cycles. The creation of an agile and collaborative production to achieve the goals is often a basic requirement. With adaptation to technical progress, requirements and goals change continuously. To be and remain competitive, companies are forced to have at least the same technological standard as their competitors. In order to meet these challenges today, the use of highly efficient mechatronic systems such as robots is necessary. The paper analyses business ethics relevant aspects of robotics by using a survey with 88 respondents.",business ethics; digitalization; ethics; robotics; sustainability,"Ludin, D; Wellbrock, W; Muller, E; Gerstlberger, W; Gray, L; Salat, S",Tallinn University of Technology,2021.0,TEHNICKI GLASNIK-TECHNICAL JOURNAL,UNIV NORTH,,10.31803/tg-20210517201926,20230520-160000,20230521-044735
,wos,Big Data Analytics Capability Ecosystem Model for SMEs,"The unprecedented COVID-19 pandemic, together with globalization and advanced technologies, has drastically changed the business environment and forced companies to become more innovative and agile in the way they run their business and respond to the needs and wants of customers. Survival highly depends on the adaptability of SMEs to this ever-changing complex dynamic environment by taking steps in implementing Big Data Analytics as the next frontier for innovation, competition, productivity, and value creation. Based on the grounded theory, this study employed a qualitative method via focus group discussion. Focus groups were conducted with 14 government agencies, SMEs associations, business owners, Chief Operating Officers (CEOs), academic and industrial experts and directors of SMEs in Malaysia. The study revealed the challenges of Malaysian SMEs in adopting Big Data Analytics Capability, presents the criticality of Big Data Analytics Capability to overcome the identified challenges, and develops a BDA Capability Ecosystem Model that integrates the internal enablers, external barriers and support to explain the adoption of BDA Capability for value creation and support the decision-making process. This paper is followed by some policy suggestions for companies' owners, policymakers, government agencies, universities, and SMEs. This study directly impacts Malaysia's economy as a whole by addressing Malaysia's Shared Prosperity Vision 2030. This research contributes to industries that are still in the low value added category with low adoption of technology. Furthermore, it will ultimately lead to the realization of SMEs as 'game changers' to transition the economy to a high-income nation. This study proposes a model that could help SMEs improve their value creation performance, directly influencing the country's GDP and employability.",Big Data Analytics; value creation; capability; competencies; business model; SMEs,"Falahat, M; Cheah, PK; Jayabalan, J; Lee, CMJ; Kai, SB",,2023.0,SUSTAINABILITY,MDPI,,10.3390/su15010360,20230520-160000,20230521-044735
,wos,The role of artificial intelligence in shaping the future of Agile fashion industry,"Artificial intelligence (AI) has become an integral part of every industry. With the emergence of big data, the industries, and more especially textile and apparel (T&A) industry, are on the brink of relationships with consumers, suppliers, and competitors. They need to handle different scenarios with a multitude of complex correlations and dependencies between them and uncertainties arising from human interaction. It has become imperative for them to manage huge amounts of data for the optimization of decision-making processes. In such circumstances, AI techniques have shown promise in every segment of the T&A value chain, from product discovery to robotic manufacturing. The potential wide-ranging applications of AI in T&A industry have found their ways into design support systems to T&A recommendation systems, intelligent tracking systems, quality control, T&A forecasting, predictive analytics in supply chain management or social networks and T&A e-commerce. The research recourses to the qualitative method in the form of systematic literature review and in-depth interviews from senior management people and industry experts. Findings identify the dimensions of AI to develop dynamic capability along with its potential impact and probable challenges. As such, the findings contribute to relevant literature and offer useful insights for academia and practitioners.",Artificial intelligence; dynamic capability; big data analytics; apparel; textile and fashion industry; Agile manufacturing,"Babu, MM; Akter, S; Rahman, M; Billah, MM; Hack-Polay, D",Coventry University; University of Wollongong; University of Lincoln; Khulna University,,PRODUCTION PLANNING & CONTROL,TAYLOR & FRANCIS LTD,,10.1080/09537287.2022.2060858,20230520-160000,20230521-044735
,wos,Nursing Value User Stories A Value Measurement Method for Linking Nurse Contribution to Patient Outcomes,"The use of nursing big data sets for value-based measurement is novel. Nursing value measurement depends on the availability of essential data attributes in the electronic health record related to nursing care delivered (what happened, when, and the result seen). Key in measuring value is a standardized structure and format of these attributes for enabling uniform consistent analysis, along with data sets that are sharable and comparable across individuals and groups, time, organization, and practice focus. The foundation of such sharable and comparable data sets would represent at a minimum individual essential nurse care actions and the resulting patient outcome(s). While nurses generate an extraordinary amount of health-related data, healthcare information systems are not designed to collect structured data that reflect the unique attributes of nursing care or support nursing analytic activities that would measure value. More important, the multidimensional features of the nursing process are difficult to untangle and differentiate from other healthcare workers and nonnursing care activities. The complexity of nursing knowledge work has limited the development of nursing data science methods like value measurement and discouraged value versus cost discussions. This article sets out to describe nursing value measurement and an approach that nurse scientists are maximizing through methods adapted from agile project management, including user stories, and business analysis processes to recognize nurses as primary contributors to patient outcomes and value generation. Nursing Value User Story methods deconstruct complex nursing scenarios into user stories that capture nursing actions as standardized data that can be mapped to a common nursing data model. Methods described here are being used in pilot research at Los Angeles Children's Hospital, and results will be available in 2019.",Clinical data model; Hospital administration; Nurse cost models; Nurse value; Nursing administration; Nursing user stories; Risk-sharing arrangements; Value-based care,"Moon, LA; Clancy, G; Welton, J; Harper, E",University of Minnesota System; University of Minnesota Twin Cities; University of Colorado System; University of Colorado Denver; University of Kansas,2019.0,CIN-COMPUTERS INFORMATICS NURSING,LIPPINCOTT WILLIAMS & WILKINS,,10.1097/CIN.0000000000000520,20230520-160000,20230521-044735
,wos,Failure Prediction Approach in Agile Software Development,"Software failure prediction is an important activity during agile software development as it can help managers to identify the failure modules. Thus, it can reduce the test time, cost, and assign testing resources efficiently. RapidMiner Studio9.4 has been used to perform all the required steps from preparing the primary data to visualizing the results and evaluating the outputs, as well as verifying and improving them in a unified environment. Two datasets are used in this work. The results for the first one indicate that the percentage of failure to predict the time used in the test for all 181 rows, for all test times recorded, is 3% for mean time between failures (MTBF). SVM achieved a 97% success in predicting compared to previous work whose results indicated that the use of administrative delay time (ADT) achieved a statistically significant overall success rate of 93.5%. At the same time, the second dataset result indicates that the percentage of failure to predict the time used is 1.5% for MTBF; SVM achieved 98.5% prediction.",Agile; Big Data; Correlation Coefficient; Decision Making; Software Failure; Software Testing; Support Vector Machine,"Alajaleen, B; Alhroob, A",Isra University,2022.0,INTERNATIONAL JOURNAL OF SOFTWARE INNOVATION,IGI GLOBAL,,10.4018/IJSI.292025,20230520-160000,20230521-044735
,wos,"Understand, develop and enhance the learning process with big data","Purpose With the advent of the internet and communication technology, the penetration of e-learning has increased. The digital data being created by the educational and research institutions is also on the ascent. The growing interest in recent years toward big data, educational data mining and learning analytics has motivated the development of new analytical ways and approaches and advancements in learning settings. The need for using big data to handle, analyze this large amount of data is prime. This trend has started attracting the interest of educational institutions which have an important role in the development skills process and the preparation of a new generation of learners. A real revolution for education, it is based on this kind of terms that many articles have paid attention to big data for learning. How can analytics techniques and tools be so efficient and become a great prospect for the learning process? Big data analytics, when applied into teaching and learning processes, might help to improvise as well as to develop new paradigms. In this perspective, this paper aims to investigate the most promising applications and issues of big data for the design of the next-generation of massive e-learning. Specifically, it addresses the analytical tools and approaches for enhancing the future of e-learning, pitfalls arising from the usage of large data sets. Globally, this paper focuses on the possible application of big data techniques on learning developments, to show the power of analytics and why integrating big data is so important for the learning context. Design/methodology/approach Big data has in the recent years been an area of interest among innovative sectors and has become a major priority for many industries, and learning sector cannot escape to this deluge. This paper focuses on the different methods of big data able to be used in learning context to understand the benefits it can bring both to teaching and learning process, and identify its possible impact on the future of this sector in general. This paper investigates the connection between big data and the learning context. This connection can be illustrated by identifying the several main analytics approaches, methods and tools for improving the learning process. This can be clearer by the examination of the different ways and solutions that contribute to making a learning process more agile and dynamic. The methods that were used in this research are mainly of a descriptive and analytical nature, to establish how big data and analytics methods develop the learning process, and understand their contributions and impacts in addressing learning issues. To this end, authors have collected and reviewed existing literature related to big data in education and the technology application in the learning context. Authors then have done the same process with dynamic and operational examples of big data for learning. In this context, the authors noticed that there are jigsaw bits that contained important knowledge on the different parts of the research area. The process concludes by outlining the role and benefit of the related actors and highlighting the several directions relating to the development and implementation of an efficient learning process based on big data analytics. Findings Big data analytics, its techniques, tools and algorithms are important to improve the learning context. The findings in this paper suggest that the incorporation of an approach based on big data is of crucial importance. This approach can improve the learning process, for this, its implementation must be correctly aligned with educational strategies and learning needs. Research limitations/implications This research represents a reference to better understanding the influence and the role of big data in educational dynamic. In addition, it leads to improve existing literature about big data for learning. The limitations of the paper are given by its nature derived from a theoretical perspective, and the discussed ideas can be empirically validated by identifying how big data helps in addressing learning issues. Originality/value Over the time, the process that leads to the acquisition of the knowledge uses and receives more technological tools and components; this approach has contributed to the development of information communication and the interactive learning context. Technology applications continue to expand the boundaries of education into an anytime/anywhere experience. This technology and its wide use in the learning system produce a vast amount of different kinds of data. These data are still rarely exploited by educational practitioners. Its successful exploitation conducts educational actors to achieve their full potential in a complex and uncertain environment. The general motivation for this research is assisting higher educational institutions to better understand the impact of the big data as a success factor to develop their learning process and achieve their educational strategy and goals. This study contributes to better understand how big data analytics solutions are turned into operational actions and will be particularly valuable to improve learning in educational institutions.",E-learning; Higher education; Big data; Learning analytics; Learning process; Algorithm,"Sedkaoui, S; Khelfaoui, M",Universite de Khemis Miliana,2019.0,INFORMATION DISCOVERY AND DELIVERY,EMERALD GROUP PUBLISHING LTD,,10.1108/IDD-09-2018-0043,20230520-160000,20230521-044735
,wos,Lean 4.0: A New Holistic Approach for the Integration of Lean Manufacturing Tools and Digital Technologies,"Due to the highly dynamic and competitive environment, organizations are led to rethink their processes and strategies. In the industrial field, Lean Manufacturing (LM) is widely recognized as a traditional approach to eliminate waste in the value stream and ensure the efficiency of production processes. On the other hand, Industry 4.0 has recently emerged, incurring disruptive changes in manufacturing processes based on a technology-driven approach. The integration of these two philosophies to achieve organizational goals is interesting in order to guarantee competitiveness, especially for manufacturing companies. This paper proposed an integration of LM tools and technologies 4.0, considering the perspectives of the industrial field in the digital era. Based on a three-step methodology, which included technological and industrial mapping, it was identified 25 synergy points. From interactions of LM tools mainly with Big Data Analytics, The Cloud, Virtual Simulation and Augmented Reality, multi-level circular diagrams pointed out the main contributions of Just in Time 4.0 (JIT 4.0), Kaizen 4.0, Kanban 4.0, Poka-Yoke 4.0, Value Stream Mapping 4.0 (VSM 4.0) and Total Productive Maintenance 4.0 (TPM 4.0). Also, five attributes of Lean 4.0 were identified, highlighting the integration between processes, devices and stakeholders; waste minimization; and autonomous, pointing to gains for the organization from this holistic integration approach.",Lean manufacturing; Industry 4.0; Lean 4.0; Digital technologies; Value chain,"Valamede, LS; Akkari, ACS",Universidade Presbiteriana Mackenzie,2020.0,INTERNATIONAL JOURNAL OF MATHEMATICAL ENGINEERING AND MANAGEMENT SCIENCES,INT JOURNAL MATHEMATICAL ENGINEERING & MANAGEMENT SCIENCES-IJMEMS,,10.33889/IJMEMS.2020.5.5.066,20230520-160000,20230521-044735
,wos,Reproducible research and GIScience: an evaluation using AGILE conference papers,"The demand for reproducible research is on the rise in disciplines concerned with data analysis and computational methods. Therefore, we reviewed current recommendations for reproducible research and translated them into criteria for assessing the reproducibility of articles in the field of geographic information science (GIScience). Using this criteria, we assessed a sample of GIScience studies from the Association of Geographic Information Laboratories in Europe (AGILE) conference series, and we collected feedback about the assessment from the study authors. Results from the author feedback indicate that although authors support the concept of performing reproducible research, the incentives for doi ng this in practice are too small. Therefore, we propose concrete actions for individual researchers and the GIScience conference series to improve transparency and reproducibility. For example, to support researchers in producing reproducible work, the GIScience conference series could offer awards and paper badges, provide author guidelines for computational research, and publish articles in Open Access formats.",GIScience; Open science; Reproducible research; Data science; AGILE; Reproducible conference publications; Open access,"Nust, D; Granell, C; Hofer, B; Konkol, M; Ostermann, FO; Sileryte, R; Cerutti, V",University of Munster; Universitat Jaume I; Salzburg University; University of Twente; Delft University of Technology,2018.0,PEERJ,PEERJ INC,,10.7717/peerj.5072,20230520-160000,20230521-044735
,wos,Data agility through clustered edge computing and stream processing,"The Internet of Things is underpinned by the global penetration of network-connected smart devices continuously generating extreme amounts of raw data to be processed in a timely manner. Supported by Cloud and Fog/Edge infrastructures - on the one hand, and Big Data processing techniques - on the other, existing approaches, however, primarily adopt a vertical offloading model that is heavily dependent on the underlying network bandwidth. That is, (constrained) network communication remains the main limitation to achieve truly agile IoT data management and processing. This paper aims to bridge this gap by defining Clustered Edge Computing - a new approach to enable rapid data processing at the very edge of the IoT network by clustering edge devices into fully functional decentralized ensembles, capable of workload distribution and balancing to accomplish relatively complex computational tasks. This paper also proposes ECStream Processing that implements Clustered Edge Computing using Stream Processing techniques to enable dynamic in-memory computation close to the data source. By spreading the workload among a cluster of collocated edge devices to process data in parallel, the proposed approach aims to improve performance, thereby supporting agile data management. The experimental results confirm that such a distributed in-memory approach to data processing at the very edge of an IoT network can outperform currently adopted Cloud-enabled architectures, and has the potential to address a wide range of IoT-related data-intensive time-critical scenarios.",cloud computing; clustered edge computing; data agility; edge computing; internet of things; stream processing,"Dautov, R; Distefano, S; Bruneo, D; Longo, F; Merlino, G; Puliafito, A",Kazan Federal University; University of Messina,2021.0,CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE,WILEY,,10.1002/cpe.5093,20230520-160000,20230521-044735
,wos,Advancing Design and Runtime Management of AI Applications with AI-SPRINT (Position Paper),"The adoption of Artificial intelligence (AI) technologies is steadily increasing. However, to become fully pervasive, AI needs resources at the edge of the network. The cloud can provide the processing power needed for big data, but edge computing is close to where data are produced and therefore crucial to their timely, flexible, and secure management. In this paper, we introduce the AI-SPRINT project, which will provide solutions to seamlessly design, partition, and run AI applications in computing continuum environments. AI-SPRINT will offer novel tools for AI applications development, secure execution, easy deployment, as well as runtime management and optimization: AI-SPRINT design tools will allow trading-off application performance (in terms of end-to-end latency or throughput), energy efficiency, and AI models accuracy while providing security and privacy guarantees. The runtime environment will support live data protection, architecture enhancement, agile delivery, runtime optimization, and continuous adaptation.",Cloud computing; fog computing; edge computing; AI and machine learning; Cloud trust security & privacy,"Sedghani, H; Ardagna, D; Matteucci, M; Fontana, GA; Verticale, G; Amarilli, F; Badia, R; Lezzi, D; Blanquer, I; Martin, A; Wawruch, K",Polytechnic University of Milan; Universitat Politecnica de Valencia; Technische Universitat Dresden,2021.0,"2021 IEEE 45TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE (COMPSAC 2021)",IEEE COMPUTER SOC,,10.1109/COMPSAC51774.2021.00216,20230520-160000,20230521-044735
,wos,Smart Cities Semantics and Data Models,"Data models and semantics are a key aspect for the valorization of data in cross-domain applications and to obtain knowledge/insights beyond the original applications (vertical use cases). An important role of Big Data and a key fundament of its success is this capacity to discover and extract new knowledge beyond the original use of data, in order to learn, optimize processes and understand the hidden rules of our world. This works presents the different data models from standardization bodies such as IEEE PAR2530, ITU-T FG DPM, ETSI ISG CIM and oneM2M, W3C SSN, OMA LwM2M etc. An analysis and comparative among all of them and also the opportunities to link them in order to guarantee that we can obtain the major value through co-operation among cities and different departments. This work is contextualized in the principles from the Open and Agile Smart Cites (OASC) and linked initiatives focused on data management cross-cities and large scale pilots.",Smart cities; Data models; Internet of things; Semantics; ETSI ISG CIM; ITU-T; oneM2M; FIWARE; Open and agile smart cities; OASC,"Jara, AJ; Serrano, M; Gomez, A; Fernandez, D; Molina, G; Bocchi, Y; Alcarria, R",University of Applied Sciences & Arts Western Switzerland; Ollscoil na Gaillimhe-University of Galway; Universidad Politecnica de Madrid,2018.0,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY & SYSTEMS (ICITS 2018),SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-319-73450-7_8,20230520-160000,20230521-044735
,wos,Harnessing The Power of the Internet of Things (IoT) to Achieve an Agile Business Education Model: A Visionary Paper,"The emergence of artificial intelligence, big data, and the Internet of Things (IoT) has shifted human-human, human-machine, and machine-machine interaction to a new level. This shift is affecting all aspects of society's behavior toward the adoption of technology. One important pillar of society that is especially impacted by this radical change is that of education. The advancement of new technologies as well as the occurrence of unexpected global events has forced education systems in many countries to look differently at traditional educational issues and work toward becoming a more agile education system. This means being responsive to any unpredicted changes that may occur in the education environment. Indeed, the agility of business schools and technological adaptability is one of the standards required by program accreditation organizations (i.e., AACSB). This paper discusses the application of IoT in business education, focusing on the opportunities, challenges, and paths forward this presents.",IoT; Business Education; Education Agility; Higher Education,"Qasim, A; El Refae, GA; Eletter, S; Al-Chahadah, AR",,2021.0,"2021 EIGHTH INTERNATIONAL CONFERENCE ON INTERNET OF THINGS, SYSTEMS, MANAGEMENT AND SECURITY (IOTSMS)",IEEE,,10.1109/IOTSMS53705.2021.9704939,20230520-160000,20230521-044735
,wos,Let's DO - Automotive Platform for Interoperability,"Developing automotive software applications is one of the most challenging and time-consuming activities in the automotive product development cycle. As of today, classical automotive software applications communicate exclusively using vehicle-specific communication protocols such as Controller Area Network (CAN) and FlexRay communication buses. Automotive applications communicate using transport layer messages that are defined and configured for each vehicle system (car model). This hard-wired design makes out-of-the box integrations between heterogeneous automotive products virtually impossible. It also renders automotive integration projects to digital world (smart devices, cloud, big data, IoT gadgets) hard to develop and maintain. We present in this paper Let's DO, a novel platform for interoperability and data exchange between different noncoherent products, systems and devices (both automotive and nonautomotive). Let's DO platform abstracts automotive communication protocol messages in a unified message standard transported over IP-based Ethernet networks enabling interoperability, quick prototyping, code reuse, and allowing more agile and efficient automotive software development cycles.",Automotive Software; Prototyping; Digital Transformation,"ElHakim, R; Elqadi, A; Torky, M; Zayed, M; Farag, I; Agamawi, M",,2021.0,2021 4TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2021),IEEE,,10.1109/ICICT52872.2021.00054,20230520-160000,20230521-044735
,wos,Project management: openings for disruption from AI and advanced analytics,"Purpose The purpose of this essay is to illustrate how project management pull and AI or analytics technology push are likely to result in incremental and disruptive evolution of project management capabilities and practices. Design/methodology/approach This paper is written as a critical essay reflecting the experience and reflections of the author with many ideas drawn from and extending selected items from project management, artificial intelligence (AI) and analytics literatures. Findings Neither AI nor sophisticated analytics is likely to elicit hands on attention from project managers, other than those producing AI or analytics-based artifacts or using these tools to create their products and services. However, through the conduit of packaged software support for project management, new tools and approaches can be expected to more effectively support current activities, to streamline or eliminate activities that can be automated, to extend current capabilities with the availability of increased data, computing capacity and mathematically based algorithms and to suggest ways to reconceive how projects are done and whether they are needed. Research limitations/implications This essay includes projections of possible, some likely and some unlikely, events and states that have not yet occurred. Although the hope and purpose are to alert readers to the possibilities of what may occur as logical extensions of current states, it is improbable that all such projections will come to pass at all or in the way described. Nonetheless, consideration of the future ranging from current trends, the interplay among intersecting trends and scenarios of future states can sharpen awareness of the effects of current choices regarding actions, decisions and plans improving the probability that the authors can move toward desired rather than undesired future states. Practical implications Project managers not involved personally with creating AI or analytics products can avoid mastering detailed skill sets in AI and analytics, but should scan for new software features and affordances that they can use enable new levels of productivity, net benefit creation and ability to sleep well at night. Originality/value This essay brings together AI, analytics and project management to imagine and anticipate possible directions for the evolution of the project management domain.",Project management; IS project management; Artificial intelligence; Business analytics; Big data; Agile development; CRISP-DM,"Niederman, F",Saint Louis University,2021.0,INFORMATION TECHNOLOGY & PEOPLE,EMERALD GROUP PUBLISHING LTD,,10.1108/ITP-09-2020-0639,20230520-160000,20230521-044735
,wos,"The adoption of Design Thinking, Agile Software Development and Co-creation concepts A case study of Digital Banking innovation","Acceleration of technology, especially the mobile internet, causes changes all aspects of human life, including in the banking sector. New emerging technology such as Artificial Intelligence, Blockchain, Big Data, and Cloud computing change the business and operation of the bank. The bank's services have become more personalized, furthermore change customers' lifestyles. Banks are competing to create innovations and breakthroughs to create added value and building a digital ecosystem with fintech and big tech companies in the era of sharing economy. This case study explores the process of creating digital innovation in banking institutions by focusing on adopting design thinking (DT), agile software development (ASD), and co-creation concepts for building digital banking platforms. The case study involved IT executives from four banks in Indonesia. Data were taken through semi-structured interviews and analyzed using NVIVO12. The implication of this research is to accelerate the process of digital banking innovation and produce high-quality digital banking platforms in terms of features and technology.",Digital Innovation; Digital Banking; Agile Software Development; Design Thinking; Co-creation,"Indriasari, E; Prabowo, H; Gaol, FL; Purwandari, B",Universitas Bina Nusantara; University of Indonesia,2021.0,2021 INTERNATIONAL CONFERENCE ON PLATFORM TECHNOLOGY AND SERVICE (PLATCON),IEEE,,10.1109/PlatCon53246.2021.9680763,20230520-160000,20230521-044735
,wos,Transformational shifts through digital servitization,"Manufacturers increasingly look to digitalization to drive service growth. However, success is far from guaranteed, and many firms focus too much on technology. Adopting a discovery-oriented, theories-in-use approach, this study examines the strategic organizational shifts that underpin digital servitization. Notwithstanding strong managerial and academic interest, this link between digitalization and servitization is still under-investigated. Depth interviews with senior executives and managers from a global market leader revealed that to achieve digital service-led growth, a firm and its network need to make three interconnected shifts: (1) from planning to discovery, (2) from scarcity to abundance, and (3) from hierarchy to partnership. Organizational identity, dematerialization, and collaboration play a key role in this transformation. For managers, the study identifies a comprehensive set of strategic change initiatives needed to ensure successful digital servitization.",Digital servitization; Digital transformation; Organizational culture; Agile mindset; Data-centric business model; Big data monetization,"Tronvoll, B; Sklyar, A; Sorhammar, D; Kowalkowski, C",Inland Norway University of Applied Sciences; Karlstad University; Linkoping University; Stockholm University; Hanken School of Economics,2020.0,INDUSTRIAL MARKETING MANAGEMENT,ELSEVIER SCIENCE INC,,10.1016/j.indmarman.2020.02.005,20230520-160000,20230521-044735
,wos,An Agile Governance of Big Data Analytics (BDA) Capabilities and Strategic Alignment to Support Malaysian Public Sector Performance: A Concept Paper,"The 11th Malaysia Plan has emphasized leverage on data (data-driven) that can contribute to the strategic organizational values and be critical to the national transformation agenda. Hence, the government has announced the implementation of the Big Data Analytics (BDA) to support in making accurate decisions; innovation in government services and strategic priorities; system delivery of responsive services; greater agility and performance management. However, BDA still poses a lot of expostulation due to the unsubstantial link between analytics capabilities and core strategic performance. Furthermore, most BDA is incapable to rigorously analyze data since big datasets are generally asymmetrical to performance. Thus, this paper aims to uncover how government agencies should acquire BDA capabilities to succeed in their BD investment to advocate performance. This paper includes a review of secondary sources on Malaysia's economy, social aspects, and technology; BDA, organization performance, government agencies; and Resource based Theory. The paper will contribute: theoretically, an empirically based framework and practically, recognizing the main areas of focus for BDA capability and OP and explaining the mechanisms through which they should be leveraged. The paper also attempts to add to literature on how more effective data-driven analytics strategies to align the performance of government agencies can be adopted. Notably, it can provide ideas to improve performance in government agencies involved in implementing national development strategies, including strengthening administrative functions, social infrastructure and also superior performance of the economy in line with the National Key Result Areas and Industrial Revolution 4.0.",Big Data; Big Data Analytics; Malaysian Public Sector; Performance Management,"Sani, MKJA; Zaini, MK; Sahid, NZ; Noordin, SA; Baba, J",Universiti Teknologi MARA; Universiti Teknologi MARA,2019.0,VISION 2025: EDUCATION EXCELLENCE AND MANAGEMENT OF INNOVATIONS THROUGH SUSTAINABLE ECONOMIC COMPETITIVE ADVANTAGE,INT BUSINESS INFORMATION MANAGEMENT ASSOC-IBIMA,,,20230520-160000,20230521-044735
,wos,ICSSP 2018-Special issue introduction,"The International Conference on Software and System Processes (ICSSP) provides a leading forum for the exchange of research outcomes and industrial best practices in process development from software and systems disciplines. ICSSP 2018 was held in Gothenburg, Sweden, May 26 to 27, 2018, colocated with the 40th International Conference on Software Engineering (ICSE). The theme of ICSSP 2018 was studying Demands on Processes, Processes on Demand by recognizing the demands on processes that include the need for both well-developed plans and incremental deliveries (agile and hybrid processes), utilization of increased automation (model-based engineering and DevOps), higher degrees of customer collaboration, comprehensive analysis of existing products for reuse (open source and COTS), and performance requirements of enterprise-level architectures. This special issue includes the revised and extended versions of the five highest ranked full research papers and industry experience papers of ICSSP 2018, including the two award-winning papers.",agile methods; continuous development; data science; deployment; hybrid systems development; product duality; project management,"O'Connor, RV; Houston, D; Hebig, R; Kuhrmann, M",Dublin City University; Aerospace Corporation - USA; Chalmers University of Technology; TU Clausthal,2019.0,JOURNAL OF SOFTWARE-EVOLUTION AND PROCESS,WILEY,,10.1002/smr.2174,20230520-160000,20230521-044735
,wos,Seven Principles for Rapid-Response Data Science: Lessons Learned from Covid-19 Forecasting,"In this article, we take a step back to distill seven principles out of our experience in the spring of 2020, when our 12-person rapid-response team used skills of data science and beyond to help distribute 340,000+ units of Covid PPE. This process included tapping into domain knowledge of epidemiology and medical logistics chains, curating a relevant data repository, developing models for short-term county-level death forecasting in the US, and building a website for sharing visualization (an automated AI machine). The principles are described in the context of working with Response4Life, a then-new nonprofit organization, to illustrate their necessity. Many of these principles overlap with those in standard data-science teams, but an emphasis is put on dealing with problems that require rapid response, often resembling agile software development. The technical work from this rapid response project resulted in a paper (Altieri et al. (2021)); see also this interview for more background (Yu and Meng (2021)).",Coronavirus; forecasting; county-level; data-science,"Yu, B; Singh, C",University of California System; University of California Berkeley; University of California System; University of California Berkeley,2022.0,STATISTICAL SCIENCE,INST MATHEMATICAL STATISTICS-IMS,,10.1214/22-STS855,20230520-160000,20230521-044735
,wos,"iOntoBioethics: A Framework for the Agile Development of Bioethics Ontologies in Pandemics, Applied to COVID-19","Background: Few ontological attempts have been reported for conceptualizing the bioethics domain. In addition to limited scope representativeness and lack of robust methodological approaches in driving research design and evaluation of bioethics ontologies, no bioethics ontologies exist for pandemics and COVID-19. This research attempted to investigate whether studying the bioethics research literature, from the inception of bioethics research publications, facilitates developing highly agile, and representative computational bioethics ontology as a foundation for the automatic governance of bioethics processes in general and the COVID-19 pandemic in particular. Research Design: The iOntoBioethics agile research framework adopted the Design Science Research Methodology. Using systematic literature mapping, the search space resulted in 26,170 Scopus indexed bioethics articles, published since 1971. iOntoBioethics underwent two distinctive stages: (1) Manually Constructing Bioethics (MCB) ontology from selected bioethics sources, and (2) Automatically generating bioethics ontological topic models with all 26,170 sources and using special-purpose developed Text Mining and Machine-Learning (TM&ML) engine. Bioethics domain experts validated these ontologies, and further extended to construct and validate the Bioethics COVID-19 Pandemic Ontology. Results: Cross-validation of the MCB and TM&ML bioethics ontologies confirmed that the latter provided higher-level abstraction for bioethics entities with well-structured bioethics ontology class hierarchy compared to the MCB ontology. However, both bioethics ontologies were found to complement each other forming a highly comprehensive Bioethics Ontology with around 700 concepts and associations COVID-19 inclusive. Conclusion: The iOntoBioethics framework yielded the first agile, semi-automatically generated, literature-based, and domain experts validated General Bioethics and Bioethics Pandemic Ontologies Operable in COVID-19 context with readiness for automatic governance of bioethics processes. These ontologies will be regularly and semi-automatically enriched as iOntoBioethics is proposed as an open platform for scientific and healthcare communities, in their infancy COVID-19 learning stage. iOntoBioethics not only it contributes to better understanding of bioethics processes, but also serves as a bridge linking these processes to healthcare systems. Such big data analytics platform has the potential to automatically inform bioethics governance adherence given the plethora of developing bioethics and COVID-19 pandemic knowledge. Finally, iOntoBioethics contributes toward setting the first building block for forming the field of Bioethics Informatics.",bioethics; COVID-19; pandemic; bioethics ontology; bioethics informatics; iOntoBioethics; agile framework; design science research methodology,"Odeh, M; Kharbat, FF; Yousef, R; Odeh, Y; Tbaishat, D; Hakooz, N; Dajani, R; Mansour, A",King Hussein Cancer Center; University of West England; University of Jordan; University of Jordan; University of Jordan; Hashemite University; University of Richmond,2021.0,FRONTIERS IN MEDICINE,FRONTIERS MEDIA SA,,10.3389/fmed.2021.619978,20230520-160000,20230521-044735
,wos,Value creation from analytics with limited data: a case study on the retailing of durably consumer goods,"Companies are pinning high hopes on competitive advantages through data analytics. So far, value gains through analytics have been demonstrated for IT-heavy and data-rich business areas. Yet, research has paid little attention to value creation through data analytics in the plethora of companies with limited data (Le. having transactions in the hundreds and attributes in the tens). Building on the literature of big data value creation and the resource-based view, we carried out an in-depth analytics case study with a retailer of renewable energy systems. Firms in this business area operate with expensive but few sales, so their available data are notoriously limited. Our findings demonstrate that data analytics capabilities and value creation mechanisms (democratise, contextualise, experiment with data, and execute data insights) are also effective in situations with limited data. Practice and research should therefore put not only emphasis on the volume and the variety of data but also on contextual factors related to managers (e.g. dear strategy, vision, leadership) and all employees (e.g. openness for agile working mode, data awareness).",data analytics; information systems (IS) value creation; resource theory; value creation mechanisms; machine learning(ML); retail; renewable energy systems (RES),"Hopf, K; Weigert, A; Staake, T",Otto Friedrich University Bamberg,,JOURNAL OF DECISION SYSTEMS,TAYLOR & FRANCIS LTD,,10.1080/12460125.2022.2059172,20230520-160000,20230521-044735
,wos,An End-to-End Recommendation System for Urban Traffic Controls and Management Under a Parallel Learning Framework,"A paradigm shift towards agile and adaptive traffic signal control empowered with the massive growth of Big Data and Internet of Things (IoT) technologies is emerging rapidly for Intelligent Transportation Systems. Generally, an adaptive signal control system fine-tunes signal timing parameters based on pre-defined control hyperparameters using instantaneous traffic detection information. Once traffic pattern changes, those hyperparameters (e.g., maximum and minimum green times) need to be adjusted according to the evolution of traffic dynamics over a very short-term period. Such adjustment processes are usually conducted by professional and experienced traffic engineers. Here we present a human-in-the-loop parallel learning framework and its utilization in an end-to-end recommendation system that mimics and enhances professional signal control engineers' behaviors. The system has been deployed into a real-world application for an extended period in Hangzhou, China, where signal control hyperparameters are recommended based on large-scale multidimensional traffic datasets. Experimental evaluations demonstrate significant improvements in traffic efficiency through the use of our signal recommendation system.",Control systems; Urban areas; Timing; Adaptive systems; Real-time systems; Recurrent neural networks; Process control; Intelligent traffic control; traffic signal control; parallel learning; recommendation systems; deep neural networks,"Jin, JC; Guo, HF; Xu, J; Wang, X; Wang, FY","Chinese Academy of Sciences; Institute of Automation, CAS; Zhejiang University of Technology; Zhejiang University",2021.0,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,,10.1109/TITS.2020.2973736,20230520-160000,20230521-044735
,wos,Applications of Machine Learning for Electronic Warfare Emitter Identification and Resource Management,"Electronic warfare (EW) operators face a multitude of challenges when performing single-and distributed-platform sensing and jamming tasks in increasingly dense and agile threat environ-ments. During an engagement timeline, actions often must be taken quickly and based on the partial information available. Recently, the world has observed a boom in artificial intelligence, a suite of data-driven lateral technologies that has already disrupted multiple fields where autonomy and big data are key elements. Although it is not the solution to all EW tasks, artificial intelligence shows promise in offering potential solutions to improve EW efficiency and effectiveness through informed decision-making beyond the capability of a human operator. The Johns Hopkins Uni-versity Applied Physics Laboratory (APL) Precision Strike Mission Area has invested in research and development in the specific EW tasks of emitter identification and autonomous resource alloca-tion. This article presents promising results from these projects and describes recommended future work in these areas, as well as additional EW applications that may benefit from research in arti-ficial intelligence.",,"Casterline, KA; Watkins, NJ; Ward, JR; Li, WL; Thommana, MJ",Johns Hopkins University; Johns Hopkins University Applied Physics Laboratory,2022.0,JOHNS HOPKINS APL TECHNICAL DIGEST,JOHNS HOPKINS UNIV APPLIED PHYSICS LABORATORY LLC,,,20230520-160000,20230521-044735
,wos,SMVS: A Web-based Application for Graphical Visualization of Malay Text Corpus,"Information visualization is an interesting field nowadays. A good information visualization ensures distraction of misleading information is not included in the visualization. Many studies have been conducted on the Quranic corpus. The advancement technology coupled with modern approach of the computer technology can support the learners to understand Qur'an easily. Smart Malay Visualization System (SMVS) is a Python Flask framework web application which help users efficiently to produce the most basic data visualization from a big data. This web application displayed information from the state-of-the-art corpus which is identified through text. Agile development has been adapted to prepare this web application. Six phases of the methodology have been implemented in this study which are requirements, analysis, planning, design, implementation, testing, and deployment. Natural Language Processing approach has been used to visualize the data. Twenty most informative word from each verse has been visualized using Frequency Distribution and has been embedded to the web application. This work focuses on the Malay translation of the Qur'an corpus.",Big data; data visualization; knowledge representation; Qur'an knowledge; natural language processing,"Baseri, NBA; Abu Bakar, J; Ahmad, A; Jafferi, H; Zamri, MF",Universiti Utara Malaysia,2020.0,IEEE 10TH SYMPOSIUM ON COMPUTER APPLICATIONS AND INDUSTRIAL ELECTRONICS (ISCAIE 2020),IEEE,,,20230520-160000,20230521-044735
,wos,Digital twin framework for reconfigurable manufacturing systems (RMSs): design and simulation,"Faced with the global crisis of COVID-19 and the strong increase in customer demands, competition is becoming more intense between companies, on the one hand, and supply chains on the other. This competition has led to the development of new strategies to manage demand and increase market share. Among these strategies are the growing interest in sustainable manufacturing and the need for customizable products that create an increasingly complex manufacturing environment. Sustainable manufacturing and the need for customizable products create an environment of increased competition and constant change. Indeed, companies are trying to establish more flexible and agile manufacturing systems through several systems of reconfiguration. Reconfiguration contributes to an extension of the manufacturing system's life cycle by modifying its physical, organizational and IT characteristics according to the changing market conditions. Due to the rapid development of new information technology (such as IoT, Big Data analytics, cyber-physical systems, cloud computing and artificial intelligence), digital twins have become intensively used in smart manufacturing. This paper proposes a digital twin design and simulation model for reconfigurable manufacturing systems (RMSs).",Reconfigurable manufacturing system (RMS); Modular framework; Generic model; Digital twin (DT); SysML,"Touckia, JK; Hamani, N; Kermad, L",Universite Paris-VIII; Picardie Universites; Universite de Picardie Jules Verne (UPJV),2022.0,INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY,SPRINGER LONDON LTD,,10.1007/s00170-022-09118-y,20230520-160000,20230521-044735
,wos,Internet of Vehicles and Real-Time Optimization Algorithms: Concepts for Vehicle Networking in Smart Cities,"Achieving sustainable freight transport and citizens' mobility operations in modern cities are becoming critical issues for many governments. By analyzing big data streams generated through IoT devices, city planners now have the possibility to optimize traffic and mobility patterns. IoT combined with innovative transport concepts as well as emerging mobility modes (e.g., ridesharing and carsharing) constitute a new paradigm in sustainable and optimized traffic operations in smart cities. Still, these are highly dynamic scenarios, which are also subject to a high uncertainty degree. Hence, factors such as real-time optimization and re-optimization of routes, stochastic travel times, and evolving customers' requirements and traffic status also have to be considered. This paper discusses the main challenges associated with Internet of Vehicles (IoV) and vehicle networking scenarios, identifies the underlying optimization problems that need to be solved in real time, and proposes an approach to combine the use of IoV with parallelization approaches. To this aim, agile optimization and distributed machine learning are envisaged as the best candidate algorithms to develop efficient transport and mobility systems.",vehicle networking; Internet of Vehicles; IoT analytics; data analytics; agile optimization; distributed machine learning; smart cities,"Adelantado, F; Ammouriova, M; Herrera, E; Juan, AA; Shinde, SS; Tarchi, D",UOC Universitat Oberta de Catalunya; Universitat Politecnica de Valencia; University of Bologna,2022.0,VEHICLES,MDPI,,10.3390/vehicles4040065,20230520-160000,20230521-044735
,wos,Management Perspectives towards the Data-Driven Organization in the Energy Sector,"This paper explores the current attitudes of managers and executives working in the energy sector towards the Data-Driven Organizational Model implied by Big Data. The aim is to explore and understand the current mindset of senior decision makers, since their success depends as much on cognitive and behavioral processes as on their technical competences. We adopt a grounded-theory approach, developing models of understanding and belief abductively, driven by the data obtained from participants through a reflection guide. We find that managers differ significantly in their understanding and engagement with their challenges; they display interest but differ in their commitment and enthusiasm; they identify a lack of strategy and skills as current barriers; and they are currently unwilling to trust data, treating evidence according to their own prior commitments. This is a significant barrier to establishing the Data-Driven Organizational Model. These findings raise concerns, and the paper concludes that by considering initiatives for implementing more agile and forward-looking approaches, establishing a data-driven organizational culture, and managing such changes effectively.",data-driven organizational model; big data; big data analytics; digitalization; energy; EU Green Deal,"Pugna, IB; Boldeanu, DM; Gheorghe, M; Cozgarea, G; Cozgarea, AN",Bucharest University of Economic Studies,2022.0,ENERGIES,MDPI,,10.3390/en15165775,20230520-160000,20230521-044735
,wos,Applications of Machine Learning for Electronic Warfare Emitter Identification and Resource Management,"Electronic warfare (EW) operators face a multitude of challenges when performing single- and distributed-platform sensing and jamming tasks in increasingly dense and agile threat environments. During an engagement timeline, actions often must be taken quickly and based on the partial information available. Recently, the world has observed a boom in artificial intelligence, a suite of data-driven lateral technologies that has already disrupted multiple fields where autonomy and big data are key elements. Although it is not the solution to all EW tasks, artificial intelligence shows promise in offering potential solutions to improve EW efficiency and effectiveness through informed decision-making beyond the capability of a human operator. The Johns Hopkins University Applied Physics Laboratory (APL) Precision Strike Mission Area has invested in research and development in the specific EW tasks of emitter identification and autonomous resource allocation. This article presents promising results from these projects and describes recommended future work in these areas, as well as additional EW applications that may benefit from research in artificial intelligence.",,"Casterline, KA; Watkins, NJ; Ward, JR; Li, WL; Thommana, MJ",Johns Hopkins University; Johns Hopkins University Applied Physics Laboratory,2022.0,JOHNS HOPKINS APL TECHNICAL DIGEST,JOHNS HOPKINS UNIV APPLIED PHYSICS LABORATORY LLC,,,20230520-160000,20230521-044735
,wos,Requirements capture and comparative analysis of open source versus proprietary service oriented architecture,"Service Oriented Architecture (SOA) integrates information systems towards an agile and reusable service-based connectivity. It is an approach amalgamating large scale private/public computer systems and other resources with continuous phenomenal advent evolution and leveraging of the World Wide Web (WWW, commonly referred to as the Web) social media, mobile communications, Big Data (BD), data analytics, Machine Learning (ML) based optimisation, Cloud Computing (CC) and Internet of Things (IoT), commonly known as Advanced Technologies (AT). Implementing SOA, whether Open Source Software (OSS) or proprietary or absolute freeware is a choice to be made which depends on the organisation's requirements in light of AT as well as a host of delivery and security concerns. In this paper, a comparative analysis of an open source vs. proprietary SOA for large scale computer systems servicing AT is presented by examining their main efficacies, features, advantages and disadvantages and capturing their generic technical functional and non-functional requirements in a unified manner. Furthermore, the SOA evaluation criteria, recommendations and conclusions are also presented.",SOA; Open source; Closed source proprietary; Requirements capture; Advanced technologies; Web services access; Web 1.0/2.0/3.0/4./5.0/6.0,"Bamhdi, A",Umm Al Qura University,2021.0,COMPUTER STANDARDS & INTERFACES,ELSEVIER,,10.1016/j.csi.2020.103468,20230520-160000,20230521-044735
,wos,Developing a Virtual Smart Total Learning Environment for Future Teaching-Learning System,"The world of education system after this COVID-19 pandemic will have to change its dimension to map the needs of learners. The proposed framework is focused on transforming the learning experience into two possible ways like online and on-campus learning through groundbreaking & agile methodologies. The new interfaces for learners will be included like Gamification, animated tutorial etc. The framework designed here is the outcome of the e-learning experiences of the authors and it tries to add all relevant technologies with cutting-edge research to provide inspirational and transformative knowledge to learners of all ages, social status, communities who form worldwide communities of special-learners. It will rise to the occasion to use its open source technology along with the emerged technologies like IoT, 5G etc, to transcend physical and social borders. This framework is a total learning environment as it will incorporate all possible latest technologies like big data and machine learning. The e-learning system possesses features like personalized e-learning, anomaly detection, student performance monitoring, dynamic content preparations, students' satisfaction monitoring etc. The new framework will include big data, cloud applications, machine learning and artificial intelligence to make the system faster, efficient and smart. The new features will make the e-learning system based on Virtual Smart Total Learning Environment (VSTLE) more technologically sound and efficient in processing, predicting, evaluating and making storage backup. This framework is designed in such a way that the minimum human intervention will be needed for its functioning. As a result, the final output will be more accurate as compared to other e-learning systems available.",smart; virtual; learning-environment; machine learning; big data; adaptive learning,"Akour, MA; Das, A",,2020.0,"PROCEEDINGS OF 2020 IEEE INTERNATIONAL CONFERENCE ON TEACHING, ASSESSMENT, AND LEARNING FOR ENGINEERING (IEEE TALE 2020)",IEEE,,10.1109/TALE48869.2020.9368373,20230520-160000,20230521-044735
,wos,An Agile Sample Maintenance Approach for Agile Analytics,"Agile analytics can help organizations to gain and sustain a competitive advantage by making timely decisions. Approximate query processing (AQP) is one of the useful approaches in agile analytics, which facilitates fast queries on big data by leveraging a pre-computed sample. One problem such a sample faces is that when new data is being imported, re-sampling is most likely needed to keep the sample fresh and AQP results accurate enough. Re-sampling from scratch for every batch of new data, called the full re-sampling method and adopted by many existing AQP works, is obviously a very costly process, and a much quicker incremental sampling process, such as reservoir sampling, may be used to cover the newly arrived data. However, incremental update methods suffer from the fact that the sample size cannot be increased, which is a problem when the underlying data distribution dramatically changes and the sample needs to be enlarged to maintain the AQP accuracy. This paper proposes an adaptive sample update (ASU) approach that avoids re-sampling from scratch as much as possible by monitoring the data distribution, and uses instead an incremental update method before a re-sampling becomes necessary. The paper also proposes an enhanced approach (T-ASU), which tries to enlarge the sample size without re-sampling from scratch when a bit of query inaccuracy is tolerable to further reduce the sample update cost. These two approaches are integrated into a state-of-the-art AQP engine for an extensive experimental study. Experimental results on both real-world and synthetic datasets show that the two approaches are faster than the full re-sampling method while achieving almost the same AQP accuracy when the underlying data distribution continuously changes.",,"Zhang, HB; Zhang, YZ; He, ZY; Jing, YA; Zhang, K; Wang, XS",Fudan University,2020.0,2020 IEEE 36TH INTERNATIONAL CONFERENCE ON DATA ENGINEERING (ICDE 2020),IEEE COMPUTER SOC,,10.1109/ICDE48307.2020.00071,20230520-160000,20230521-044735
,wos,Digital Contact Tracing Based on a Graph Database Algorithm for Emergency Management During the COVID-19 Epidemic: Case Study,"Background: The COVID-19 epidemic is still spreading globally. Contact tracing is a vital strategy in epidemic emergency management; however, traditional contact tracing faces many limitations in practice. The application of digital technology provides an opportunity for local governments to trace the contacts of individuals with COVID-19 more comprehensively, efficiently, and precisely. Objective: Our research aimed to provide new solutions to overcome the limitations of traditional contact tracing by introducing the organizational process, technical process, and main achievements of digital contact tracing in Hainan Province. Methods: A graph database algorithm, which can efficiently process complex relational networks, was applied in Hainan Province; this algorithm relies on a governmental big data platform to analyze multisource COVID-19 epidemic data and build networks of relationships among high-risk infected individuals, the general population, vehicles, and public places to identify and trace contacts. We summarized the organizational and technical process of digital contact tracing in Hainan Province based on interviews and data analyses. Results: An integrated emergency management command system and a multi-agency coordination mechanism were formed during the emergency management of the COVID-19 epidemic in Hainan Province. The collection, storage, analysis, and application of multisource epidemic data were realized based on the government's big data platform using a centralized model. The graph database algorithm is compatible with this platform and can analyze multisource and heterogeneous big data related to the epidemic. These practices were used to quickly and accurately identify and trace 10,871 contacts among hundreds of thousands of epidemic data records; 378 closest contacts and a number of public places with high risk of infection were identified. A confirmed patient was found after quarantine measures were implemented by all contacts. Conclusions: During the emergency management of the COVID-19 epidemic, Hainan Province used a graph database algorithm to trace contacts in a centralized model, which can identify infected individuals and high-risk public places more quickly and accurately. This practice can provide support to government agencies to implement precise, agile, and evidence-based emergency management measures and improve the responsiveness of the public health emergency response system. Strengthening data security, improving tracing accuracy, enabling intelligent data collection, and improving data-sharing mechanisms and technologies are directions for optimizing digital contact tracing.",COVID-19; digital contact tracing; emergency management; graph database; big data; visualization; China,"Mao, ZJ; Yao, H; Zou, Q; Zhang, WT; Dong, Y",Huazhong University of Science & Technology; Huazhong University of Science & Technology; China University of Mining & Technology,2021.0,JMIR MHEALTH AND UHEALTH,"JMIR PUBLICATIONS, INC",,10.2196/26836,20230520-160000,20230521-044735
,wos,Energy Efficient Double Critic Deep Deterministic Policy Gradient Framework for Fog Computing,"Nowadays the data is growing at a faster pace and the big data applications are required to be more agile and flexible. There is a need for a decentralized model to carry out the required substantial amount of computation across edge devices as they has led to the innovation of fog computing. Energy consumption among the edge devices is one of the potential threatening issues in fog computing. Their high energy demand also contributes to higher computation cost. In this paper Double Critic (DC) approach is enforced over the Deep Deterministic Policy Gradient (DDPG) technique to design the DC-DDPG framework which formulates high quality energy efficiency policies for fog computing. The performance of the proposed framework is outstanding compared to existing works based on the metrics like energy consumption, response time, total cost, and throughput. They are measured under two different fog computing scenarios i.e., fog layer with multiple entities in a region and fog layer with multiple entities in multiple regions. Mathematical modeling reveals that the energy efficiency policies formulated are of high quality as they satisfy the quality assurance metrics, such as empirical correctness, robustness, model relevance, and data privacy.",Deterministic Policy Gradient; Fog computing; Energy; Q-learning; Double Critic,"Krishnamurthy, B; Shiva, SG",Siddaganga Institute of Technology; University of Memphis,2022.0,2022 IEEE WORLD AI IOT CONGRESS (AIIOT),IEEE,,10.1109/AIIoT54504.2022.9817157,20230520-160000,20230521-044735
,wos,CHALLENGES OF LOGISTICS IN THE CONCEPT OF INDUSTRY 4.0,"Concerns about the marginalization of Europe in world production prompted the development of the concept of Industry 4.0, which will allow highly developed countries to regain the status of leaders in industrialization. As part of the proposed concept, which is to form the basis of the fourth industrial revolution, it is planned to closely connect physical objects with the information network. The application of new production strategies, such as Agile Manufacturing and Mass Customization, are causing manufacturing companies to be transformed into integrated networks in which they combine their core competencies. The idea of Industry 4.0 is to create sophisticated business networks, connected by intelligent resources communicating via the internet, using well-known and already used technologies, including the Internet of Things (IoT), big data, cloud computing, etc. As a result of applying these solutions, autonomous systems exchange data between themselves and the boundaries between enterprises are disappearing more and more. Therefore, the aim of the article is to identify key challenges of today's logistics (called Logistics 4.0) in the perspective of the development of the industry concept 4.0.",Industry 4.0; Logistics 4.0; production networks management; Internet of Things (IoT),"Saniuk, S; Graczyk, M; Kulyk, P",University of Zielona Gora,2019.0,8TH CARPATHIAN LOGISTICS CONGRESS (CLC 2018),TANGER LTD,,,20230520-160000,20230521-044735
,wos,Reimagining the Fashion Retail Industry Through the Implications of COVID-19 in the Gulf Cooperation Council (GCC) Countries,"The COVID-19 pandemic has disrupted the fashion retail industry. The Gulf Cooperation Council Countries (GCC) is the home of family-centric shopping malls and brick and mortar stores (B&M). This article aims to provide a critical look at the business strategies which the fashion retail companies need to adopt to provide consumers with an integrated online and B&M service which will be essential to survive in the post-pandemic business environment. This article is based on the rich industry experience of the authors and extensive secondary research on the business strategies being employed by the leading fashion retailers in the GCC region to combat the pandemic disruption. The study highlights the importance of a comprehensive rethink on business strategy for the GCC fashion retailers with adoption of digitalization technologies and an adaptive supply chain as the pillars to survive the post-pandemic normal of business environments. The study concludes with a look to the future strategies for fashion retailers in developing a digitalization blueprint, using cloud technologies and big data analytics, leveraging social media, building an agile and adaptive supply chain with omnichannel capability, and ensuring that future products and services are sustainable and socially responsible.",COVID-19; fashion retail; GCC; e-commerce; business strategy; B&M stores,"Rao, PHN; Vihari, NS; Jabeen, SS",,2021.0,FIIB BUSINESS REVIEW,SAGE PUBLICATIONS INDIA  PVT LTD,,10.1177/23197145211039580,20230520-160000,20230521-044735
,wos,Collective Intelligence Systems from an Organizational Perspective,"In this talk, we consider Collective Intelligence (CI) systems [11-13, 15] from an organizational perspective. CI systems offer a solution to problems that need cognitive skills, problem-solving capabilities, knowledge, know-how or experience at large scale. They help to facilitate and streamline large-scale problem-solving endeavours. The organizational perspective on CI systems offers us two strands of discussion. On the one hand, it can be about understanding the potential of CI systems for today's organizations. On the other hand, CI systems can be considered as organizations themselves and can be investigated as such. We start by reviewing the state-of-the art of CI frameworks [19, 20]. What are the essential building blocks of a CI system? Who uses them? For what, how and why? We come up with a generalized framework [19] that serves us as a basis for further investigations. From a governance perspective, today's organizations are recursive-feedback control systems, usually expressed in the form of process-oriented management [5, 9, 10], see also [6, 7]. A deeper look reveals a plethora of different styles of organizational culture [14, 18]. Still, viable organizations have in common certain essential sub systems, which are policy making, external and internal steering, the primary activities and an informational backbone [13]. How can we exploit CI systems to support these organizational building blocks? Can CI systems be made an integral part of organizations to make them more stable towards distortions; more adaptive towards an ever changing environment; more agile towards the organization's innovative potential? Answers to such questions would free CI systems from being niche players in certain large-scale problem-solving initiatives. Reflecting back from the potential of CI systems in today's organizations, we ask: what can be learned with respect to the design and implementation of future CI systems; and: how to break the silos, i.e., how to integrate CI systems with related paradigms such as knowledge management systems, compare also with [4, 8, 16, 17] and latest computing resources such as big data and the data science toolkit?",collective intelligence; crowdsourcing; human computer interaction; Web 2.0,"Draheim, D",Tallinn University of Technology,2019.0,IIWAS2019: THE 21ST INTERNATIONAL CONFERENCE ON INFORMATION INTEGRATION AND WEB-BASED APPLICATIONS & SERVICES,ASSOC COMPUTING MACHINERY,,10.1145/3366030.3368457,20230520-160000,20230521-044735
,wos,Big Data Analytics APIs Architecture for Formative Assessors,"This Research to Practice Full Paper is driven by the question: Within limited time resources available to trainers in projects for Big Data Analytics (BDA) problems, how can they define project requirements for Formative Assessment (FA) actions? The paper suggests BDA APIs architecture as helping tool for formative assessors. It helps them effectively produce and adapt visual diagnostic reports for FA-actions in agile based requirements (i.e. features) definition. The paper presents two core architectures: Architecture for a parametrized feature-descriptor-system to define/refine a BDA API feature and its visual diagnostic reports, and an initial resources architecture for BDA API to initialize an analytics algorithm with its input big data sets. Clarifying visually the trainee's challenges (i.e. incremental features in a BDA API) is our main FA action. The FA action is designed based on Csikszentmihalyi's flow model to support a trainee in matching balance between his/her challenges and his/her skills. To test the architecture's functions, the paper has test setups for two formal projects (each has 1 to 6 trainees) and two informal projects (each has 1 to 3 trainees). The projects are to attack BDA problems in learning analytics and in image automatic classification. The test results show that the visual diagnostic reports produced by the trainers are very effective in clarifying visually incremental BDA API features not only for simple classifiers (i.e. classical data mining algorithms) but also for complex classifiers (i.e. deep learning algorithms). The results show also how visual diagnostic reports are easily produced for comparing the algorithm performances using different input big data sets, whereas other reports are produced for comparing performances between different algorithms, using one input data set. Related works are also discussed to show the architecture's differences and advantages. Its main advantages are: 1) it enables the trainers to use deep learning algorithms beside classical data mining algorithms in its BDA API parameterizable feature descriptors for visual diagnostic reports. 2) The descriptors can be extended, reused, shared, and scaled out to help trainers in other universities providing flow model based FA actions. 3) Finally, it has extensions to integrate other theoretical frameworks like Buckingham Shum and Deakin Crick's framework for dispositional learning analytics instead of the used flow model.",assessment in engineering education; planning for formative assessment; Big Data Analytics,"Mahfouz, W; Wuttke, HD",Technische Universitat Ilmenau,2021.0,2021 IEEE FRONTIERS IN EDUCATION CONFERENCE (FIE 2021),IEEE,,10.1109/FIE49875.2021.9637431,20230520-160000,20230521-044735
,wos,Key Considerations in Optimizing the Deployment of Big Data Analytics-as-a-Service Utilizing Cloud Architecture and Machine Learning,"Cloud computing is usually associated with storage and processing on a back-end server that is accessible over the Internet. Increasingly the Cloud has transcended the boundaries of storage and retrieval and moved into the realms of offering services, performing analytics, undertaking collaborations and much more while ensuring security and privacy of data for the user applications. As a result, deployment of any application or service on the Cloud requires a carefully constructed strategy - especially with respect to managing the dynamicity and balancing of that deployment. This strategic need for Cloud Architecture is further important because of the advent of Big Data and Analytics-as-a-Service (AaaS). The Analytical services utilizing Big Data are not limited to a specific Cloud server. Instead, Big Data Analytics are carried out across the entire spectrum of Internet-based nodes ranging from the back-end Cloud server through to the End-user Internet of Things (IoT) devices and everything in between. The time, location and granularity of Big Data Analytics on and off the Cloud is a crucial strategic question. The Quality of Service (QoS) and security of deployment of Analytics-as-a-Service depends on the key considerations in answering this question. This strategic question relates to the dynamic decision making required to deploy a Cloud-based Big Data Analytics solution - which, in turn, is based on understanding the current conditions security, volume, performance, criticality, among others - of Cloud-based deployment. The need for automation and intelligence with the dynamic optimization of Analytics on the Cloud requires the application of Machine Learning. This paper explores these key considerations of the strategic aspects of deploying Big Data Analytics using a Cloud Architecture. The practical application of these key considerations is demonstrated through the education domain. Finally, this paper proposes areas of research emanating from the study of Cloud Architecture and Machine Learning for Big Data Analytics.",Data Analytics; Analytics-as-a-Service; Big data strategies; IoT; Cloud Architecture; Cloud deployment; Machine Learning; Agile business,"Unhelkar, B; Rao, VT",State University System of Florida; University of South Florida,2020.0,PROCEEDINGS OF ICETIT 2019: EMERGING TRENDS IN INFORMATION TECHNOLOGY,SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-030-30577-2_73,20230520-160000,20230521-044735
,wos,Rethinking data-driven decision support in flood risk management for a big data age,"Decision-making in flood risk management is increasingly dependent on access to data, with the availability of data increasing dramatically in recent years. We are therefore moving towards an era of big data, with the added challenges that, in this area, data sources are highly heterogeneous, at a variety of scales, and include a mix of structured and unstructured data. The key requirement is therefore one of integration and subsequent analyses of this complex web of data. This paper examines the potential of a data-driven approach to support decision-making in flood risk management, with the goal of investigating a suitable software architecture and associated set of techniques to support a more data-centric approach. The key contribution of the paper is a cloud-based data hypercube that achieves the desired level of integration of highly complex data. This hypercube builds on innovations in cloud services for data storage, semantic enrichment and querying, and also features the use of notebook technologies to support open and collaborative scenario analyses in support of decision making. The paper also highlights the success of our agile methodology in weaving together cross-disciplinary perspectives and in engaging a wide range of stakeholders in exploring possible technological futures for flood risk management.",big data; cloud computing; data hypercube; data science; flexible querying; semantic web; uncertainty,"Towe, R; Dean, G; Edwards, L; Nundloll, V; Blair, G; Lamb, R; Hankin, B; Manson, S",N8 Research Partnership; RLUK- Research Libraries UK; Lancaster University; Royal Dutch Shell; UK Centre for Ecology & Hydrology (UKCEH); N8 Research Partnership; RLUK- Research Libraries UK; Lancaster University,2020.0,JOURNAL OF FLOOD RISK MANAGEMENT,WILEY,,10.1111/jfr3.12652,20230520-160000,20230521-044735
,wos,Large scale quality transformation in hybrid development organizations - A case study,"As the software industry transitions to a subscription-based software-as-a-service (SaaS) model, soft-ware development companies are transforming to hybrid development organizations with increased adoption of Agile and Continuous Integration/ Continuous Delivery (CI/CD) development practices for newer products while continuing to use Waterfall methods for older products. This transformation is a huge undertaking impacting all aspects of the software development life cycle (SDLC), including the quality management system. This paper presents a case study of a large-scale transformation of a legacy quality management system to a modern system developed and implemented at Cisco Systems. The framework for this transformation is defined by six distinct areas: metrics, process, measurement, reporting, quality analytics, and culture & leadership. Our implementation leveraged recent advances in Machine Learning (ML), Artificial Intelligence (AI), connected data, integrated operations, and big data technologies to solve the challenges created by a hybrid software development organization. We believe this case study will help researchers and industry leaders understand the benefits and potential challenges of such sizeable transformations. (c) 2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Quality transformation; Quality management system; Agile; Waterfall; Hybrid development organization,"Pradhan, S; Nanniyur, V",Cisco Systems Inc,2021.0,JOURNAL OF SYSTEMS AND SOFTWARE,ELSEVIER SCIENCE INC,,10.1016/j.jss.2020.110836,20230520-160000,20230521-044735
,wos,Management Consulting Business Models: Operations through and for Digital Transformation,"Management consulting as a service has become part of almost every company's daily business. The growth is being exponential, even with all the non-consensual issues and controversies in the industry. However, the market is increasingly competitive, with new competitors coming from everywhere. At the same time, the world is changing at a speed never seen before, and the challenges are several: automatization, scarcity of resources, democratization of the information, big data, and regulation are some examples. Thus, it's not possible for consulting firms to keep providing the market needs without adapting continuously their own business models. The companies that can outperform these challenges more efficiently will win against the competitors. Investigate which strategies and mechanisms adopt to be agile and flexible enough, in which sectors invest the most, and how reinvent their business model in order to be resilient in a fast changing and technological world are the main objectives of this research. Several interviews with the top management of fifteen of the biggest consulting companies in Portugal were conducted. The results suggested that companies are now trying to differentiate by the services delivered, and these business models' adaptation to the digital transformation is rather than a reality, a need.",Management Consulting; Digital Transformation; Business Models,"Jeronimo, C; Pereira, L; Sousa, H",Instituto Universitario de Lisboa,2019.0,"2019 IEEE INTERNATIONAL CONFERENCE ON ENGINEERING, TECHNOLOGY AND INNOVATION (ICE/ITMC)",IEEE,,,20230520-160000,20230521-044735
,wos,Evolving profiles of financial risk management in the era of digitization: The tomorrow that began in the past,"The initial phases of digitization have automatized the front-end of banks and financial institutions (FIs). This paper documents the automation of the back-end in the current wave of digitization. In particular, it highlights the use of technology in streamlining risk management and its potential to provide competitive advantage to the FIs embracing digitization. For instance, automated big data credit scoring tools built on predictive analytics and machine learning algorithms are employed to examine several credit propositions. This can accurately construct the credit worthiness and risk profile of public, even without any credit history. These developments can widen the access of credit and other financial services to the society. However, on a cautionary note, this study emphasizes that although digitization of back-end financial transactions carries substantive advantages, the FIs must be guarded against cyber, outsourcing, financial exclusion, and macrofinance risks that can manifest with this automation. In this backdrop, the need for robust yet agile regulations and supervisory counsel to control and exploit the digitization towards optimal benefits for banks and FIs and society at large, acquires salience. Furthermore, regulators and supervisory authorities can mitigate the digitization risks and prevent any public fallout by leveraging the use of digitization itself.",,"Chakraborty, G",KREA University; IFMR - Graduate School of Business (GSB); University of Madras,2020.0,JOURNAL OF PUBLIC AFFAIRS,WILEY,,10.1002/pa.2034,20230520-160000,20230521-044735
,wos,Cyber-Physical Systems in Smart City: Challenges and Future Trends for Strategic Research,"Modern cities today compete with each other to be smarter, maintain a more sustainable with high-quality living, acquire talents, and provide jobs. This digital transformation through agile drivers will help address the increasing challenges of urbanization in a couple of decades. The Cyber-Physical System (CPS) is becoming pervasive in every aspect of smart city daily life and considered as one of the four fundamental conceptual approaches of the fourth generation industrial revolution (Industry 4.0). CPS used to describe the next generation of a diverse spectrum of complicated, multidisciplinary, physically comprehending engineered systems that integrates embedded cyber aspects into the physical world. It implants computation technologies, communication control, the convergence of information, and physical processes together with strategic importance internationally. CPS is still a vast research area. As a result, it opens venues for applications across multiple scales. This paper presents an in-depth survey of the related works, focusing on the design and how it relates to different research fields, current concepts, and real-life applications to understand CPS more precisely. Further, it enumerates an extensive set of CPS challenges and opportunities, introducing visionary ideas, research strategies, and future trends expected for future-oriented technological solutions, like cloud computing, Internet of Things, and Big Data. These technological solutions are to play a critical role in CPS research and have significant impacts on the smart city.",Cyber-physical systems; Smart city; Industry 4.0; Big Data; Cloud computing; Internet of Things; Research strategies; Future trends,"Juma, M; Shaalan, K",,2020.0,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ADVANCED INTELLIGENT SYSTEMS AND INFORMATICS 2019,SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-030-31129-2_78,20230520-160000,20230521-044735
,wos,Infoxication in the Genomic Data Era and Implications in the Development of Information Systems,"We live in an age where data acquisition is no longer a problem and the real challenge is how to determine which information is the right one to take important and sometimes difficult decisions. Infoxication (also known as Infobesity or Information Overload) is a term used to describe the difficulty of adapting to new situations and effectively making decisions when there is too much information to manage. With the advent of the Big Data, infoxication is affecting critical domains such as Health Sciences, where tough decisions for patient's health is being taken every day based on heterogeneous, unconnected and sometimes conflicting information. In order to understand the magnitude of the challenge, based on the information publicly available about the genetic causes of the disease and using data quality assessment techniques, we performed an exhaustive analysis of the DNA variations that have been associated to the risk of suffering migraine headache. The same analysis has been repeated 8 months after, and the results have allowed us to exemplify i) how fragile is the information in this domain, ii) the difficulty of finding repositories of contrasted and reliable data, and iii) the need to have information systems that, far from integrating and storing huge volumes of data, are able to support the decision-making process by providing mechanisms agile and flexible enough to be able to adapt to the changing user needs.",Infoxication; Genomics; Information Systems; SILE method,"Palacio, AL; Lopez, OP",Universitat Politecnica de Valencia,2019.0,2019 13TH INTERNATIONAL CONFERENCE ON RESEARCH CHALLENGES IN INFORMATION SCIENCE (RCIS),IEEE,,,20230520-160000,20230521-044735
,wos,Smart hospitality: from smart cities and smart tourism towards agile business ecosystems in networked destinations,"Purpose Building on recent smart hospitality systematic reviews and extensive literature analyses, this paper aims to explore recent developments, themes and issues within smart hospitality. It synthesises existing knowledge, extrapolating forward and contributes to the future development of smart hospitality by serving as a reference to enrich academic/industry discussions and stimulate future research. Design/methodology/approach The research examined 8 recent review articles on smart hospitality and tourism and extracted 145 articles in peer-reviewed sources from Web of Science focussed on smart hospitality. These publications supported in-depth analysis to explore the body of knowledge and develop foresight for the future of smart hospitality within business ecosystems at tourism destinations. It synthesises knowledge and provides the basis for the development of a comprehensive in-depth research agenda in smart hospitality innovations as well as the formulation of agile hospitality ecosystems. Findings This paper illustrates that smart hospitality introduces disruptive innovations that affect the entire hospitality ecosystem. Smart hospitality takes advantage of smart cities and smart tourism towards establishing agile business ecosystems in networked destinations. Having reviewed the existing literature, the study developed a conceptual framework and introduced a comprehensive future research agenda. This includes the drivers of smart hospitality, namely, customer-centricity, personalisation, individualisation and contextualisation; marketing-driven hospitality excellence and metaverse; as well as operation agility, asset strategy, talent management and supplier interoperation. It also identified the foundations that provide the infostructure for smart hospitality, including ambient intelligence, big data, processes and sustainability, providing the capability blocks to co-create value for all stakeholders in the hospitality ecosystem. Originality/value This study conceptualises smart hospitality as a disruptive and innovative power that will affect the competitiveness of hospitality and tourism organisations as part of a comprehensive ecosystem. It identifies the key stakeholders and explores how they can take advantage of emerging developments. This paper proposes the drivers and foundation for future research on smart hospitality. The research provides a conceptual synthesis of the literature and the concepts that have been elaborated. The foundations are effectively the infostructure that enables the drivers to add value to different stakeholders. Key issues are identified to stimulate further research on the area to support smart hospitality development and adoption.",Smart hospitality; Hospitality ecosystem; Research directions,"Buhalis, DT; O'Connor, P; Leung, R",Bournemouth University; Hong Kong Polytechnic University; University of South Australia; I Shou University,2023.0,INTERNATIONAL JOURNAL OF CONTEMPORARY HOSPITALITY MANAGEMENT,EMERALD GROUP PUBLISHING LTD,,10.1108/IJCHM-04-2022-0497,20230520-160000,20230521-044735
,wos,A Framework for Partitioning Support Vector Machine Models on Edge Architectures,"Current IoT applications generate huge volumes of complex data that requires agile analysis in order to obtain deep insights, often by applying Machine Learning (ML) techniques. Support vector machine (SVM) is one such ML technique that has been used in object detection, image classification, text categorization and Pattern Recognition. However, training even a simple SVM model on big data takes a significant amount of computational time. Due to this, the model is unable to react and adapt in real-time. There is an urgent need to speedup the training process. Since organizations typically use the cloud for this data processing, accelerating the training process has the advantage of bringing down costs. In this paper, we propose a model partitioning approach that partitions the tasks of Stochastic Gradient Descent based Support Vector Machines (SGD-SVM) on various edge devices for concurrent computation, thus reducing the training time significantly. The proposed partitioning mechanism not only brings down the training time but also maintains the approximate accuracy over the centralized cloud approach. With a goal of developing a smart objection detection system, we conduct experiments to evaluate the performance of the proposed method using SGD-SVM on an edge based architecture. The results illustrate that the proposed approach significantly reduces the training time by 47%, while decreasing the accuracy by 2%, and offering an optimal number of partitions.",partitioning; edge computing; SGD-SVM,"Sahi, M; Al Maruf, M; Azim, A; Auluck, N",Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Ropar,2021.0,2021 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP 2021),IEEE,,10.1109/SMARTCOMP52413.2021.00062,20230520-160000,20230521-044735
,wos,"Operational Data-Driven Intelligent Modelling and Visualization System for Real-World, On-Road Vehicle Emissions-A Case Study in Hangzhou City, China","On-road vehicle emissions play a crucial role in affecting air quality and human exposure, particularly in megacities. In the absence of comprehensive traffic monitoring networks with the general lack of intelligent transportation systems (ITSs) and big-data-driven, high-performance-computing (HPC) platforms, it remains challenging to constrain on-road vehicle emissions and capture their hotspots. Here, we established an intelligent modelling and visualization system driven by ITS traffic data for real-world, on-road vehicle emissions. Based on the HPC platform (named City Brain) and an agile Web Geographic Information System (WebGISs), this system can map real-time (hourly), hyperfine (10 similar to 1000 m) vehicle emissions (e.g., PM2.5, NOx, CO, and HC) and associated traffic states (e.g., vehicle-specific categories and traffic fluxes) over the Xiaoshan District in Hangzhou. Our results show sharp variations in on-road vehicle emissions on small scales, which even fluctuated up to 31.2 times within adjacent road links. Frequent and widespread emission hotspots were also exposed. Over custom spatiotemporal scopes, we virtually investigated and visualized the impacts of traffic control policies on the traffic states and on-road vehicle emissions. Such results have important implications for how traffic control policies should be optimized. Integrating this system with chemical transport models and air quality measurements would bridge the technical gap between air pollutant emissions, concentrations, and human exposure.",big-data intelligent system; on-road vehicle emissions; traffic monitoring; hyperfine modelling; real-time visualization,"Wang, L; Chen, X; Xia, Y; Jiang, LH; Ye, JJ; Hou, TY; Wang, LQ; Zhang, YB; Li, MY; Li, Z; Song, Z; Jiang, YP; Liu, WP; Li, PF; Zhang, XY; Yu, SC",Zhejiang University; Hebei Agricultural University; China Meteorological Administration; Chinese Academy of Meteorological Sciences (CAMS),2022.0,SUSTAINABILITY,MDPI,,10.3390/su14095434,20230520-160000,20230521-044735
,wos,Brains for Dementia Research: Evolution in a Longitudinal Brain Donation Cohort to Maximize Current and Future Value,"Brain banking has a long and distinguished past, contributing greatly to our understanding of human neurological and psychiatric conditions. Brain banks have been operationally diverse, collecting primarily end stage disease, with variable quality clinical data available, yet it is now recognized the most informative brain donations are from those in longitudinally studied cohorts. The Brains for Dementia Research (BDR) cohort and program was for planned brain donation across five UK brain banks and one donation point, with standardized operating procedures, following longitudinal clinical and psychometric assessments for people with no cognitive impairment as well as those with dementia. Lay representatives with experience of dementia were involved from inception of BDR and 74.5% of all enquiries about participation came through routes that were directly attributable to or influenced by lay representatives. Ten years after inception, this ongoing project has received over 700 brain donations from the recruited cohort of 3,276 potential brain donors. At cohort census for this paper, 72.2% of the living cohort have no cognitive impairment by assessment, whereas only 28.3% of the donated cohort were without cognitive impairment. It is important that brain banks are agile and reflect the changing needs of the research community, given that `big data', readiness cohorts, and GWAS demand large sample numbers of highly characterized individuals to facilitate new approaches and understanding of pathological processes in dementia.",Brain donation; cohort; control; dementia; research tissue bank,"Francis, PT; Costello, H; Hayes, GM",RLUK- Research Libraries UK; University of London; King's College London,2018.0,JOURNAL OF ALZHEIMERS DISEASE,IOS PRESS,,10.3233/JAD-180699,20230520-160000,20230521-044735
,wos,A Simulator and Compiler Framework for Agile Hardware-Software Co-design Evaluation and Exploration,"As Moore's Law has slowed and Dennard Scaling has ended, architects are increasingly turning to heterogeneous parallelism and hardware-software co-design. These trends present new challenges for simulation-based performance assessments that are central to early-stage architectural exploration. Simulators must be lightweight to support heterogeneous combinations of general-purpose cores and specialized processing units. They must also support agile exploration of hardware-software co-design, i.e. changes in the programming model, compiler, ISA, and specialized hardware. To meet these challenges, we describe our compiler and simulator pair: DEC++ and MosaicSim. Together, they provide a lightweight, modular simulator for heterogeneous systems, offering accuracy and agility designed specifically for hardware-software co-design explorations. The simulator and corresponding compiler were developed as part of the DECADES project, a multi-team effort to design and tape out a new heterogeneous architecture. We will present two case-studies in important data-science applications where DEC++ and MosaicSim enable straightforward design space explorations for emerging full-stack systems.",performance modeling; heterogeneous systems; hardware-software co-design; LLVM simulation,"Sorensen, T; Manocha, A; Tureci, E; Orenes-Vera, M; Aragon, JL; Martonosi, M",University of California System; University of California Santa Cruz; Princeton University; University of Murcia,2020.0,2020 IEEE/ACM INTERNATIONAL CONFERENCE ON COMPUTER AIDED-DESIGN (ICCAD),IEEE,,10.1145/3400302.3415751,20230520-160000,20230521-044735
,wos,E-monitoring the nature of water,"The critical need for hydrological observations in support of water resources management, particularly during extreme events, has transformed traditional methods of hydrological data management. This transformation has given rise to a framework of e-monitoring the hydrological cycle, the aim of which is to improve understanding of the nature of water. New trends in data science, coupled with increasing technological evolution, make the new generation of data systems more agile and responsive to the needs and expectations for efficient and effective data sharing and service delivery. The WMO Hydrological Observing System was designed around the integration of observations, data exchange, research, data processing, modelling and forecasting, in such a way that societal needs for disaster risk reduction, improved sustainability of environmental resources, climate resilience and economic growth can be effectively met. With its implementation of conceptual functionalities for sustainable data management, the WHOS operational architecture is hydrology's system for the future.",hydrological observations; data discovery and access; interoperability,"Pecora, S; Lins, HF",,2020.0,HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES,TAYLOR & FRANCIS LTD,,10.1080/02626667.2020.1724296,20230520-160000,20230521-044735
,wos,Software packaging and distribution for LHCb using Nix,"Software is an essential and rapidly evolving component of modern high energy physics research. The ability to be agile and take advantage of new and updated packages from the wider data science community is allowing physicists to efficiently utilise the data available to them. However, these packages often introduce complex dependency chains and evolve rapidly introducing specific, and sometimes conflicting, version requirements which can make managing environments challenging. Additionally, there is a need to replicate old environments when generating simulated data and to utilise pre-existing datasets. Nix is a purely functional package manager which allows for software to be built and distributed with fully specified dependencies, making packages independent from those available on the host. Builds are reproducible and multiple versions/configurations of each package can coexist with the build configuration of each perfectly preserved. Here we will give an overview of Nix followed by the work that has been done to use Nix in LHCb and the advantages and challenges that this brings.",,"Burr, C; Clemencic, M; Couturier, B",N8 Research Partnership; RLUK- Research Libraries UK; University of Manchester; European Organization for Nuclear Research (CERN),2019.0,23RD INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY AND NUCLEAR PHYSICS (CHEP 2018),E D P SCIENCES,,10.1051/epjconf/201921405005,20230520-160000,20230521-044735
,wos,Information Resilience: the nexus of responsible and agile approaches to information use,"The appetite for effective use of information assets has been steadily rising in both public and private sector organisations. However, whether the information is used for social good or commercial gain, there is a growing recognition of the complex socio-technical challenges associated with balancing the diverse demands of regulatory compliance and data privacy, social expectations and ethical use, business process agility and value creation, and scarcity of data science talent. In this vision paper, we present a series of case studies that highlight these interconnected challenges, across a range of application areas. We use the insights from the case studies to introduce Information Resilience, as a scaffold within which the competing requirements of responsible and agile approaches to information use can be positioned. The aim of this paper is to develop and present a manifesto for Information Resilience that can serve as a reference for future research and development in relevant areas of responsible data management.",Information Resilience; Data quality; Responsible data science; Effective information use; Value creation,"Sadiq, S; Aryani, A; Demartini, G; Hua, W; Indulska, M; Burton-Jones, A; Khosravi, H; Benavides-Prado, D; Sellis, T; Someh, I; Vaithianathan, R; Wang, S; Zhou, XF",University of Queensland; Swinburne University of Technology; Auckland University of Technology; Facebook Inc; Hong Kong University of Science & Technology,2022.0,VLDB JOURNAL,SPRINGER,,10.1007/s00778-021-00720-2,20230520-160000,20230521-044735
,wos,Structured Data for Product Performance Improvement,"Aftermarket reliability data is a cornerstone to understand the performance of one's products against requirements. A successful aftermarket data system goes beyond the basics of supplying reliability figures. Its attributes also include additional metrics for an effective alerting and reporting system to enable proactive response to aftermarket issues. While these system features are key, the implementation and maintenance methodology of the system is crucial to its success. This is because these systems involve big data. In the case presented, it is data which spans several years, for a variety of model numbers on a variety of aircraft platforms or applications. Each set of circumstances yields different reliability figures and associated metrics. With this big data, it is equally crucial to its success to have a methodology to address data integrity, the speed of data, and the portability of data. Our solution with this successful methodology of these features is called Structured Data. A good aftermarket data system is a backbone for any successful organization. A good system in the aerospace industry goes beyond ATA Spec 2000 [1] formatted data and standard reliability figures such as MTBUR (Mean Time Between Unscheduled Removal) and MTBF (Mean Time Between Failure). It is also beyond implementing a Failure Reporting and Corrective Action System (FRACAS). A comprehensive system in the aerospace industry includes several additional measures (i.e. frequency, severity, risk) to represent the Voice of the Customer. And with a built-in mechanism for proactive response to the data, the system can then be considered World-Class. While designing a system with these features is important, its success also hinges upon the methodology of implementation and maintenance. As stated earlier, aftermarket data is considered big data due to the volume of highly specific data. With this big data, it is critical to success to address data integrity, the speed of data, and the portability of data all within a user-friendly experience. For data integrity, do we trust the data? This takes on many forms from cross referencing input and output data to determining an accurate mixed fleet factor. For speed of data, do we have a system in place to handle the cadence of data efficiently? For portability, do we structure our data in such a way where we can be agile to serve potential changes to our system or new systems as our company evolves? For a user-friendly experience, can we structure the data for intuitive analysis for all stakeholders? Thus in the proposed system, all of these aspects of data integrity, the speed of data, the portability of data, and formatting the data per stakeholders are addressed. Our solution with this successful methodology of these features is called Structured Data. The benefits of this newly developed Structured Data extend beyond ATA Spec 2000 in which it is based. The data is structured in a dynamic and interactive environment. This environment includes intuitive analysis and a system of prioritization for corrective action. The key benefit of this Structured Data system is proactive response to aftermarket data analysis.",Product performance; Reliability metrics; MTBUR / MTBF; FRACAS; Voice of Customer; Big data; Data integrity Speed of data; Portability of data; Natural Language Programming (NLP),"Peter, P; Parendo, C",Raytheon Technologies; Collins Aerospace,2021.0,67TH ANNUAL RELIABILITY & MAINTAINABILITY SYMPOSIUM (RAMS 2021),IEEE,,10.1109/RAMS48097.2021.9605770,20230520-160000,20230521-044735
,wos,Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter,"The amount of information needed to acquire knowledge on today's acquisition systems is growing exponentially due to more complex, higher resolution, software-intensive acquisition systems that need to operate in System-of-Systems (SoS), Family-of-Systems (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary to rapidly collect, aggregate, and analyze this information have not evolved as a whole in conjunction with this increased system complexity and, therefore, has made analysis and evaluation increasingly deficient and ineffective. The Test Resource Management Center's (TRMC's) vision is to build a DoD test and evaluation (T&E) knowledge management (KM) and analysis capability that leverages commercial big data analysis and cloud computing technologies to improve evaluation quality and reduce decision-making time. An evaluation revolution, starting with the Joint Strike Fighter (JSF) program, is underway to ensure the T&E community can support the demands of next-generation weapon systems. The true product of T&E is knowledge ascertained through the collection of information about a system or item under test. However, the T&E community's ability to provide this knowledge is hampered by more complex systems, more complex environments, and the need to be more agile in support of strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This increased complexity and need for speed cause delayed analysis and problems that go undetected during T&E. The primary reason for these shortfalls is antiquated tools and processes that make data hard to locate, aggregate, and convert into knowledge. In short, DoD has not evolved its evaluation infrastructure as its weapon systems have evolved. Conversely, commercial entities, such as medical observation and diagnosis, electric power distribution, retail, and industrial manufacturing, have embraced agility in their methodologies while modernizing analytics capabilities to keep up with the massive influx of data. Raw physical sensors could provide data, higher-quality image or video cameras, radio frequency identification (RFID) devices, faster data collectors, more detailed point-of-sale information or digitized records, and ultimately is providing more data to analysts in size and complexity than ever before. As more data has become available, an interrelated phenomenon is the desire of analysts to ask more detailed questions about their consumers and their business infrastructure. To drive the process of implementing big data analytics, businesses have begun establishing analytics centers which either take pre-defined business cases and apply methods to address them or implement existing knowledge within the data architecture to create a higher level of awareness to business groups or the company at-large. To meet these demands, data storage and computation architectures have become more sophisticated, dozens of technologies were developed for large-scale processing (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data to be processed and actioned on in real-time as it is collected have become commonplace. The net result of these commercial best practices is a solid foundation for the DoD to transform how it uses data to achieve faster, better, and smarter decisions throughout the acquisition lifecycle.",Big Data; Data Analytics; Knowledge Management; Data Management; Virtualization; Cloud Computing; Predictive Maintainance; Department of Defense; Test and Evaluation,"Norman, R; Bolin, J; Powell, ET; Amin, S; Nacker, J",,2018.0,2018 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,,20230520-160000,20230521-044735
,wos,LEADERSHIP DECISION-MAKING PROCESSES IN THE CONTEXT OF DATA DRIVEN TOOLS,"Digital economy vast streams of data have created a new paradigm for the business intelligence processes, increasing the potential of advanced analytics and cognitive data tools. Big data structures are used in business intelligence to work with massive amount of dataset to extract value for effective business decision. The current research seeks to address the following question: how can leaders integrate new technology in their decision process to achieve business goals? Emerging technologies directly created organizational power shift and internal bureaucracy adjustments as a result of data transparency trend and decision-making levels changes. A new type of organizational culture and the leadership role in the organizational development becomes necessary. The significant impact over the organizational systems and business goals requires a strategic approach in implementing data driven decision-making processes.",Big Data; Advanced analytics; Machine learning; Artificial intelligence; Organization culture; Data management; Technology; Digital Economy; Business Intelligence; Agile; Continuous Development,"Bratasanu, V",,2018.0,QUALITY-ACCESS TO SUCCESS,SOC ROMANA PENTRU ASIGURAREA CALITATII,,,20230520-160000,20230521-044735
,wos,Towards Prediction of Security Attacks on Software Defined Networks: A Big Data Analytic Approach,"Cyber-physical systems (CPS) tightly integrate physical and computing processes by monitoring and control data interacting between them via underlying networks. Software Defined Network (SDN) Technology has increasingly become essential in many advanced computer networks, including those in modern CPS, to provide flexible and agile network development. Despite many benefits that SDN offers, malicious attacks that can eventually prevent network services are unavoidable. Among the most predominant attacks on SDN controller layer, Link Discovery Attack and ARP (Address Resolution Protocol) Spoofing Attack are fundamental in that they are the gateways of many other SDN threats and attacks. To defend these attacks, most existing techniques either rely on relatively complex data validation techniques or use thresholds that can be subjective and unable to detect more than one type of attacks at a time if one deciding factor is used. While Big data technology, particularly machine learning, has been widely used for intrusion/anomaly detection, little has been done in SDN. This paper explores how well this technology can be used to predict these SDN attacks. By employing typical machine learning algorithms on simulated data of routing in SDN when attacks occur, preliminary results, obtained from four machine learning models, show the average area under ROC curve of over 96% and 92% for sample size 50,970 (12 switches) and 60,000 (20 switches), respectively. Further experiments show near-linear scaling in training time for the best performing algorithm when sample size grows up to 100,000.",Software-Defined Networking; SDN-specific security; Link Discovery attack; ARP Spoofing attack; Machine Learning; Data Analytic Applications,"Unal, E; Sen-Baidya, S; Hewett, R",Texas Tech University System; Texas Tech University,2018.0,2018 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),IEEE,,,20230520-160000,20230521-044735
,wos,A Cost-Efficient Container Orchestration Strategy in Kubernetes-Based Cloud Computing Infrastructures with Heterogeneous Resources,"Containers, as a lightweight application virtualization technology, have recently gained immense popularity in mainstream cluster management systems like Google Borg and Kubernetes. Prevalently adopted by these systems for task deployments of diverse workloads such as big data, web services, and loT, they support agile application deployment, environmental consistency, OS distribution portability, application-centric management, and resource isolation. Although most of these systems are mature with advanced features, their optimization strategies are still tailored to the assumption of a static cluster. Elastic compute resources would enable heterogeneous resource management strategies in response to the dynamic business volume for various types of workloads. Hence, we propose a heterogeneous task allocation strategy for cost-efficient container orchestration through resource utilization optimization and elastic instance pricing with three main features. The first one is to support heterogeneous job configurations to optimize the initial placement of containers into existing resources by task packing. The second one is cluster size adjustment to meet the changing workload through autoscaling algorithms. The third one is a rescheduling mechanism to shut down underutilized VM instances for cost saving and reallocate the relevant jobs without losing task progress. We evaluate our approach in terms of cost and performance on the Australian National Cloud Infrastructure (Nectar). Our experiments demonstrate that the proposed strategy could reduce the overall cost by 23% to 32% for different types of cloud workload patterns when compared to the default Kubernetes framework.",Cluster management; container orchestration; resource heterogeneity; cost efficiency,"Zhong, ZH; Buyya, R",University of Melbourne,2020.0,ACM TRANSACTIONS ON INTERNET TECHNOLOGY,ASSOC COMPUTING MACHINERY,,10.1145/3378447,20230520-160000,20230521-044735
,wos,Machine Learning based Digital Twin Framework for Production Optimization in Petrochemical Industry,"Digital twins, along with the internet of things (IoT), data mining, and machine learning technologies, offer great potential in the transformation of today's manufacturing paradigm toward intelligent manufacturing. Production control in petrochemical industry involves complex circumstances and a high demand for timeliness; therefore, agile and smart controls are important components of intelligent manufacturing in the petrochemical industry. This paper proposes a framework and approaches for constructing a digital twin based on the petrochemical industrial IoT, machine learning and a practice loop for information exchange between the physical factory and a virtual digital twin model to realize production control optimization. Unlike traditional production control approaches, this novel approach integrates machine learning and real-time industrial big data to train and optimize digital twin models. It can support petrochemical and other process manufacturing industries to dynamically adapt to the changing environment, respond in a timely manner to changes in the market due to production optimization, and improve economic benefits. Accounting for environmental characteristics, this paper provides concrete solutions for machine learning difficulties in the petrochemical industry, e.g., high data dimensions, time lags and alignment between time series data, and high demand for immediacy. The approaches were evaluated by applying them in the production unit of a petrochemical factory, and a model was trained via industrial IoT data and used to realize intelligent production control based on real-time data. A case study shows the effectiveness of this approach in the petrochemical industry.",digital twin; machine learning; internet of things; petrochemical industry; production control optimization,"Min, QF; Lu, YG; Liu, ZY; Su, C; Wang, B",Dalian University of Technology,2019.0,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,ELSEVIER SCI LTD,,10.1016/j.ijinfomgt.2019.05.020,20230520-160000,20230521-044735
,wos,Di-ANFIS: an integrated blockchain-IoT-big data-enabled framework for evaluating service supply chain performance,"Service supply chain management is a complex process because of its intangibility, high diversity of services, trustless settings, and uncertain conditions. However, the traditional evaluating models mostly consider the historical performance data and fail to predict and diagnose the problems' root. This paper proposes a distributed, trustworthy, tamper-proof, and learning framework for evaluating service supply chain performance based on blockchain and adaptive network-based fuzzy inference systems (ANFIS) techniques, named Di-ANFIS. The main objectives of this research are as follows: (1) presenting hierarchical criteria of service supply chain performance to cope with the diagnosis of the problems' root; (2) proposing a smart learning model to deal with the uncertainty conditions by a combination of neural network and fuzzy logic; and (3) introducing a distributed blockchain-based framework due to the dependence of ANFIS on big data and the lack of trust and security in the supply chain. Furthermore, the proposed six-layer conceptual framework consists of the data layer, connection layer, blockchain layer, smart layer, ANFIS layer, and application layer. This architecture creates a performance management system using the Internet of Things, smart contracts, and ANFIS based on the blockchain platform. The Di-ANFIS model provides a performance evaluation system without needing a third party and a reliable intermediary that provides an agile and diagnostic model in a smart and learning process. It also saves computing time and speeds up information flow.",blockchain; industry 4.0; Internet of Things (IoT); big data; service supply chain; performance evaluation,"Bamakan, SMH; Faregh, N; ZareRavasan, A",University of Yazd; University of Yazd; Masaryk University Brno,2021.0,JOURNAL OF COMPUTATIONAL DESIGN AND ENGINEERING,OXFORD UNIV PRESS,,10.1093/jcde/qwab007,20230520-160000,20230521-044735
,wos,Bean counter to value-adding business partner: the changing role of the accountant and situated rationality in a multinational firm,"Purpose This paper aims to explore the changing role of the accountant amid multiple drivers, responses of accountants and situated rationality in a multinational firm, Max-choice Lanka. Design/methodology/approach It adopts the single-site case study approach under the qualitative methodology and leans on institutional theory, specifically Ter Bogt and Scapens (2019) framework. Findings The case study findings reveal that the role of the accountant has undergone change amid local and broader institutions linked to organizational culture/norms, the influence of the parent company, global trends and technological advancements. Based on evolving situated rationalities, the contemporary accountant performs an agile role as a value-adding business partner; data scientist; strategic decision-maker; and a cross-functional team member. Practical implications At the practice level, identifying drivers influencing the changing role of accountants enables organizations to shape their accounting functions attuned to evolving needs by implementing appropriate strategies and recruiting competent personnel. In the realm of education, it calls for incorporating areas such as big data analytics, artificial intelligence, reporting nonfinancial information and integrated accounting software to the accounting curricular and upskill students based on industry expectations catering to changing roles. Originality/value This paper adds to the ongoing debate on the contemporary role of the accountant. Capitalizing on case study data, this research illuminates the influence of multiplicity of institutions, different forms and situated rationality within this changing role and extends the Ter Bogt and Scapens (2019) framework.",Accountant; Case study; Situated rationality; Changing role; Forms of rationality; Multiplicity of institutions,"Samanthi, D; Gooneratne, T",University of Colombo,,JOURNAL OF ACCOUNTING AND ORGANIZATIONAL CHANGE,EMERALD GROUP PUBLISHING LTD,,10.1108/JAOC-04-2022-0063,20230520-160000,20230521-044735
,wos,Investigating potential interventions on disruptive impacts of Industry 4.0 technologies in circular supply chains: Evidence from SMEs of an emerging economy,"As a transversal theme, the intertwining of digitalization and sustainability has crossed all Supply Chains (SCs) levels dealing with widespread environmental and societal concerns. This paper investigates the potential in-terventions and disruptive impacts that Industry 4.0 technologies may have on pharmaceutical Circular SCs (CSCs). To accomplish this, a novel method involving a literature review and Pythagorean fuzzy-Delphi has initially been employed to identify and screen categorized lists of Industry 4.0 Disruptive Technologies (IDTs) and their impacts on pharmaceutical CSC. Subsequently, the weight of finalized impacts and the performance score of finalized IDTs have simultaneously been measured via a novel version of Pythagorean fuzzy SECA (Simultaneously Evaluation of Criteria and Alternatives). Then, the priority of each intervention for disruptive impacts of Industry 4.0 has been determined via the Hanlon method. This is one of the first papers to provide in-depth insights into advancing the study of the disruptive action of Industry 4.0 technologies cross-fertilizing CE throughout pharmaceutical SCs in the emerging economy of Iran. The results indicate that digital technologies such as Big Data Analytics, Global Positioning Systems, Enterprise Resource Planning, and Digital Platforms are quite available in the Irans' pharmaceutical industry. These technologies, along with four available in-terventions, e.g., environmental regulations, subsidy, fine, and reward, would facilitate moving towards a lean, agile, resilient, and sustainable supply chain through the efficient utilization of resources, optimized waste management, and substituting the human workforce by machines.",Industry 4; 0 technologies; Pythagorean fuzzy Delphi; Pythagorean fuzzy SECA; Hanlon method,"Mahdiraji, HA; Yaftiyan, F; Abbasi-Kamardi, A; Garza-Reyes, JA",RLUK- Research Libraries UK; University of Leicester; University of Tehran; University of Derby,2022.0,COMPUTERS & INDUSTRIAL ENGINEERING,PERGAMON-ELSEVIER SCIENCE LTD,,10.1016/j.cie.2022.108753,20230520-160000,20230521-044735
,wos,Combining Terrier with Apache Spark to create Agile Experimental Information Retrieval Pipelines,"Experimentation using information retrieval (IR) systems has traditionally been a procedural and laborious process. Queries must be run on an index, with any parameters of the retrieval models suitably tuned. With the advent of learning-to-rank, such experimental processes (including the appropriate folding of queries to achieve cross-fold validation) have resulted in complicated experimental designs and hence scripting. At the same time, machine learning platforms such as Scikit Learn and Apache Spark have pioneered the notion of an experimental pipeline, which naturally allows a supervised classification experiment to be expressed as a series of stages, which can be learned or transformed. In this demonstration, we detail Terrier-Spark, a recent adaptation to the Terrier IR platform which permits it to be used within the experimental pipelines of Spark. We argue that this (1) provides an agile experimental platform for information retrieval, comparable to that enjoyed by other branches of data science; (2) aids research reproducibility in information retrieval by facilitating easily-distributable notebooks containing conducted experiments; and (3) facilitates the teaching of information retrieval experiments in educational environments.",,"Macdonald, C",RLUK- Research Libraries UK; University of Glasgow,2018.0,ACM/SIGIR PROCEEDINGS 2018,ASSOC COMPUTING MACHINERY,,10.1145/3209978.3210174,20230520-160000,20230521-044735
,wos,Working Life Within a Hybrid World - How Digital Transformation and Agile Structures Affect Human Functions and Increase Quality of Work and Business Performance,"Digitization is dramatically changing economy and society. With current developments in the field of e.g. artificial intelligence and machine learning, big data and data analytics, cloud computing, conversational systems and adaptive architectures, robotics as well as virtual and augmented reality work life is facing huge challenges. On the other side the networking over the internet, more effective handling and sharing of data and new forms of human-machine-collaboration offer a great variety of potentials for designing even more flexible business processes, agile working structures and even smarter working setups and environments. Technique, organizational aspects and humans in the future are going to be within a new triad. Instead of taking the role of a dominator or captain as in former times, humans now more and more have to fulfill tasks as a conductor. The role of building up and interacting within new hybrid networks and holistic systems is gaining higher importance - leading to massive changes with reference to all dimensions of work. Total new requirements concerning work objectives, working tasks, work equipment, workspace as well as new challenges for organization, qualification, employment and leadership arise. Work is becoming more and more digitally and going to look quite different than expected today. Combining the physical and virtual world is representing the key success factor for future work. The study examines how digitization is going to penetrate working life further on displaying central measures and selected solutions for resulting organizational structures, human qualification needs and optimized working conditions in a hybrid world.",Agility; Collaboration; Data; Digitization; Industry 4.0; Organization; Performance; Qualifications; Requirements; Technology,"Bauer, W; Schlund, S; Vocke, C",Fraunhofer Gesellschaft,2018.0,"ADVANCES IN HUMAN FACTORS, BUSINESS MANAGEMENT AND LEADERSHIP, AHFE 2017",SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-319-60372-8_1,20230520-160000,20230521-044735
,wos,"Big data empowered agility for dynamic, volatile, and time-sensitive service industries: the case of tourism sector","Purpose Dynamic, volatile, and time-sensitive industries, such as tourism, travel and hospitality require agility and market intelligence to create value and achieve competitive advantage. The aim of the current study is to examine the influence of big data (BD) on the performance of service organizations and to probe for a deeper understanding of implementing BD, based on available technologies. Design/methodology/approach An ethnographic study was conducted following an abductive approach. A primary qualitative research scheme was used with 35 information technology and database professionals participating in five online focus groups of seven participants each. Analytical themes were developed simultaneously with the literature being revisited throughout the study to ultimately create sets of common themes and dimensions. Findings BD can help organizations build agility, especially within dynamic industries, to better predict customer behavioral patterns and make tailor-made propositions from the BD. An integrated BD-specific framework is proposed to address value according to the dimensions of need, value, time and utility. Research limitations/implications Little research exists on the key drivers of BD use for dynamic, real-time and agile businesses. This research adds to the developing literature on BD applications to support organizational decision-making and business performance in the tourism industry. Originality/value This study responds to scholars' recent calls for more empirical research with contextual understanding of the use of BD to add value in marketing intelligence within business ecosystems. It delineates factors contributing to BD value creation and explores the impacts on the respective service encounters.",Big data; IT experts; Online focus groups; Tourism organizations; BD-specific framework,"Stylos, N; Zwiegelaar, J; Buhalis, D",RLUK- Research Libraries UK; University of Bristol; Oxford Brookes University; Bournemouth University,2021.0,INTERNATIONAL JOURNAL OF CONTEMPORARY HOSPITALITY MANAGEMENT,EMERALD GROUP PUBLISHING LTD,,10.1108/IJCHM-07-2020-0644,20230520-160000,20230521-044735
,wos,Data Centered and Usage-Based Security Service,"Protecting Information Systems (IS) relies traditionally on security risk analysis methods. Designed for well-perimetrised environments, these methods rely on a systematic identification of threats and vulnerabilities to identify efficient control-centered protection countermeasures. Unfortunately, this does not fit security challenges carried out by the opened and agile organizations provided by the Social, Mobile, big data Analytics, Cloud and Internet of Things (SMACIT) environment. Due to their inherently collaborative and distributed organization, such multi-tenancy systems require the integration of contextual vulnerabilities, depending on the a priori unknown way of using, storing and exchanging data in opened cloud environment. Moreover, as data can be associated to multiple copies, different protection requirements can be set for each of these copies, which may lead the initial data owner lose control on the data protection. This involves (1) turning the traditional control-centered security vision to a dynamic data-centered protection and even (2) considering that the way a data is used can be a potential threat that may corrupt data protection efficiency. To fit these challenges, we propose a Data-centric Usage-based Protection service (DUP). This service is based on an information system meta-model, used to identify formally data assets and store the processes using copies of these assets. To define a usage-entered protection, we extend the Usage Based Access Control model, which is mostly focused on managing CRUD operations, to more complex operation fitting the SMACIT context. These usage rules are used to generate smart contracts, storing usage consents and managing usage control for cloud services.",Privacy; Data-driven organization; Blockchain; GDPR; Usage governance,"Yuan, JY; Biennier, F; Benharkat, N",Centre National de la Recherche Scientifique (CNRS); Institut National des Sciences Appliquees de Lyon - INSA Lyon,2021.0,"SERVICE-ORIENTED COMPUTING, ICSOC 2020",SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-030-76352-7_42,20230520-160000,20230521-044735
,wos,An Approach to Build e-Health IoT Reactive Multi-Services Based on Technologies around Cloud Computing for Elderly Care in Smart City Homes,"Although there are e-health systems for the care of elderly people, the reactive characteristics to enhance scalability and extensibility, and the use of this type of system in smart cities, have been little explored. To date, some studies have presented healthcare systems for specific purposes without an explicit approach for the development of health services. Moreover, software engineering is hindered by agile management challenges regarding development and deployment processes of new applications. This paper presents an approach to develop health Internet of Things (IoT) reactive applications that can be widely used in smart cities for the care of elderly individuals. The proposed approach is based on the Rozanski and Woods's iterative architectural design process, the use of architectural patterns, and the Reactive Manifesto Principles. Furthermore, domain-driven design and the characteristics of the emerging fast data architecture are used to adapt the functionalities of services around the IoT, big data, and cloud computing paradigms. In addition, development and deployment processes are proposed as a set of tasks through DevOps techniques. The approach validation was carried out through the implementation of several e-health services, and various workload experiments were performed to measure scalability and performance in certain parts of the architecture. The system obtained is flexible, scalable, and capable of handling the data flow in near real time. Such features are useful for users who work collaboratively in the care of elderly people. With the accomplishment of these results, one can envision using this approach for building other e-health services.",cloud computing; container as a service; DevOps; e-health; emerging fast data architecture; Internet of Things (IoT); reactive system,"Perez, LJ; Salvachua, J",Universidad Politecnica de Madrid,2021.0,APPLIED SCIENCES-BASEL,MDPI,,10.3390/app11115172,20230520-160000,20230521-044735
,wos,"Enterprise Meta-architecture for Megacorps of Unmanageably Great Size, Speed, and Technological Complexity","The discipline of enterprise architecture (EA) provides valuable tools for aligning an organization's business strategy and processes, IT strategy and systems, personnel structures, and organizational culture, with the goal of enhancing organizational agility, adaptability, and efficiency. However, the centralized and exhaustively detailed approach of conventional EA is susceptible to failure when employed in organizations demonstrating exceedingly great size, speed of operation and change, and IT complexity - a combination of traits that characterizes, for example, some emerging types of technologized oligopolistic megacorps reflecting the Industry 4.0 paradigm. This text develops the conceptual basis for a variant form of enterprise architecture that can be used to enact improved target architectures for organizations whose characteristics would otherwise render them unmanageable from the perspective of conventional EA. The proposed approach of enterprise meta-architecture (or EMA) disengages human enterprise architects from the fine-grained details of architectural analysis, design, and implementation, which are handled by artificially intelligent systems functioning as active agents rather than passive tools. The role of the human enterprise architect becomes one of determining the types of performance improvements a target architecture should ideally generate, establishing the operating parameters for an EMA system, and monitoring and optimizing its functioning. Advances in Big Data and parametric design provide models for enterprise meta-architecture, which is distinct from other new approaches like agile and adaptive EA. Deployment of EMA systems should become feasible as ongoing advances in AI result in an increasing share of organizational agency and decision-making responsibility being shifted to artificial agents.",Enterprise Architecture; Organizational complexity; Unmanageability; Industry 4.0; Megacorps; Parametric design,"Gladden, ME",Polish Academy of Sciences; Institute of Computer Science of the Polish Academy of Sciences,2019.0,"INFORMATION SYSTEMS ARCHITECTURE AND TECHNOLOGY, ISAT 2018, PT III",SPRINGER INTERNATIONAL PUBLISHING AG,,10.1007/978-3-319-99993-7_22,20230520-160000,20230521-044735
,wos,Impact of artificial intelligence-driven big data analytics culture on agility and resilience in humanitarian supply chain: A practice-based view,"This study attempts to understand the role of artificial intelligence-driven big data analytics capability in hu-manitarian relief operations. These disasters play an important role in mobilizing several organizations to counteract them, but the organizations often find it hard to strike a fine balance between agility and resilience. Operations Management Scholars' opinion remains divided between responsiveness and efficiency. However, to manage unexpected events like disasters, organizations need to be agile and resilient. In previous studies, scholars have adopted the resource-based view or dynamic capability view to explain the combination of re-sources and capabilities (i.e., technology, agility, and resilience) to explain their performance. However, following some recent scholarly debates, we argue that organizational theories like the resource-based view or dynamic capability view are not suitable enough to explain humanitarian supply chain performance. As the underlying assumptions of the commercial supply chain do not hold true in the case of the humanitarian supply chain. We note this as a potential research gap in the existing literature. Moreover, humanitarian organizations remain sceptical regarding the adoption of artificial intelligence-driven big data analytics capability (AI-BDAC) in the decision-making process. To address these potential gaps, we grounded our theoretical model in the practice-based view which is proposed as an appropriate lens to examine the role of practices that are not rare and are easy to imitate in performance. We used Partial Least Squares (PLS) to test our theoretical model and research hypotheses, using 171 useable responses gathered through a web survey of international non-governmental organizations (NGOs). The findings of our study suggest that AI-BDAC is a significant determi-nant of agility, resilience, and performance of the humanitarian supply chain. Furthermore, the reduction of the level of information complexity (IC) on the paths joining agility, resilience, and performance in the humanitarian supply chain. These results offer some useful theoretical contributions to the contingent view of the practice -based view. In a way, we have tried to establish empirically that the humanitarian supply chain designs are quite different from their commercial counterparts. Hence, the use of a resource-based view or dynamic capa-bility view as theoretical lenses may not help capture true perspectives. Thus, the use of a practice-based view as an alternative theoretical lens provides a better understanding of humanitarian supply chains. We have further outlined the limitations and the future research directions of the study.",Artificial intelligence; Big data analytics; Culture; Supply chain agility; Supply chain resilience; Humanitarian supply chain; Practice-based view; Humanitarian operations management; PLS-SEM,"Dubey, R; Bryde, DJ; Dwivedi, YK; Graham, G; Foropon, C",Montpellier Business School; Liverpool John Moores University; Swansea University; Symbiosis International University; Symbiosis Institute of Business Management (SIBM) Pune; N8 Research Partnership; RLUK- Research Libraries UK; White Rose University Consortium; University of Leeds,2022.0,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,ELSEVIER,,10.1016/j.ijpe.2022.108618,20230520-160000,20230521-044735
,wos,The future of leadership-How is leadership in small and medium-sized enterprises going to change?,"This paper in the Journal Gruppe. Interaktion. Organisation. (GIO) addresses changes in leadership through digitalization and their consequences for leaders. For years, digitalization has been heralding changes such as increasing leadership at a distance or use of digital communication media. Small and medium-sized enterprises (SMEs) now face the task of coping with these changes and have to contend with major uncertainties: What are major determining trends for leaders in SMEs? Which changes will shape leadership and how will they change leadership tasks and success-critical behavior? In semi-structured interviews with seven experts from SMEs we have explored these questions. Trends expected by the experts describe changes in the organizational structures and in work within the company. Structurally, companies will become more agile and diverse, hierarchies will play a less strong role and companies will cooperate more closely with each other. Work will become more location-independent, more influenced by Big Data and many tasks will be made easier or taken over by technology. In relation to established models of leadership tasks and behavior, the experts see a clear shift in tasks in favor of managing human resources, including the development of employees through coaching and the transfer of responsibility. In addition to previous tasks, the experts see managing change as a new task area. This area consists of accompanying change, acting flexibly and agilely, communicating openly and transparently and allowing failure. With regard to changes in success-critical behavior, leaders have to show more strategy orientation, communicate clearly and be open to new ideas and further development.",Leadership; Digitalization; Tasks; Skills; Leadership behavior,"Otting, SK; Masjutin, L; Maier, GW",University of Bielefeld,2021.0,GIO-GRUPPE-INTERAKTION-ORGANISATION-ZEITSCHRIFT FUER ANGEWANDTE ORGANISATIONSPSYCHOLOGIE,SPRINGER VIEWEG-SPRINGER FACHMEDIEN WIESBADEN GMBH,,10.1007/s11612-021-00610-9,20230520-160000,20230521-044735
,wos,Disruptive Technologies for Labor Market Information System Implementation Enhancement in the UAE: A Conceptual Perspective,"In December 2019, the world learned about the first outbreak of the novel coronavirus (COVID-19) that first broke out in Wuhan, China. This limited outbreak in a small province of China has rapidly evolved into a global pandemic that has led to a health and economic crisis. As millions of individuals have lost their lives, others have lost their jobs due to the recession of 2020. While the skills and educational mismatch have been a prevalent problem in the UAE labor market, it is logical to assume that the global pandemic has likely increased this problem's extent. Therefore, there is an urgent need to adopt an agile, innovative solution to address the upcoming challenges in the labor markets due to the lack of skilled resources and the fear of future work amid the COVID-19 pandemic. Since industry and academia have identified skills and educational mismatch as a complex and multivariate problem, the paper builds a conceptual case from a system engineering perspective to solve this problem efficiently. Based on the literature reviewed related to disruptive technologies and labor market management systems, the paper proposes a new implementation approach for an integrated labor market information system enabled by the most widely used disruptive technologies components in the UAE (Machine Learning, AI, Blockchain, Internet of Things, Big Data Analytics, and Cloud Computing). The proposed approach is considered one of the immediate course of actions required to minimize the UAE economy's negative impact due to the presence of the skills and educational mismatch phenomena.",Disruptive technologies; labor market information systems; skills and educational mismatch; future of work; system engineering; system design thinking; COVID-19,"Goher, G; Masrom, M; Amrin, A; Abd Rahim, N",Universiti Teknologi Malaysia,2021.0,INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS,SCIENCE & INFORMATION SAI ORGANIZATION LTD,,,20230520-160000,20230521-044735
,wos,"MosaicSim: A Lightweight, Modular Simulator for Heterogeneous Systems","As Moore's Law has slowed and Dennard Scaling has ended, architects are increasingly turning to heterogeneous parallelism and domain-specific hardware-software co-designs. These trends present new challenges for simulation-based performance assessments that are central to early-stage architectural exploration. Simulators must be lightweight to support rich heterogeneous combinations of general purpose cores and specialized processing units. They must also support agile exploration of hardware-software co-design, i.e. changes in the programming model, compiler, ISA, and specialized hardware. To meet these challenges, we introduce MosaicSim, a lightweight, modular simulator for heterogeneous systems, offering accuracy and agility designed specifically for hardware-software co-design explorations. By integrating the LLVM toolchain, MosaicSim enables efficient modeling of instruction dependencies and flexible additions across the stack. Its modularity also allows the composition and integration of different hardware components. We first demonstrate that MosaicSim captures architectural bottlenecks in applications, and accurately models both scaling trends in a multicore setting and accelerator behavior. We then present two case-studies where MosaicSim enables straightforward design space explorations for emerging systems, i.e. data science application acceleration and heterogeneous parallel architectures.",heterogeneity; hardware-software co-design; performance modeling; multi-core architectures; accelerators,"Matthews, O; Manocha, A; Giri, D; Orenes-Vera, M; Tureci, E; Sorensen, T; Ham, TJ; Aragon, JL; Carloni, LP; Martonosi, M",Princeton University; Columbia University; University of California System; University of California Santa Cruz; Seoul National University (SNU); University of Murcia,2020.0,2020 IEEE INTERNATIONAL SYMPOSIUM ON PERFORMANCE ANALYSIS OF SYSTEMS AND SOFTWARE (ISPASS),IEEE,,10.1109/ISPASS48437.2020.00029,20230520-160000,20230521-044735
,wos,Context and Machine Learning Based Trust Management Framework for Internet of Vehicles,"Trust is one of the core components of any ad hoc network security system. Trust management (TM) has always been a challenging issue in a vehicular network. One such developing network is the Internet of vehicles (IoV), which is expected to be an essential part of smart cities. IoV originated from the merger of Vehicular ad hoc networks (VANET) and the Internet of things (IoT). Security is one of the main barriers in the on-road IoV implementation. Existing security standards are insufficient to meet the extremely dynamic and rapidly changing IoV requirements. Trust plays a vital role in ensuring security, especially during vehicle to vehicle communication. Vehicular networks, having a unique nature among other wireless ad hoc networks, require dedicated efforts to develop trust protocols. Current TM schemes are inflexible and static. Predefined scenarios and limited parameters are the basis for existing TM models that are not suitable for vehicle networks. The vehicular network requires agile and adaptive solutions to ensure security, especially when it comes to critical messages. The vehicle network's wireless nature increases its attack surface and exposes the network to numerous security threats. Moreover, internet involvement makes it more vulnerable to cyberattacks. The proposed TM framework is based on context-based cognition and machine learning to be best suited to IoV dynamics. Machine learning is the best solution to utilize the big data produced by vehicle sensors. To handle the uncertainty Bayesian machine learning statistical model is used. The proposed framework can adapt scenarios dynamically and infer using the maximum possible parameter available. The results indicated better performance than existing TM methods. Furthermore, for future work, a high-level machine learning model is proposed.",Internet of vehicles (IoV); trust management (TM); vehicular ad hoc network (VANET); machine learning; context awareness; bayesian learning,"Rehman, A; Hassan, MF; Hooi, YK; Qureshi, MA; Chung, TD; Akbar, R; Safdar, S",Universiti Teknologi Petronas; FPT University,2021.0,CMC-COMPUTERS MATERIALS & CONTINUA,TECH SCIENCE PRESS,,10.32604/CMC.2021.017620,20230520-160000,20230521-044735
,wos,The mediating role of knowledge management and information systems selection management capability on Big Data Analytics quality and firm performance,"This study draws on organizational learning and strategic decision-making theory to develop a conceptual framework to explore how the selection measures of BDA systems and external support partners are linked to BDA system quality, and how these influence firms' competitive position. Through a cross-sectional survey of 523 IT professionals from the US, UK, and India, using path analysis and structural equation modeling, we found that the information systems selection process, enhanced by knowledge management capabilities, is positively related to the BDA system quality and firms' performance. However, inconsistent with prior studies on transactional systems, we found no support for the hypothesis that software vendor criteria influence BDA quality. Also, in selecting systems and external facilitators, organizations appear to be pivoting towards parameters that are considered emerging, such as cloud computing, DevOps, and agile experiences, as they increase the likelihood of unlocking business value from BDA.",Big data analytics; cloud computing; DevOps; agile; information system criteria; knowledge management; third-party consultant; software vendor criteria; business values and competitive advantage,"Obitade, OP",University of North Texas System; University of North Texas Denton,,JOURNAL OF DECISION SYSTEMS,TAYLOR & FRANCIS LTD,,10.1080/12460125.2021.1966162,20230520-160000,20230521-044735
,wos,DMISTA: Conceptual Data Model for Interactions in Support Ticket Administration,"Changing business models and dynamic markets in the globally connected world results in more and more complex system environments. The IT service infrastructure as enabler of innovative business models has to support these innovations by providing agile methods to quickly adapt to new use-cases. This underlines the need to manage the digitized environment systematically in order to foster efficiency. IT Service Management (ITSM) as a discipline evolved and now provides the framework to orchestrate the complexity in Information Technology. The activities, processes, and capabilities to maintain the portfolio are served by individuals, who interact with each other. There is an emphasized need for identifying, acquiring, organizing, storing, retrieving, and analyzing data related to human interaction processes to support finally the business processes. This paper proposes a conceptual data model to capture information about human interactions during support ticket administration (DMISTA). The presented model-structure and -requirements allow for efficient selection of appropriate data for various data science use-cases to understand and optimize business processes. The DMISTA supports different types of relationships (based on causality, joint cases, and joint activities) to enable efficient processing of specific analysis methods. The applicability of the model is shown based on a typical use-case.",IT Service Management; Enterprise Information System; Conceptual Data Model; Data Mining; Data Flow Architectures; Requirements Engineering; Support Ticket Administration,"Mertens, C; Nurnberger, A",Otto von Guericke University,2022.0,ICEIS: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS - VOL 1,SCITEPRESS,,10.5220/0010999100003179,20230520-160000,20230521-044735
,wos,IBM Z development transformation,"This article discusses how the product development cycle is being transformed with Artificial Intelligence (AI) for the first time in zSeries history. This new era of AI, under the project name IBM Z Development Transformation (zDT), has allowed the team to grow and learn new skills in data science. This transformation forces change structurally in how data is prepared and stored. In z14, there were incremental productivity gains with enhancements to automation with eServer Automation Test Solution and a technology data analysis engine called zDataAssist. However, in z15, AI will significantly accelerate our efficiency. This article explains how Design Thinking and Agile principles were used to identify areas that are of high impact and feasible to implement: 1) what and how data is collected via System Test Event Logging and Analysis engine, Problem ticket management system (Jupitr), and Processor data analysis engine (Xrings); 2) problem identification, analysis, and management (AutoJup) along with Intelligent Recovery Verification Assistant; 3) product design documentation search engine (AskiheMachine); and 4) prototype microprocessor allocation processes Intelligent Commodity Fulfillment System using Machine Learning. This article details the approach of these areas for z15, the implementation of these solutions under the zDT project, as well as the results and future work.",,"McCain, EC; Bastien, P; Belmar, BF; Bhattacharya, B; Cheruiyot, KK; Coq, M; Dartey, R; Deekaram, K; Ghadai, K; Lalima, LD; Nettey, J; Owolabi, AW; Phillips, K; Shiling, TM; Schroeder, DT; Slegel, C; Steen, B; Thorne, DA; Venuto, E; Willoughby, JD; Yaniv, D; Ziemis, N",International Business Machines (IBM); International Business Machines (IBM),2020.0,IBM JOURNAL OF RESEARCH AND DEVELOPMENT,IBM CORP,,10.1147/JRD.2020.3008122,20230520-160000,20230521-044735
,wos,Testing a Generalizable Machine Learning Workflow for Aquatic Invasive Species on Rainbow Trout (Oncorhynchus mykiss) in Northwest Montana,"Biological invasions are accelerating worldwide, causing major ecological and economic impacts in aquatic ecosystems. The urgent decision-making needs of invasive species managers can be better met by the integration of biodiversity big data with large-domain models and data-driven products. Remotely sensed data products can be combined with existing invasive species occurrence data via machine learning models to provide the proactive spatial risk analysis necessary for implementing coordinated and agile management paradigms across large scales. We present a workflow that generates rapid spatial risk assessments on aquatic invasive species using occurrence data, spatially explicit environmental data, and an ensemble approach to species distribution modeling using five machine learning algorithms. For proof of concept and validation, we tested this workflow using extensive spatial and temporal hybridization and occurrence data from a well-studied, ongoing, and climate-driven species invasion in the upper Flathead River system in northwestern Montana, USA. Rainbow Trout (RBT; Oncorhynchus mykiss), an introduced species in the Flathead River basin, compete and readily hybridize with native Westslope Cutthroat Trout (WCT; O. clarkii lewisii), and the spread of RBT individuals and their alleles has been tracked for decades. We used remotely sensed and other geospatial data as key environmental predictors for projecting resultant habitat suitability to geographic space. The ensemble modeling technique yielded high accuracy predictions relative to 30-fold cross-validated datasets (87% 30-fold cross-validated accuracy score). Both top predictors and model performance relative to these predictors matched current understanding of the drivers of RBT invasion and habitat suitability, indicating that temperature is a major factor influencing the spread of invasive RBT and hybridization with native WCT. The congruence between more time-consuming modeling approaches and our rapid machine-learning approach suggest that this workflow could be applied more broadly to provide data-driven management information for early detection of potential invaders.",invasive species; machine learning; species distribution modeling; remote sensing; big data analytics; early detection and rapid response,"Carter, S; van Rees, CB; Hand, BK; Muhlfeld, CC; Luikart, G; Kimball, JS",University of Montana System; University of Montana; University of Montana System; University of Montana; United States Department of the Interior; United States Geological Survey; University of Montana System; University of Montana; University System of Georgia; University of Georgia; University System of Georgia; University of Georgia,2021.0,FRONTIERS IN BIG DATA,FRONTIERS MEDIA SA,,10.3389/fdata.2021.734990,20230520-160000,20230521-044735
,wos,Diaspore: Diagnosing Performance Interference in Apache Spark,"Apache Spark is being increasingly used to execute big data applications on cluster computing platforms. To increase system utilization, cluster operators often configure their clusters such that multiple co-located applications can simultaneously share the resources of a cluster node. With resource sharing, applications can compete with each other for shared node resources thereby interfering with each other's performance. Many Spark applications take a long time to execute. Performance interference from other applications can thus cause a Spark application to fail or take even longer time to execute thereby wasting cluster resources and frustrating users. This motivates the need for an automated technique that can detect interference quickly and also diagnose the root cause of the interference to facilitate mitigation of the problem. Most existing approaches are not designed to offer quick interference detection and diagnosis. For example, they typically require extensive training data for every application of interest under various possible input data sizes and resource allocations. In this paper, we systematically investigate the design of a Machine Learning (ML) based technique that addresses this open problem. We implement a tool called Diaspore that integrates our findings. We evaluate the tool with a diverse set of 13 Spark applications executing on a real cluster. Experimental results show that Diaspore requires only small scale training data, i.e., executions under small input sizes and resource allocations. Furthermore, our results show that the tool can offer accurate predictions for applications not present in the training data. Consequently, Diaspore reduces the training time needed to offer predictions. Finally, the feature engineering underlying Diaspore ensures that the tool can detect and diagnose interference quickly in an online manner by sampling only a small fraction of a long running application's execution. This can allow cluster operators to mitigate interference in an agile manner.",Interference; Sparks; Task analysis; Measurement; Training; Resource management; Training data; Interference detection; big data; machine learning,"Shah, SR; Amannejad, Y; Krishnamurthy, D",University of Calgary; Mount Royal University,2021.0,IEEE ACCESS,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,,10.1109/ACCESS.2021.3098426,20230520-160000,20230521-044735
,wos,Contexts Enhance Accuracy: On Modeling Context Aware Deep Factorization Machine for Web API QoS Prediction,"Service-oriented computing (SOC) promises a world of cooperating services loosely connected, constructing agile Web applications in heterogeneous environments conveniently. Web application interface (API) as an emerging technique attracts more and more enterprises and organizations to publish their deep computing functionalities and big data on the Internet, Web API has become the backbone to promote the development of SOC, thus forming the prosperous Web API economy. However, the number of available Web APIs on the Internet is massive and growing constantly, which causes the Web API overload problem. Quality of service (QoS) as an indicator is able to well differentiate the quality of Web APIs and has been widely applied for high quality Web API selection. Since testing QoS for massive Web APIs is resource-consuming, and the QoS performance depends on contextual information such as network and location, hence accurate QoS prediction has become very crucial for personalized Web API recommendation and high quality Web application construction. To address the above issue, this paper presents a context aware deep factorization machine model (CADFM for short) for accurate Web API QoS prediction. Specifically, we first carry out detailed data analysis using real-world QoS dataset and discover a positive relationship between QoS and contextual information, which motivates us to incorporate beneficial contexts for enhancing QoS prediction accuracy. Then, we treat QoS prediction as a regression problem and propose a context aware CADFM framework that integrates the contextual information via embedding technique. Particularly, we adopt MF and MLP for high-order and nonlinear interaction modeling, so as to learn the complex interaction between users and Web APIs accurately. Finally, the experimental results on real-world QoS dataset demonstrate that CADFM outperforms the classic and the state-of-the-art baselines, thereby generating the most accurate QoS predictions and increasing the revenue of Web APIs recommendation.",Quality of service; Context modeling; Predictive models; Context-aware services; Internet; Organizations; Software; Service-oriented computing; Web API; quality of service prediction; context aware; deep factorization machine,"Shen, LM; Pan, MS; Liu, LL; You, DL; Li, F; Chen, Z","Yanshan University; Chinese Academy of Sciences; National Science Library, CAS; Northeastern University - China",2020.0,IEEE ACCESS,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,,10.1109/ACCESS.2020.3022891,20230520-160000,20230521-044735
,wos,Agile Deep Learning UAVs Operating in Smart Spaces: Collective Intelligence Versus Mission-Impossible,"The environments, in which we all live, are known to be complex and unpredictable. The complete discovery of these environments aiming to take full control over them is a mission-impossible, however, still in our common agenda. People intend to make their living spaces smarter utilizing innovations from the Internet of Things and Artificial Intelligence. Unmanned aerial vehicles (UAVs) as very dynamic, autonomous and intelligent things capable to discover and control large areas are becoming important inhabitants within existing and future smart cities. Our concern in this paper is to challenge the potential of UAVs in situations, which are evolving fast in a way unseen before, e.g., emergency situations. To address such challenges, UAVs have to be intelligent enough to be capable to autonomously and in near real-time evaluate the situation and its dynamics. Then, they have to discover their own missions and set-up suitable own configurations to perform it. This configuration is the result of flexible plans which are created in mutual collaboration. Finally, the UAVs execute the plans and learn from the new experiences for future reuse. However, if to take into account also the Big Data challenge, which is naturally associated with the smart cities, UAVs must be also wise in a sense that the process of making autonomous and responsible real-time decisions must include continuous search for a compromise between efficiency (acceptable time frame to get the decision and reasonable resources spent for that) and effectiveness (processing as much of important input information as possible and to improve the quality of the decisions). To address such a skill we propose to perform the required computations using Cloud Computing enhanced with Semantic Web technologies and potential tools (agile deep learning) for compromising, such as, e.g., focusing, filtering, forgetting, contextualizing, compressing and connecting.",,"Cochez, M; Periaux, J; Terziyan, V; Tuovinen, T",University of Jyvaskyla; Fraunhofer Gesellschaft; RWTH Aachen University; Universitat Politecnica de Catalunya; Centre Internacional de Metodes Numerics en Enginyeria (CIMNE),2018.0,COMPUTATIONAL METHODS AND MODELS FOR TRANSPORT: NEW CHALLENGES FOR THE GREENING OF TRANSPORT SYSTEMS,SPRINGER-VERLAG BERLIN,,10.1007/978-3-319-54490-8_3,20230520-160000,20230521-044735
,wos,Artificial Intelligence Potential in Higher Education Institutions Enhanced Learning Environment in Romania and Serbia,"In their struggle to offer a sustainable educational system and transversal competencies for market requests, significant transformations characterise the higher education system in Serbia and Romania. According to EU policy, these transformations are related to educational reforms and the introduction of new technology and methodologies in teaching and learning. They are expected to answer to the PISA requirements and to increase the DESI (Digital Economy and Society Index). They are also likely to mitigate the inequity of HEIs (higher education institutions), empowered by a structured, goal-oriented strategy towards agile management in HEIs that is also appropriate for new market demands. Our study is based on an exploratory survey applied to 139 Romanian and Serbian teachers from the Information Technology School-ITS, Belgrade, and Spiru Haret University, Romania. The survey let them provide their knowledge of AI or their perceptions of the difficulties and opportunities of these technologies in HEIs. Our study discovered how difficulties and opportunities associated with AI impact HEIs. This study aims to see how AI might assist higher education in Romania and Serbia. We also considered how they might be integrated with the educational system, and if instructors would utilise them. Developing creative and transversal skills is required to anticipate future breakthroughs and technological possibilitiesThe new methods of education focuses on ethics, values, problem-solving, and daily activities. Students' learning material, how they might achieve critical abilities, and their educational changes must be addressed in the future. In this environment, colleges must create new digital skills in IA, machine learning, IoT, 5G, the cloud, big data, blockchain, data analysis, using MS Office and other applications, MOOCs, simulation applications, VR/AR, and gamification. They must also develop cross-disciplinary skills and a long-term mindset.",higher education institutions (HEI); artificial intelligence (AI); transversal skills,"Bucea-Manea-Tonis, R; Kuleto, V; Gudei, SCD; Lianu, C; Lianu, C; Ilic, MP; Paun, D",National University of Physical Education & Sport; Bucharest University of Economic Studies; Spiru Haret University; Spiru Haret University,2022.0,SUSTAINABILITY,MDPI,,10.3390/su14105842,20230520-160000,20230521-044735
,wos,Impact of the worldwide trends on the development of the digital economy,"The aim of the article is to develop a new model of the digital economy as a new scientific direction of the philosophy of economics. Analysis methodology is the use of methods such as cross-cultural, systemic, synergetic, informational, axiological, cybernetic to develop a new model of the digital economy as a new scientific direction, in the context of which a new information space is being formed. The problems of solving the digital economy are taking place against the backdrop of new trends - globalization 4.0, Industry 4.0, Enlightenment 2.0, Agile management, in the context of which there is a transition from simple interconnection to hyperconnection and the spread of Moore's law, according to which there is a doubling of information. The results of the study. 1) The development of the digital economy as a new scientific field, which is based on a combination of concepts of informatization, digitalization, robotics, developing under the influence of global trends is studied. 2) It has been established that the digital economy contributes to technological progress and, under the pressure of global trends, develops a variety of economic models of scientific, technical and digital progress, which are based on the solution of problems of man, science, society. 3) The problematic issues of the digital economy and the conditions for its solution are identified. Conclusions. The prospects that the digital economy opens up thanks to modern technologies representing a technological breakthrough are analyzed. The digital technology network is designed so that it moves with the least loss and the smallest pieces of calculus are at the heart of this new constant flow system. All this indicates that in the context of globalization and the BIG DATA era, humanity is entering a new stage of calculus, when information doubles in accordance with Moore's law.",global trends; digital economy; information-digital technologies; philosophy of economics,"Voronkova, HV; Nikitenko, AV; Teslenko, VT; Bilohur, EV",Ministry of Education & Science of Ukraine; Zaporizhzhia National University; Ministry of Education & Science of Ukraine; Ukrainian State University of Science & Technologies; Bogdan Khmelnitsky Melitopol State Pedagogical University,2020.0,AMAZONIA INVESTIGA,UNIV AMAZONIA,,10.34069/AI/2020.32.08.9,20230520-160000,20230521-044735
,wos,The ecology of open innovation units: adhocracy and competing values in public service systems,"There have been concerted efforts to encourage innovation and to foster a more innovative and open culture to government and public service institutions. Policy and service innovation labs constitute one part of a broader open innovation movement which also includes open data, behavioral insights, digital services, data science units, visualization capabilities, and agile and lean methods. This article argues that we need to step back and better understand these ecologies of innovation capabilities that have emerged across public service institutions, and to recognize that as fellow innovation traveling companions they collectively seek to transform the culture of government and public service institutions, producing more effective, efficient and tailored policies and services. This article introduces analytic frameworks that should help locate policy and innovation labs amidst these other innovating entities. First, it delineates the various units and initiatives which can be seen as committed to new ways of working and innovating in public service institutions, often relying on open innovation rhetoric and approaches. Second, it shows how - despite the diversity among these entities - they nevertheless share similar attributes as adhocracies and are located as part of a broader movement and class of organizations. Third, we locate these diverse OI entities amidst broader public service systems using the Competing Values Framework. Fourth, this article situates the challenges confronting OI units developing and sustaining or broadening niches in public service systems. Finally, it identifies future research questions to take up.",Open innovation; public service; adhocracies; competing values framework; policy labs; innovation labs; service labs,"Lindquist, EA; Buttazzoni, M",University of Victoria,2021.0,POLICY DESIGN AND PRACTICE,TAYLOR & FRANCIS LTD,,10.1080/25741292.2021.1941569,20230520-160000,20230521-044735
,wos,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be entangled in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",AI; Software engineering; process; data,"Amershi, S; Begel, A; Bird, C; DeLine, R; Gall, H; Kamar, E; Nagappan, N; Nushi, B; Zimmermann, T",Microsoft; University of Zurich,2019.0,2019 IEEE/ACM 41ST INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING: SOFTWARE ENGINEERING IN PRACTICE (ICSE-SEIP 2019),IEEE,,10.1109/ICSE-SEIP.2019.00042,20230520-160000,20230521-044735
,wos,A Mathematics Pipeline to Student Success in Data Analytics through Course-Based Undergraduate Research,"This paper reports on Data Analytics Research (DAR), a course-based undergraduate research experience (CURE) in which undergraduate students conduct data analysis research on open real-world problems for industry, university, and community clients. We describe how DAR, offered by the Mathematical Sciences Department at Rensselaer Polytechnic Institute (RPI), is an essential part of an early low-barrier pipeline into data analytics studies and careers for diverse students. Students first take a foundational course, typically Introduction to Data Mathematics, that teaches linear algebra, data analytics, and R programming simultaneously using a project-based learning (PBL) approach. Then in DAR, students work in teams on open applied data analytics research problems provided by the clients. We describe the DAR organization which is inspired in part by agile software development practices. Students meet for coaching sessions with instructors multiple times a week and present to clients frequently. In a fully remote format during the pandemic, the students continued to be highly successful and engaged in COVID-19 research producing significant results as indicated by deployed online applications, refereed papers, and conference presentations. Formal evaluation shows that the pipeline of the single on-ramp course followed by DAR addressing real-world problems with societal benefits is highly effective at developing students' data analytics skills, advancing creative problem solvers who can work both independently and in teams, and attracting students to further studies and careers in data science.",undergraduate education; data analytics; course-based undergraduate research; linear algebra; project-based learning; data visualization; machine learning,"Bennett, KP; Erickson, JS; Svirsky, A; Seddon, JC",Rensselaer Polytechnic Institute; University of Rochester,2022.0,MATHEMATICS ENTHUSIAST,"UNIV MONTANA, DEPT MATHEMATICAL SCIENCES",,,20230520-160000,20230521-044735
,wos,"Transitioning from Legacy Air Traffic Management to Airspace Management through Secure, Cloud-Native Automation Solutions","Advancements in Cloud-native services, Machine-Learning (ML), Artificial Intelligence (AI), and Rapid Application Development (RAD) using the Agile methodology has led countless industries to achieving desirable levels of automation while reducing cost and improving quality software deployments, timely / iterative delivery, and accountability. Coupling this framework with the principle of security as a shared responsibility further enhances the efficacy of an integrated Development, Security, and Operations (DevSecOps) Team within organizations to deliver secured digital solutions. Air Navigation Service Providers (ANSPs) around the world are currently exploring and embracing the digital evolution shifting from monolithic, legacy automation platforms to an application framework of microservices to allow for flexible operations as capabilities and airspace operations evolve. Specific to the US, the ATM automation system of today is comprised of both safety and non-safety critical systems, with mission-essential, efficiency-critical, and mission-support services that are predominately maintained and evolved through multiyear, one contractor-led programs. Although the system has proven resilient, it has not proven to be agile and flexible to allow for advances in capabilities on-board aircraft or in the data integration and sharing with other NAS automation systems. This creates significant overhead in development, sustainability, and operations of the current automation system, and leaves modernization efforts-in terms of new capabilities-in constant investment decision planning cycles, costing agencies not just money, but more time to innovate. To advance aviation into a new generation of interoperability leveraging collaborative frameworks and application specific capabilities, ANSPs must adapt to innovative methods to collect, process, and deliver critical and essential aeronautical, weather, and flight information to air traffic control operators and ultimately to airspace users. Doing so can not only lead to sustaining NAS automation systems while reducing the costs to develop and operate these systems, but it also provides an opportunity to present strategies on how to dramatically reduce the time and integration efforts needed to deploy new capabilities. Leveraging cloud-native technologies and services is a way to realize this automation evolution vision for ANSPs. This paper examines the migration from today's systems to secure, cloud-native platforms to prove that Mission Services and Mission Applications can be rapidly available / deployable to operators who provide separation and flow management services, using a cyber-secured cloud-native environment. Aeronautical data typically used for tactical decision making is now seen as crucial to the decision-making process in Air Traffic Management (ATM). Integrating global and localized datasets into a digital aviation data platform enhances the capabilities of the solutions and opens the possibilities of leveraging big data analytics and microservices to compute trajectory predictions (TP), demand capacity balancing (DCB), arrival and departure sequencing, airspace delay, among others, in real-time to achieve operator-driven mission objectives. Technology has reached a state of maturity, especially in cloud and hybrid cloud solutions, to support safety of life operations, like ATM. This paper identifies approaches that are being considered for that migration to support the integration of new airspace entrants, the use of application services to provide a dynamic, evolutionary ATM platform, and addresses some of the safety and security strategies that must be considered for this evolution.",Automation; ATM; Cloud-native; Microservices,"Solomon, A; Crawford, Z",Thales Group,2021.0,2021 IEEE/AIAA 40TH DIGITAL AVIONICS SYSTEMS CONFERENCE (DASC),IEEE,,10.1109/DASC52595.2021.9594313,20230520-160000,20230521-044735
,wos,Convergence and digital fusion lead to competitive differentiation,"Purpose Organizations are consistently seeking innovative strategies and novel pathways to enhance business processes and create differentiation. The global business ecosystem is changing and there is growing demand for multi-modal digital technologies, big data consolidation and data analytics to harness a cost-competitive agile system. Technological convergence and integration of digital systems is one of the preferred methodologies that facilitates new and effective workflows and revives business processes. The progressive interlinking of digital technologies with business operations leads to the convergence and blending of management disciplines, devices and applications. The growing inconsistencies in managerial understanding regarding the benefits of convergence prompts a comprehensive examination of digital convergence pathways, identifying the impacts on converging entities and business objectives. The State bank of India (SBI) mega-merger case study was selected to investigate the pragmatic framework of digital convergence and to understand the impacts on interlinked entities such as: business operations, strategic management, project team that support value creation and competitive differentiation. The purpose of this paper is to focus on the phenomena of techno-fusion of emerging technologies creating new opportunities, business models and unique strategies for global banking and financial service organizations. Design/methodology/approach This study applies the qualitative, inductive research method using critical reflection of before and after the implementation of convergence and digital integration strategies. The SBI case study employs this research strategy based on the premise that banks must stay agile and highly responsive to the changing environment to enhance its value proposition and competitive differentiation objectives. The study methodology incorporates cooperative inquiry and multiple levels of analysis using data collection techniques of exhaustive review of archives, informal interviews, questionnaires and observations to identify the synergistic process improvement pathway. The study is grounded on the concept that the convergence of diverse business pathways involves innovative and interlinked project, strategic and information technology (IT) workflows that results in open innovative systems. Findings The studies identify that organizational innovation and creative solutions are a result of ecosystem turbulence, environmental force diversity, competitive pressure and the need for differentiation. Organizations that harness the power of digital fusion and convergence of management, systems and data generate a competitive advantage. The technological convergence strategy pulls multiple business and technology processes (project, strategic, IT, Cloud, AI and business process management) at the organizational, divisional or functional level generating new opportunities and threats, new business models and unique growth strategies for global banking and financial services organizations. Organizations that fully integrate techno-fusion of business and digital strategies produce synergistic effects and enhance adaptability, innovation and resiliency in the face of competitive challenges. Research limitations/implications Additional areas that can be explored further as an extension of this study are listed below: identifying factors to improve the speed of convergence; the current results are limited to large size organizations where formal management and technology functions are distinctive. Similar studies on smaller organizations are warranted. Originality/value This study focuses on the evolving field of technology innovation, which is increasingly being intertwined with business operations. Innovative digital technology is enabling the convergence of the disciplines of management, digital devices and applications. This facilitates the creation of a pragmatic framework that supports convergence of business operations, strategic management and digital fusion which leads to value creation and competitive differentiation. The techno-fusion of emerging technologies and digital strategies generates new opportunities and threats, new business models and unique growth strategies for organizations.",Convergence; Open innovation; Digital technology; Agile synergistic interaction; Business and technology management; Digital fusion,"Thomas, A",,2020.0,BUSINESS PROCESS MANAGEMENT JOURNAL,EMERALD GROUP PUBLISHING LTD,,10.1108/BPMJ-01-2019-0001,20230520-160000,20230521-044735
,wos,Digital Transformation in Smart Farm and Forest Operations Needs Human-Centered AI: Challenges and Future Directions,"The main impetus for the global efforts toward the current digital transformation in almost all areas of our daily lives is due to the great successes of artificial intelligence (AI), and in particular, the workhorse of AI, statistical machine learning (ML). The intelligent analysis, modeling, and management of agricultural and forest ecosystems, and of the use and protection of soils, already play important roles in securing our planet for future generations and will become irreplaceable in the future. Technical solutions must encompass the entire agricultural and forestry value chain. The process of digital transformation is supported by cyber-physical systems enabled by advances in ML, the availability of big data and increasing computing power. For certain tasks, algorithms today achieve performances that exceed human levels. The challenge is to use multimodal information fusion, i.e., to integrate data from different sources (sensor data, images, *omics), and explain to an expert why a certain result was achieved. However, ML models often react to even small changes, and disturbances can have dramatic effects on their results. Therefore, the use of AI in areas that matter to human life (agriculture, forestry, climate, health, etc.) has led to an increased need for trustworthy AI with two main components: explainability and robustness. One step toward making AI more robust is to leverage expert knowledge. For example, a farmer/forester in the loop can often bring in experience and conceptual understanding to the AI pipeline-no AI can do this. Consequently, human-centered AI (HCAI) is a combination of artificial intelligence and natural intelligence to empower, amplify, and augment human performance, rather than replace people. To achieve practical success of HCAI in agriculture and forestry, this article identifies three important frontier research areas: (1) intelligent information fusion; (2) robotics and embodied intelligence; and (3) augmentation, explanation, and verification for trusted decision support. This goal will also require an agile, human-centered design approach for three generations (G). G1: Enabling easily realizable applications through immediate deployment of existing technology. G2: Medium-term modification of existing technology. G3: Advanced adaptation and evolution beyond state-of-the-art.",sensors; cyber-physical systems; machine learning; artificial intelligence; human-centered AI; smart farming; smart forestry; precision farming; precision forestry; AI for good,"Holzinger, A; Saranti, A; Angerschmid, A; Retzlaff, CO; Gronauer, A; Pejakovic, V; Medel-Jimenez, F; Krexner, T; Gollob, C; Stampfer, K","University of Natural Resources & Life Sciences, Vienna; University of Alberta; Technical University of Berlin; University of Natural Resources & Life Sciences, Vienna; University of Natural Resources & Life Sciences, Vienna; University of Natural Resources & Life Sciences, Vienna",2022.0,SENSORS,MDPI,,10.3390/s22083043,20230520-160000,20230521-044735
,wos,GELAB - The Cutting Edge of Grammatical Evolution,"The advent of cloud-based super-computing platforms has given rise to a Data Science (DS) boom. Many types of technological problems that were once considered prohibitively expensive to tackle are now candidates for exploration. Machine Learning (ML) tools that were valued only in academic environments are quickly being embraced by industrial giants and tiny startups alike. Coupled with modern-day computing power, ML tools can be looked at as hammers that can deal with even the most stubborn nails. ML tools have become so ubiquitous that the current industrial expectation is that they should not only deliver accurate and intelligent solutions but also do so rapidly. In order to keep pace with these requirements, a new enterprise, referred to as MLOps has blossomed in recent years. MLOps combines the process of ML and DS with an agile software engineering technique to develop and deliver solutions in a fast and iterative way. One of the key challenges to this is that ML and DS tools should be efficient and have better usability characteristics than were traditionally offered. In this paper, we present a novel software for Grammatical Evolution (GE) that meets both of these expectations. Our tool, GELAB, is a toolbox for GE in Matlab which has numerous features that distinguish it from existing contemporary GE software. Firstly, it is user-friendly and its development was aimed for use by non-specialists. Secondly, it is capable of hybrid optimization, in which standard numerical optimization techniques can be added to GE. We have shown experimentally that when hybridized with meta-heuristics GELAB has an overall better performance as compared with standard GE.",Germanium; Matlab; Software; Optimization; Grammar; Statistics; Sociology; Grammatical evolution; diversity; hybrid optimization,"Gupt, KK; Youssef, A; Murphy, A; Raja, MA; Ryan, C",University of Limerick; University of Limerick; University College Dublin; Egyptian Knowledge Bank (EKB); Electronics Research Institute (ERI); University of Limerick,2022.0,IEEE ACCESS,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,,10.1109/ACCESS.2022.3166115,20230520-160000,20230521-044735
,wos,Role of chemical reaction engineering for sustainable growth: One industrial perspective from India,"Chemical reaction engineering (CRE) is vital to solve many of the pressing societal challenges-energy and energy transition, materials, food, mobility, and so forth, to meet the aspirational goals of developing country population in the face of climate change, changing demographics, and geopolitical challenges. Application of the core principles of CRE to the emerging societal challenges is creating new technologies and cost-effective solutions by integrating the widely varied CRE activities into broad, powerful, systems descriptions with the help of interdisciplinary teams with broad expertise including chemistry, catalysis, chemical kinetics, transport phenomena, biology, applied mathematics and modeling, emerging data science technologies to design and optimize chemical/biochemical reactors. Such developments will be critical for CRE to play an important role in the emerging fourth industrial revolution-amalgamation of physical, digital, and biological worlds, where the velocity of disruption and acceleration of innovation are hard to comprehend or anticipate and such broadening of CRE discipline will be critical for the field to remain agile and relevant. This article describes some latest technical advances in Reliance Industries Ltd. using this philosophy to help achieve sustainable growth and Net Zero business targets. We will broadly discuss renewable hydrogen from novel biomass catalytic gasification, multizone catalytic cracking process to convert crude oil and low value hydrocarbon streams to petrochemical building blocks, an adsorption/desorption process for CO2 concentration and monetization from industrial flue gases. Furthermore, biotechnology advances in leveraging photosynthesis kinetics, synthetic biology, and genetic modifications for converting solar energy and carbon dioxide through algae production will be discussed to produce proteins, biomaterials, renewable biocrude, and so forth. We will also discuss new catalytic technologies to convert mixed plastic waste to stable oil and organic waste such as agri and municipal solid waste, and so forth, to biocrude for circular economy, and biodegradable plastics production to manage plastics pollution.",,"Sapre, A",Reliance Industries,2023.0,AICHE JOURNAL,WILEY,,10.1002/aic.17685,20230520-160000,20230521-044735
,wos,Exploiting Page Table Locality for Agile TLB Prefetching,"Frequent Translation Lookaside Buffer (TLB) misses incur high performance and energy costs due to page walks required for fetching the corresponding address translations. Prefetching page table entries (PTEs) ahead of demand TLB accesses can mitigate the address translation performance bottleneck, but each prefetch requires traversing the page table, triggering additional accesses to the memory hierarchy. Therefore, TLB prefetching is a costly technique that may undermine performance when the prefetches are not accurate. In this paper we exploit the locality in the last level of the page table to reduce the cost and enhance the effectiveness of TLB prefetching by fetching cache-line adjacent PTEs for free. We propose Sampling-Based Free TLB Prefetching (SBFP), a dynamic scheme that predicts the usefulness of these free PTEs and prefetches only the ones most likely to prevent TLB misses We demonstrate that combining SBFP with novel and state-of-the-art TLB prefetchers significantly improves miss coverage and reduces most memory accesses due to page walks. Moreover, we propose Agile TLB Prefetcher (ATP), a novel composite TLB prefetcher particularly designed to maximize the benefits of SBFP. ATP efficiently combines three low-cost TLB prefetchers and disables TLB prefetching for those execution phases that do not benefit from it. Unlike state-of-the-art TLB prefetchers that correlate patterns with only one feature (e.g., strides, PC, distances), ATP correlates patterns with multiple features and dynamically enables the most appropriate TLB prefetcher per TLB miss. To alleviate the address translation performance bottleneck, we propose a unified solution that combines ATP and SBFP. Across an extensive set of industrial workloads provided by Qualcomm, ATP coupled with SBFP improves geometric speedup by 16.2%, and eliminates on average 37% of the memory references due to page walks. Considering the SPEC CPU 2006 and SPEC CPU 2017 benchmark suites, ATP with SBFP increases geometric speedup by 11.1%, and eliminates page walk memory references by 26%. Applied to big data workloads (GAP suite, XSBench), ATP with SBFP yields a geometric speedup of 11.8% while reducing page walk memory references by 5%. Over the best state-of-the-art TLB prefetcher for each benchmark suite, ATP with SBFP achieves speedups of 8.7%, 3.4%, and 4.2% for the Qualcomm, SPEC, and GAP+XSBench workloads, respectively.",,"Vavouliotis, G; Alvarez, L; Karakostas, V; Nikas, K; Koziris, N; Jimenez, DA; Casas, M",Universitat Politecnica de Catalunya; Barcelona Supercomputer Center (BSC-CNS); Universitat Politecnica de Catalunya; National Technical University of Athens; Texas A&M University System; Texas A&M University College Station,2021.0,2021 ACM/IEEE 48TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2021),IEEE COMPUTER SOC,,10.1109/ISCA52012.2021.00016,20230520-160000,20230521-044735
,wos,CONSOLIDATING THE DIGITAL EDUCATIONAL TRANSFORMATION IN ANDALUSIA,"This paper presents the project for the implementation of the Virtual Learning Environment for non university education in Andalusia, a Spanish region with more than 8 million inhabitants, which educational community consists of over 2 million people (teachers, students and families), in a very heterogeneous scenario, encompassing geography, socio-economy and technology. The project is led by the General Directorate for Teacher Training and Educational Innovation (DGFPIE) of the Ministry of Education and Sports (CED), and its main objective is the consolidation of the Digital Educational Transformation in Andalusia (TDE), focusing on the optimisation of services to grant the best use of the resources. The project' strategy is fully aligned with the EU's Digital Education Action Plan, upon a global, collaborative and inclusive approach, making emphasis in granting universal access to resources and services to all the groups of the Educational Community. Once high connectivity is provided to schools, the project is defined around three action lines that consolidate the TDE ecosystem of the DGFPIE: Improving the use of digital technology in teaching-learning processes Developing digital competencies and skills relevant to the educational community Promoting the improvement of school results and reduction of dropout through Big Data (BD), Learning Analytics (LA) and Artificial Intelligence (AI). The project methodologies cover technological, implementation and dissemination aspects, and consist of adaptations of gap analysis, design thinking, Lean IT, BPM and TOGAF for the definition phase, scrum for the development and ITIL, PMP and OPPM services provision and project management. Regarding implementation and dissemination, any new service provided is tested in a piloting phase prior to its go alive, with a defined piloting methodology. The result, the Andalusian Digital Education Ecosystem is a personalized teaching-learning environment that provides access to a digital classroom, integrated with the backoffice management system, which allows class organization and planning, connected with the digital content of publishers and with an open educational resources banks (LOR). This pedagogical support is complemented with services to promote the digital training of students, families and teachers through a MOOC, MoocEdu, fully aligned with the DigCompX frameworks. A Digital Proficiency Test is included to diagnose the user skills and stablish a starting point, recommending training actions to achieve the defined objectives. Through this single access point, schools benefit from facilities to collaborate which include a vast community of school blogs and classroom spaces, BlogsAverroes, ever growing to support communication needs for different decision-making bodies within the CED. AI allows the provision of a Pedagogical Assessment Service, a personalized and contextualized educational reinforcement of the students through recommendations of itineraries, contents, tasks and/or readings adapted to their academic situation, which take into account information related to the characteristics of their teachers, their socioeconomic context and their academic performance, among others. The project results are measured by means of access and use indicators, which together with the user's satisfaction, checked by the change management service, establish the conclusions and the basis for the continuous improvement of the TDE ecosystem.",New projects and innovations; APP for education; Identity Management; Learning Analytics; Open Educational Resources; LOR; Digital Education; Collaboration,"Segura, A; Agromayor, JA; Conde, A",,2020.0,"14TH INTERNATIONAL TECHNOLOGY, EDUCATION AND DEVELOPMENT CONFERENCE (INTED2020)",IATED-INT ASSOC TECHNOLOGY EDUCATION & DEVELOPMENT,,,20230520-160000,20230521-044735
,wos,Successful operational integration of healthcare analytics at Seattle Children's,"Introduction: As the quantity and complexity of health data grows, it is critical for healthcare organizations to devise analytic strategies that power data innovation so they can take advantage of new opportunities and improve outcomes. Seattle Children's Healthcare System (Seattle Children's) is an example of an organization that has built an operating model that integrates analytics into their business and daily operations. We present a roadmap for how Seattle Children's consolidated its fragmented analytics operations into a unified cohesive ecosystem capable of supporting advanced analytics capabilities and operational integration to transform care and accelerate research. Methods: In-depth interviews were conducted with ten leaders at Seattle Children's who have been instrumental in developing their enterprise analytics program. Interviews included the following leadership roles: Chief Data & Analytics Officer, Director of Research Informatics, Principal Systems Architect, Manager of Bioinformatics and High Throughput Analytics, Director of Neurocritical Care, Strategic Program Manager & Neuron Product Development Lead, Director of Dev Ops,Director of Clinical Analytics, Data Science Manager, and Advance Analytics Product Engineer. The interviews were unstructured and consisted of conversations intended to gather information from leadership about their experiences in building out Enterprise Analytics at Seattle Children's. Results: Seattle Children's has built an advanced enterprise analytics ecosystem that is integrated into its daily operations by applying an entrepreneurial mindset and agile development practices that are common in a startup environment. Analytics efforts were approached iteratively by selecting high-value projects that were delivered through Multidisciplinary Delivery Teams that were integrated into service lines. Service line leadership, in partnership with the Delivery Team leads, were responsible for the success of the team by setting project priorities, determining project budgets, and maintaining overall governance of their analytics endeavors. This organizational structure has led to the development of a wide range of analytic products that have been used to improve both operations and clinical care at Seattle Children's. Conclusions: Seattle Children's has demonstrated how a leading healthcare system can successfully create a robust, scalable, near real-time analytics ecosystem- one that delivers significant value to the organization from the ever-expanding volume of health data we see today.",analytics operating model; data ecosystems; healthcare analytics; organizational innovation; scientific computing,"Frisbee, KL; Sousa, R",Seattle Children's Hospital,2023.0,LEARNING HEALTH SYSTEMS,WILEY,,10.1002/lrh2.10331,20230520-160000,20230521-044735
,wos,Characterization and Identification of Variations in Types of Primary Care Visits Before and During the COVID-19 Pandemic in Catalonia: Big Data Analysis Study,"Background: The COVID-19 pandemic has turned the care model of health systems around the world upside down, causing the abrupt cancellation of face-to-face visits and redirection of the model toward telemedicine. Digital transformation boosts information systems-the more robust they are, the easier it is to monitor the health care system in a highly complex state and allow for more agile and reliable analysis. Objective: The purpose of this study was to analyze diagnoses from primary care visits and distinguish between those that had higher and lower variations, relative to the 2019 and 2020 periods (roughly before and during COVID-19), to identify clinical profiles that may have been most impaired from the least-used diagnostic codes for visits during the pandemic. Methods: We used a database from the Primary Care Services Information Technologies Information System of Catalonia. We analyzed the register of visits (n=2,824,185) and their International Classification of Diseases (ICD-10) diagnostic codes (n=3,921,974; mean 1.38 per visit), as approximations of the reasons for consultations, at 3 different grouping levels. The data were represented by a term frequency matrix and analyzed recursively in different partitions aggregated according to date. Results: The increase in non-face-to-face visits (+267%) did not counterbalance the decrease in face-to-face visits (-47%), with an overall reduction in the total number of visits of 1.36%, despite the notable increase in nursing visits (10.54%). The largest increases in 2020 were visits with diagnoses related to COVID-19 (ICD-10 codes Z20-Z29: 2.540%), along with codes related to economic and housing problems (ICD-10 codes Z55-Z65: 44.40%). Visits with most of the other diagnostic codes decreased in 2020 relative to those in 2019. The largest reductions were chronic pathologies such as arterial hypertension (ICD-10 codes I10-I16: -32.73%) or diabetes (ICD-10 codes E08-E13: -21.13%), but also obesity (E65-E68: -48.58%) and bodily injuries (ICD-10 code T14: -33.70%). Visits with mental health-related diagnostic codes decreased, but the decrease was less than the average decrease. There was a decrease in consultations-for children, adolescents, and adults-for respiratory infections (ICD-10 codes J00-J06: -40.96%). The results show large year-on-year variations (in absolute terms, an average of 12%), which is representative of the strong shock to the health system. Conclusions: The disruption in the primary care model in Catalonia has led to an explosive increase in the number of non-face-to-face visits. There has been a reduction in the number of visits for diagnoses related to chronic pathologies, respiratory infections, obesity, and bodily injuries. Instead, visits for diagnoses related to socioeconomic and housing problems have increased, which emphasizes the importance of social determinants of health in the context of this pandemic. Big data analytics with routine care data yield findings that are consistent with those derived from intuition in everyday clinical practice and can help inform decision making by health planners in order to use the next few years to focus on the least-treated diseases during the COVID-19 pandemic.",COVID-19; primary care; diagnose variations; big data; ICD10; health system; big data; primary care; healthcare system,"Segui, FL; Guillamet, GH; Arolas, HP; Marin-Gomez, FX; Comellas, AR; Morros, AMR; Mas, CA; Vidal-Alaball, J",Pompeu Fabra University; Universitat de Vic - Universitat Central de Catalunya (UVic-UCC),2021.0,JOURNAL OF MEDICAL INTERNET RESEARCH,"JMIR PUBLICATIONS, INC",,10.2196/29622,20230520-160000,20230521-044735
